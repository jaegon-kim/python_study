# python_study

## Goal 1 - Deep learning & Mathmatics

https://toyourlight.tistory.com/category/%ED%8C%8C%EC%9D%B4%EC%8D%AC%20%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B3%BC%20%EC%88%98%ED%95%99

### 함수와 합성 합수
https://toyourlight.tistory.com/4
* src/math/ch1_composite_func.py

https://toyourlight.tistory.com/5
* src/math/ch2_derive.py
* src/math/ch2_drive_2.py

https://toyourlight.tistory.com/6
* src/math/ch3_comp_func.py
* src/math/ch3_2_comp_func_derive.py

https://toyourlight.tistory.com/7
* src/math/ch4_comp_vector_input.py

https://toyourlight.tistory.com/8 (전치행렬, transpose 사용)
* src/math/ch5_comp_vector_derive.py

https://toyourlight.tistory.com/9

https://toyourlight.tistory.com/10 (2차원 행렬을 입력 받는 합성 함수의 도함수)
* src/math/ch7_comp_2xmatrix_derive.py
* src/math/ch7_pytorch.py

### 평균(average), 편차(deviation), 분산(variance), 표준편차(variance), 정규화(normalization), 표준화(standardization)
https://toyourlight.tistory.com/36
* ch36_representative_value.py
* min-max 정규화 (0~1 까지의 값으로 normalziation)
* Z score 표준화 (deviation/standard-deviation)

### Numpy Deep Learning

https://toyourlight.tistory.com/11
 * 참고: https://heytech.tistory.com/362 (나중에 따로 볼만한 내용임)
 * 참조: http://infoso.kr/?p=2645 (∑연산 성질. 오차 제곱 합에 대한 편미분) 
 * 참고: https://mazdah.tistory.com/761
 * 참고: https://j1w2k3.tistory.com/1491,

https://toyourlight.tistory.com/12 (선형 회귀)
 * ch12_linear_regression.py
 * ch12_linear_regression_pytorch.py

https://toyourlight.tistory.com/13 (입력 특성이 2개인 선형 회귀)
 * ch13_multi_attribute.py

current: https://toyourlight.tistory.com/14 (로지스틱 회귀 구현하기)


https://toyourlight.tistory.com/category/%ED%8C%8C%EC%9D%B4%EC%8D%AC%20%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/Numpy%20%EB%94%A5%EB%9F%AC%EB%8B%9D?page=12
next:https://toyourlight.tistory.com/11


## Goal 2 - Natural Language Processing with Deep Learning
https://wikidocs.net/book/2155


## Metaplot library Usage References
https://jehyunlee.github.io/2021/07/10/Python-DS-80-mpl3d2/