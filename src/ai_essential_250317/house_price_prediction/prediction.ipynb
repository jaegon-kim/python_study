{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaegon-kim/python_study/blob/main/src/ai_essential_250317/house_price_prediction/prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C3HNZJH5LuqI",
      "metadata": {
        "id": "C3HNZJH5LuqI"
      },
      "source": [
        "# House Price Prediction\n",
        "- **목표**\n",
        "  - 이 워크샵은 주어진 데이터셋을 이용해 심층신경망 모델을 학습시켜 주택의 최종 판매 가격(SalePrice)을 예측하는 것이 최종 목표입니다.\n",
        "\n",
        "- **데이터셋 정보**\n",
        "  - 데이터셋은 총 79개의 설명 변수와 타겟 변수인 주택 가격(SalePrice)로 구성됩니다.\n",
        "  - 설명 변수는 주택의 다양한 특성(예: 건축 연도, 면적, 위치, 방 개수 등)을 포함합니다.\n",
        "  - 데이터는 판매 가격이 포함된 학습용 데이터인 `X`, `y` 와 판매 가격이 포함되지 않은 평가용 데이터인 `TEST`파일로 나뉘며, 각각 모델 학습 및 평가에 사용됩니다.\n",
        "    - 평가용 데이터 `TEST`의 판매 가격(SalePrice)를 예측 후 리더보드로 제출하여 평가합니다.\n",
        "\n",
        "- **문제 유형**\n",
        "  - 이 워크샵은 회귀 문제로 연속형 변수를 예측하는 것이 목표입니다. 모델의 성능은 `Mean Absolute Error`로 측정됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FerWbWa8ML9S",
      "metadata": {
        "id": "FerWbWa8ML9S"
      },
      "source": [
        "## 1. 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fbebc02c",
      "metadata": {
        "id": "fbebc02c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install JAEN -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2b192c23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b192c23",
        "outputId": "4d7d85a0-4ae8-4aec-dd08-7a677faecb84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 그대로 실행하세요.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "from JAEN.utils import plot_training_results\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OSiIE4tdPcSV",
      "metadata": {
        "id": "OSiIE4tdPcSV"
      },
      "source": [
        "## 2. 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Cwo9d1i-ON3u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwo9d1i-ON3u",
        "outputId": "60b066db-5119-4811-e120-1b936e725692"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1460, 79]), torch.Size([1460, 1]), torch.Size([1459, 79]))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from JAEN.datasets import load_house_price\n",
        "X, y, TEST = load_house_price()\n",
        "X.shape, y.shape, TEST.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "77cea581",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1000, 79]),\n",
              " torch.Size([1000, 1]),\n",
              " torch.Size([460, 79]),\n",
              " torch.Size([460, 1]))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_X, train_y = X[:1000, :], y[:1000, :]\n",
        "test_X, test_y = X[1000:, :], y[1000:, :]\n",
        "\n",
        "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I_Vc3a22PgBm",
      "metadata": {
        "id": "I_Vc3a22PgBm"
      },
      "source": [
        "## 3. 제출 예시 코드"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4wNCB3ATlBe4",
      "metadata": {
        "id": "4wNCB3ATlBe4"
      },
      "source": [
        "## 4. 심층신경망 모델을 구성하고 학습하여 TEST를 예측해보세요.\n",
        "- TEST의 예측 결과는 `comp.prediction`에 대입해주세요. **torch.tensor** 형태여야합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "KtwmpH1EibaM",
      "metadata": {
        "id": "KtwmpH1EibaM"
      },
      "outputs": [],
      "source": [
        "# DataLoader 생성\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "train_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(test_X, test_y), batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "-37EJXcZibcK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-37EJXcZibcK",
        "outputId": "88c4d3be-c0d4-4770-8402-4d60c05cb577"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "DNN                                      --\n",
              "├─Linear: 1-1                            10,240\n",
              "├─Linear: 1-2                            8,256\n",
              "├─Linear: 1-3                            65\n",
              "├─ReLU: 1-4                              --\n",
              "=================================================================\n",
              "Total params: 18,561\n",
              "Trainable params: 18,561\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DNN 모델 구성\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(79, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = DNN().to(device)\n",
        "summary(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f7b6fa57",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "    for data, labels in train_loader: # 미니 배치 별 파라미터 업데이트 수행\n",
        "        data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "        # 순전파\n",
        "        outputs = model(data)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 손실 누적\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    # - len(train_loader): 평균 Loss\n",
        "    return running_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3bb36d73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 평가 함수 정의\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "    with torch.no_grad():  # 평가 중에는 기울기 계산을 하지 않음\n",
        "        for data, labels in test_loader: # 미니 배치 별 손실 계산\n",
        "            data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "            # 순전파\n",
        "            outputs = model(data)\n",
        "\n",
        "            # 손실 계산\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 손실 누적\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    return running_loss / len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a2237784",
      "metadata": {
        "id": "a2237784"
      },
      "outputs": [],
      "source": [
        "# 손실함수 및 옵티마이저 설정\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "dacebf8e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Train Loss : 25304421.453125 Test Loss : 93586537.33333333\n",
            "Epoch 2 Train Loss : 24858897.296875 Test Loss : 92795209.6\n",
            "Epoch 3 Train Loss : 24690817.1953125 Test Loss : 93742575.46666667\n",
            "Epoch 4 Train Loss : 27346378.08203125 Test Loss : 96205270.93333334\n",
            "Epoch 5 Train Loss : 24853734.671875 Test Loss : 96368266.4\n",
            "Epoch 6 Train Loss : 25373622.9375 Test Loss : 94214922.66666667\n",
            "Epoch 7 Train Loss : 28101398.05078125 Test Loss : 93480498.93333334\n",
            "Epoch 8 Train Loss : 25099520.390625 Test Loss : 95231627.73333333\n",
            "Epoch 9 Train Loss : 25465063.6484375 Test Loss : 97884704.53333333\n",
            "Epoch 10 Train Loss : 24016013.4140625 Test Loss : 95112249.86666666\n",
            "Epoch 11 Train Loss : 24599497.07421875 Test Loss : 99622926.13333334\n",
            "Epoch 12 Train Loss : 28290213.578125 Test Loss : 97505525.33333333\n",
            "Epoch 13 Train Loss : 24504060.095703125 Test Loss : 97119172.0\n",
            "Epoch 14 Train Loss : 24821410.0 Test Loss : 99835738.4\n",
            "Epoch 15 Train Loss : 23597469.8125 Test Loss : 101227720.66666667\n",
            "Epoch 16 Train Loss : 23641731.765625 Test Loss : 98595957.33333333\n",
            "Epoch 17 Train Loss : 22897484.2265625 Test Loss : 98846218.66666667\n",
            "Epoch 18 Train Loss : 23069006.046875 Test Loss : 129125677.86666666\n",
            "Epoch 19 Train Loss : 24260295.375 Test Loss : 99199927.46666667\n",
            "Epoch 20 Train Loss : 22420832.90625 Test Loss : 98714336.8\n",
            "Epoch 21 Train Loss : 22707727.82421875 Test Loss : 101824578.93333334\n",
            "Epoch 22 Train Loss : 22574872.69921875 Test Loss : 101724886.13333334\n",
            "Epoch 23 Train Loss : 22535461.984375 Test Loss : 100573723.46666667\n",
            "Epoch 24 Train Loss : 22756657.875 Test Loss : 103511200.26666667\n",
            "Epoch 25 Train Loss : 22740162.515625 Test Loss : 101183779.73333333\n",
            "Epoch 26 Train Loss : 22808275.109375 Test Loss : 104514286.4\n",
            "Epoch 27 Train Loss : 22226960.96875 Test Loss : 101736739.73333333\n",
            "Epoch 28 Train Loss : 21842185.1796875 Test Loss : 102909198.26666667\n",
            "Epoch 29 Train Loss : 21884024.26171875 Test Loss : 104637154.4\n",
            "Epoch 30 Train Loss : 21710422.015625 Test Loss : 103775798.4\n",
            "Epoch 31 Train Loss : 21532260.796875 Test Loss : 105787336.53333333\n",
            "Epoch 32 Train Loss : 21106206.734375 Test Loss : 108765063.73333333\n",
            "Epoch 33 Train Loss : 21235897.2109375 Test Loss : 113086207.2\n",
            "Epoch 34 Train Loss : 21299365.484375 Test Loss : 106292842.13333334\n",
            "Epoch 35 Train Loss : 21108220.578125 Test Loss : 105400921.86666666\n",
            "Epoch 36 Train Loss : 21787281.6875 Test Loss : 103783376.13333334\n",
            "Epoch 37 Train Loss : 21245732.078125 Test Loss : 106997813.06666666\n",
            "Epoch 38 Train Loss : 21035989.73046875 Test Loss : 109027334.26666667\n",
            "Epoch 39 Train Loss : 24787880.9921875 Test Loss : 109606782.4\n",
            "Epoch 40 Train Loss : 21256634.140625 Test Loss : 108591991.06666666\n",
            "Epoch 41 Train Loss : 20532088.734375 Test Loss : 108404188.26666667\n",
            "Epoch 42 Train Loss : 20401713.546875 Test Loss : 106664396.66666667\n",
            "Epoch 43 Train Loss : 20680375.8671875 Test Loss : 108411625.06666666\n",
            "Epoch 44 Train Loss : 20471678.3515625 Test Loss : 110351592.26666667\n",
            "Epoch 45 Train Loss : 21163466.1640625 Test Loss : 112332311.46666667\n",
            "Epoch 46 Train Loss : 20176220.875 Test Loss : 109629381.6\n",
            "Epoch 47 Train Loss : 20209107.55078125 Test Loss : 109268782.13333334\n",
            "Epoch 48 Train Loss : 19913920.140625 Test Loss : 110529853.86666666\n",
            "Epoch 49 Train Loss : 19706160.2578125 Test Loss : 110580922.93333334\n",
            "Epoch 50 Train Loss : 20754182.61328125 Test Loss : 113653116.26666667\n",
            "Epoch 51 Train Loss : 22696834.375 Test Loss : 119372107.73333333\n",
            "Epoch 52 Train Loss : 20298947.7890625 Test Loss : 112275676.53333333\n",
            "Epoch 53 Train Loss : 19944546.328125 Test Loss : 111927497.06666666\n",
            "Epoch 54 Train Loss : 19533862.255859375 Test Loss : 114085217.86666666\n",
            "Epoch 55 Train Loss : 19083664.4140625 Test Loss : 113258368.53333333\n",
            "Epoch 56 Train Loss : 19309980.9453125 Test Loss : 113984954.66666667\n",
            "Epoch 57 Train Loss : 18831839.231445312 Test Loss : 114625335.2\n",
            "Epoch 58 Train Loss : 19909869.75 Test Loss : 115736658.13333334\n",
            "Epoch 59 Train Loss : 18897220.544921875 Test Loss : 115718399.46666667\n",
            "Epoch 60 Train Loss : 18869038.8046875 Test Loss : 119027402.93333334\n",
            "Epoch 61 Train Loss : 19399858.515625 Test Loss : 114579586.0\n",
            "Epoch 62 Train Loss : 18359878.40625 Test Loss : 118414660.8\n",
            "Epoch 63 Train Loss : 20879786.0546875 Test Loss : 120213912.53333333\n",
            "Epoch 64 Train Loss : 19215637.3515625 Test Loss : 117950891.06666666\n",
            "Epoch 65 Train Loss : 19192197.7109375 Test Loss : 123646198.4\n",
            "Epoch 66 Train Loss : 18161799.15625 Test Loss : 128530888.0\n",
            "Epoch 67 Train Loss : 19543150.8125 Test Loss : 128090585.06666666\n",
            "Epoch 68 Train Loss : 21704767.9765625 Test Loss : 125213649.6\n",
            "Epoch 69 Train Loss : 18156743.45703125 Test Loss : 118517179.06666666\n",
            "Epoch 70 Train Loss : 18551169.3203125 Test Loss : 122215820.8\n",
            "Epoch 71 Train Loss : 17942733.328125 Test Loss : 120376561.6\n",
            "Epoch 72 Train Loss : 17636999.9296875 Test Loss : 122600660.0\n",
            "Epoch 73 Train Loss : 17527190.65625 Test Loss : 147176010.93333334\n",
            "Epoch 74 Train Loss : 17269845.9765625 Test Loss : 120617572.93333334\n",
            "Epoch 75 Train Loss : 20256532.9296875 Test Loss : 129180700.0\n",
            "Epoch 76 Train Loss : 21399379.609375 Test Loss : 123035044.0\n",
            "Epoch 77 Train Loss : 17879724.49609375 Test Loss : 122564704.4\n",
            "Epoch 78 Train Loss : 17070971.4140625 Test Loss : 127926927.2\n",
            "Epoch 79 Train Loss : 16980257.65625 Test Loss : 129993365.33333333\n",
            "Epoch 80 Train Loss : 16862228.578125 Test Loss : 129280274.4\n",
            "Epoch 81 Train Loss : 17506081.0546875 Test Loss : 124425798.66666667\n",
            "Epoch 82 Train Loss : 17769034.4765625 Test Loss : 125639841.06666666\n",
            "Epoch 83 Train Loss : 16600173.03125 Test Loss : 125274541.33333333\n",
            "Epoch 84 Train Loss : 17150614.984375 Test Loss : 125106756.8\n",
            "Epoch 85 Train Loss : 16331401.30078125 Test Loss : 125118654.13333334\n",
            "Epoch 86 Train Loss : 16666775.75 Test Loss : 127144434.4\n",
            "Epoch 87 Train Loss : 16674750.234375 Test Loss : 125231935.6\n",
            "Epoch 88 Train Loss : 16599519.78125 Test Loss : 129174144.53333333\n",
            "Epoch 89 Train Loss : 16213962.86328125 Test Loss : 127144732.53333333\n",
            "Epoch 90 Train Loss : 16086961.53125 Test Loss : 130710345.06666666\n",
            "Epoch 91 Train Loss : 16382180.953125 Test Loss : 130999240.0\n",
            "Epoch 92 Train Loss : 16745629.8125 Test Loss : 139492487.33333334\n",
            "Epoch 93 Train Loss : 15949645.734375 Test Loss : 132167828.53333333\n",
            "Epoch 94 Train Loss : 16079599.125 Test Loss : 129035955.46666667\n",
            "Epoch 95 Train Loss : 16159664.50390625 Test Loss : 137589462.93333334\n",
            "Epoch 96 Train Loss : 15621934.8359375 Test Loss : 129937743.46666667\n",
            "Epoch 97 Train Loss : 15677998.5078125 Test Loss : 132687529.6\n",
            "Epoch 98 Train Loss : 15533964.4765625 Test Loss : 168642546.13333333\n",
            "Epoch 99 Train Loss : 15546783.65625 Test Loss : 133968402.93333334\n",
            "Epoch 100 Train Loss : 15419196.15625 Test Loss : 141207411.46666667\n",
            "Epoch 101 Train Loss : 15846849.7421875 Test Loss : 134076175.73333333\n",
            "Epoch 102 Train Loss : 14962843.0859375 Test Loss : 142538537.86666667\n",
            "Epoch 103 Train Loss : 15596656.83203125 Test Loss : 138025264.26666668\n",
            "Epoch 104 Train Loss : 15081036.59375 Test Loss : 139901796.0\n",
            "Epoch 105 Train Loss : 14980592.3125 Test Loss : 134427240.26666668\n",
            "Epoch 106 Train Loss : 15611069.375 Test Loss : 144698336.26666668\n",
            "Epoch 107 Train Loss : 15996287.84375 Test Loss : 134447089.33333334\n",
            "Epoch 108 Train Loss : 16642052.6953125 Test Loss : 135918385.06666666\n",
            "Epoch 109 Train Loss : 15622380.375 Test Loss : 136269914.13333333\n",
            "Epoch 110 Train Loss : 14641663.0625 Test Loss : 137842315.73333332\n",
            "Epoch 111 Train Loss : 14910434.734375 Test Loss : 138485286.93333334\n",
            "Epoch 112 Train Loss : 15108582.48828125 Test Loss : 146338061.86666667\n",
            "Epoch 113 Train Loss : 14727158.75 Test Loss : 137777129.33333334\n",
            "Epoch 114 Train Loss : 17122288.140625 Test Loss : 141931696.0\n",
            "Epoch 115 Train Loss : 14540590.389160156 Test Loss : 139057737.06666666\n",
            "Epoch 116 Train Loss : 14095823.625 Test Loss : 142497134.4\n",
            "Epoch 117 Train Loss : 14014011.78125 Test Loss : 138418895.2\n",
            "Epoch 118 Train Loss : 14106625.55078125 Test Loss : 164964009.06666666\n",
            "Epoch 119 Train Loss : 14077285.603515625 Test Loss : 142405661.6\n",
            "Epoch 120 Train Loss : 13946305.55859375 Test Loss : 143123406.93333334\n",
            "Epoch 121 Train Loss : 13847051.3359375 Test Loss : 145520848.8\n",
            "Epoch 122 Train Loss : 13910659.40625 Test Loss : 139400723.33333334\n",
            "Epoch 123 Train Loss : 14009507.64453125 Test Loss : 146878611.2\n",
            "Epoch 124 Train Loss : 13828373.03125 Test Loss : 167606490.13333333\n",
            "Epoch 125 Train Loss : 13590201.51171875 Test Loss : 145673596.0\n",
            "Epoch 126 Train Loss : 13997217.68359375 Test Loss : 142151209.06666666\n",
            "Epoch 127 Train Loss : 13383727.25 Test Loss : 144985602.13333333\n",
            "Epoch 128 Train Loss : 14407219.234375 Test Loss : 146989901.33333334\n",
            "Epoch 129 Train Loss : 14073097.291015625 Test Loss : 144307454.4\n",
            "Epoch 130 Train Loss : 13285760.580078125 Test Loss : 147377400.0\n",
            "Epoch 131 Train Loss : 13417844.4375 Test Loss : 145227866.4\n",
            "Epoch 132 Train Loss : 12934505.88671875 Test Loss : 144849345.06666666\n",
            "Epoch 133 Train Loss : 13255825.97265625 Test Loss : 144374316.53333333\n",
            "Epoch 134 Train Loss : 14688752.1328125 Test Loss : 147114629.6\n",
            "Epoch 135 Train Loss : 12830827.953125 Test Loss : 162219812.26666668\n",
            "Epoch 136 Train Loss : 13226113.71875 Test Loss : 149276933.33333334\n",
            "Epoch 137 Train Loss : 12873689.408203125 Test Loss : 153384674.13333333\n",
            "Epoch 138 Train Loss : 12951164.953125 Test Loss : 152228038.93333334\n",
            "Epoch 139 Train Loss : 12392576.73828125 Test Loss : 149524184.0\n",
            "Epoch 140 Train Loss : 12950055.5546875 Test Loss : 147144046.66666666\n",
            "Epoch 141 Train Loss : 12370950.859375 Test Loss : 147192898.13333333\n",
            "Epoch 142 Train Loss : 12869994.069335938 Test Loss : 150394889.86666667\n",
            "Epoch 143 Train Loss : 12456341.9453125 Test Loss : 152972666.93333334\n",
            "Epoch 144 Train Loss : 12487022.21484375 Test Loss : 149934635.2\n",
            "Epoch 145 Train Loss : 12552314.2578125 Test Loss : 152242123.2\n",
            "Epoch 146 Train Loss : 12400775.65234375 Test Loss : 159259551.46666667\n",
            "Epoch 147 Train Loss : 12723836.45703125 Test Loss : 152138492.26666668\n",
            "Epoch 148 Train Loss : 12258996.46875 Test Loss : 155620843.2\n",
            "Epoch 149 Train Loss : 11893583.533203125 Test Loss : 156730376.53333333\n",
            "Epoch 150 Train Loss : 11837282.794921875 Test Loss : 154055052.26666668\n",
            "Epoch 151 Train Loss : 12389588.5078125 Test Loss : 152978212.8\n",
            "Epoch 152 Train Loss : 11916261.32421875 Test Loss : 156716982.66666666\n",
            "Epoch 153 Train Loss : 11950614.072265625 Test Loss : 153349717.33333334\n",
            "Epoch 154 Train Loss : 11648742.58203125 Test Loss : 157208931.6\n",
            "Epoch 155 Train Loss : 11495701.55859375 Test Loss : 157170720.53333333\n",
            "Epoch 156 Train Loss : 12083516.98828125 Test Loss : 171242693.33333334\n",
            "Epoch 157 Train Loss : 11674118.75390625 Test Loss : 154605270.66666666\n",
            "Epoch 158 Train Loss : 11365190.8125 Test Loss : 168900150.93333334\n",
            "Epoch 159 Train Loss : 12604456.44921875 Test Loss : 156713486.4\n",
            "Epoch 160 Train Loss : 12309887.9609375 Test Loss : 157657517.86666667\n",
            "Epoch 161 Train Loss : 11743608.0 Test Loss : 155818333.33333334\n",
            "Epoch 162 Train Loss : 11288347.169921875 Test Loss : 155463450.93333334\n",
            "Epoch 163 Train Loss : 11288256.2734375 Test Loss : 156695972.26666668\n",
            "Epoch 164 Train Loss : 10804385.349609375 Test Loss : 156468915.73333332\n",
            "Epoch 165 Train Loss : 12215045.2265625 Test Loss : 159502732.8\n",
            "Epoch 166 Train Loss : 11253851.265625 Test Loss : 158353164.8\n",
            "Epoch 167 Train Loss : 11180510.04296875 Test Loss : 173513882.66666666\n",
            "Epoch 168 Train Loss : 10716877.2578125 Test Loss : 158282411.06666666\n",
            "Epoch 169 Train Loss : 10759062.6328125 Test Loss : 166279067.46666667\n",
            "Epoch 170 Train Loss : 10819342.029296875 Test Loss : 175768737.06666666\n",
            "Epoch 171 Train Loss : 10788638.3515625 Test Loss : 159442400.53333333\n",
            "Epoch 172 Train Loss : 11004160.28125 Test Loss : 159480643.2\n",
            "Epoch 173 Train Loss : 10907025.81640625 Test Loss : 169875550.93333334\n",
            "Epoch 174 Train Loss : 10694868.78125 Test Loss : 161006400.0\n",
            "Epoch 175 Train Loss : 10517486.234375 Test Loss : 161201352.53333333\n",
            "Epoch 176 Train Loss : 10433484.13671875 Test Loss : 175001110.93333334\n",
            "Epoch 177 Train Loss : 10723526.0546875 Test Loss : 171248436.8\n",
            "Epoch 178 Train Loss : 11812251.9296875 Test Loss : 161539019.2\n",
            "Epoch 179 Train Loss : 10404779.0078125 Test Loss : 167847252.26666668\n",
            "Epoch 180 Train Loss : 10396339.77734375 Test Loss : 165950356.26666668\n",
            "Epoch 181 Train Loss : 10465438.44921875 Test Loss : 165390313.06666666\n",
            "Epoch 182 Train Loss : 9907655.1875 Test Loss : 166650338.66666666\n",
            "Epoch 183 Train Loss : 9974916.609375 Test Loss : 166068800.0\n",
            "Epoch 184 Train Loss : 9977798.716796875 Test Loss : 167147464.53333333\n",
            "Epoch 185 Train Loss : 9937107.66015625 Test Loss : 167813724.26666668\n",
            "Epoch 186 Train Loss : 10048849.890625 Test Loss : 170708128.0\n",
            "Epoch 187 Train Loss : 9878702.333984375 Test Loss : 165871087.46666667\n",
            "Epoch 188 Train Loss : 9435140.5 Test Loss : 165989236.8\n",
            "Epoch 189 Train Loss : 9715878.46484375 Test Loss : 164579200.93333334\n",
            "Epoch 190 Train Loss : 10615915.5390625 Test Loss : 166689581.33333334\n",
            "Epoch 191 Train Loss : 10673764.91796875 Test Loss : 170031988.26666668\n",
            "Epoch 192 Train Loss : 9642720.137695312 Test Loss : 168621751.46666667\n",
            "Epoch 193 Train Loss : 9471865.203125 Test Loss : 169011991.46666667\n",
            "Epoch 194 Train Loss : 9324961.03125 Test Loss : 171835306.66666666\n",
            "Epoch 195 Train Loss : 9315592.50390625 Test Loss : 171413768.0\n",
            "Epoch 196 Train Loss : 11005340.625 Test Loss : 168805587.46666667\n",
            "Epoch 197 Train Loss : 10121322.400390625 Test Loss : 172777116.8\n",
            "Epoch 198 Train Loss : 9818804.0 Test Loss : 179367556.8\n",
            "Epoch 199 Train Loss : 9273644.62109375 Test Loss : 180412177.6\n",
            "Epoch 200 Train Loss : 9174764.99609375 Test Loss : 187990098.66666666\n",
            "Epoch 201 Train Loss : 8927021.8671875 Test Loss : 172134851.46666667\n",
            "Epoch 202 Train Loss : 9040676.80078125 Test Loss : 192280218.13333333\n",
            "Epoch 203 Train Loss : 10874111.474609375 Test Loss : 172765740.8\n",
            "Epoch 204 Train Loss : 10637456.484375 Test Loss : 174289704.26666668\n",
            "Epoch 205 Train Loss : 9701104.828125 Test Loss : 177321728.26666668\n",
            "Epoch 206 Train Loss : 9032183.794921875 Test Loss : 174648989.33333334\n",
            "Epoch 207 Train Loss : 9223449.83203125 Test Loss : 184094594.66666666\n",
            "Epoch 208 Train Loss : 8560394.328125 Test Loss : 176972478.4\n",
            "Epoch 209 Train Loss : 8607240.2578125 Test Loss : 172673511.2\n",
            "Epoch 210 Train Loss : 8865087.15234375 Test Loss : 178541415.46666667\n",
            "Epoch 211 Train Loss : 8657098.00390625 Test Loss : 178148775.46666667\n",
            "Epoch 212 Train Loss : 8688899.9765625 Test Loss : 174996955.2\n",
            "Epoch 213 Train Loss : 8454853.1484375 Test Loss : 186209465.06666666\n",
            "Epoch 214 Train Loss : 9074178.546875 Test Loss : 177769676.26666668\n",
            "Epoch 215 Train Loss : 8240373.609375 Test Loss : 194301163.2\n",
            "Epoch 216 Train Loss : 8331296.09375 Test Loss : 179090117.86666667\n",
            "Epoch 217 Train Loss : 8404112.7734375 Test Loss : 177047695.46666667\n",
            "Epoch 218 Train Loss : 8261604.41796875 Test Loss : 191566585.6\n",
            "Epoch 219 Train Loss : 9450508.59375 Test Loss : 183355049.33333334\n",
            "Epoch 220 Train Loss : 8589806.935546875 Test Loss : 179466660.8\n",
            "Epoch 221 Train Loss : 8264680.4208984375 Test Loss : 177768258.93333334\n",
            "Epoch 222 Train Loss : 8010248.70703125 Test Loss : 180031914.13333333\n",
            "Epoch 223 Train Loss : 10231374.66015625 Test Loss : 179990624.53333333\n",
            "Epoch 224 Train Loss : 8697419.883789062 Test Loss : 188237455.73333332\n",
            "Epoch 225 Train Loss : 8462632.048828125 Test Loss : 185041692.8\n",
            "Epoch 226 Train Loss : 8189322.212890625 Test Loss : 180103660.8\n",
            "Epoch 227 Train Loss : 8165146.884765625 Test Loss : 180742115.73333332\n",
            "Epoch 228 Train Loss : 8570089.90625 Test Loss : 182462195.73333332\n",
            "Epoch 229 Train Loss : 7519894.9375 Test Loss : 180551730.13333333\n",
            "Epoch 230 Train Loss : 8416818.04296875 Test Loss : 218725888.26666668\n",
            "Epoch 231 Train Loss : 7613412.09765625 Test Loss : 182499392.53333333\n",
            "Epoch 232 Train Loss : 7500261.0400390625 Test Loss : 182573819.73333332\n",
            "Epoch 233 Train Loss : 7639941.187011719 Test Loss : 185942281.06666666\n",
            "Epoch 234 Train Loss : 8237331.603515625 Test Loss : 184968871.46666667\n",
            "Epoch 235 Train Loss : 7583776.21875 Test Loss : 202215436.26666668\n",
            "Epoch 236 Train Loss : 7452258.466796875 Test Loss : 200262194.93333334\n",
            "Epoch 237 Train Loss : 7679988.14453125 Test Loss : 184323584.53333333\n",
            "Epoch 238 Train Loss : 7231420.7109375 Test Loss : 184726325.86666667\n",
            "Epoch 239 Train Loss : 7263328.98046875 Test Loss : 193949993.6\n",
            "Epoch 240 Train Loss : 7369276.369140625 Test Loss : 185872938.4\n",
            "Epoch 241 Train Loss : 7188198.873046875 Test Loss : 188712946.13333333\n",
            "Epoch 242 Train Loss : 7207067.671875 Test Loss : 189631030.93333334\n",
            "Epoch 243 Train Loss : 7510984.548828125 Test Loss : 206870510.93333334\n",
            "Epoch 244 Train Loss : 7322597.380371094 Test Loss : 191936484.8\n",
            "Epoch 245 Train Loss : 7052575.3828125 Test Loss : 189642786.66666666\n",
            "Epoch 246 Train Loss : 6948650.48828125 Test Loss : 190278869.33333334\n",
            "Epoch 247 Train Loss : 7277334.228515625 Test Loss : 194459813.33333334\n",
            "Epoch 248 Train Loss : 6973367.017822266 Test Loss : 211597419.2\n",
            "Epoch 249 Train Loss : 7137457.53515625 Test Loss : 190519751.46666667\n",
            "Epoch 250 Train Loss : 8578718.19921875 Test Loss : 190678976.0\n",
            "Epoch 251 Train Loss : 6785453.7734375 Test Loss : 188693299.2\n",
            "Epoch 252 Train Loss : 7786984.044921875 Test Loss : 193039370.13333333\n",
            "Epoch 253 Train Loss : 7045826.548828125 Test Loss : 187042077.33333334\n",
            "Epoch 254 Train Loss : 6798685.0078125 Test Loss : 198848521.6\n",
            "Epoch 255 Train Loss : 6704301.478515625 Test Loss : 191803317.33333334\n",
            "Epoch 256 Train Loss : 6588026.46875 Test Loss : 195476929.33333334\n",
            "Epoch 257 Train Loss : 6442027.496582031 Test Loss : 189920344.0\n",
            "Epoch 258 Train Loss : 6627748.21875 Test Loss : 191019901.33333334\n",
            "Epoch 259 Train Loss : 6408594.1240234375 Test Loss : 192441572.26666668\n",
            "Epoch 260 Train Loss : 6620683.3046875 Test Loss : 192453791.73333332\n",
            "Epoch 261 Train Loss : 6443686.580078125 Test Loss : 192765316.0\n",
            "Epoch 262 Train Loss : 6484729.89453125 Test Loss : 206764674.13333333\n",
            "Epoch 263 Train Loss : 6356355.392578125 Test Loss : 196840308.26666668\n",
            "Epoch 264 Train Loss : 6758916.396484375 Test Loss : 193347665.6\n",
            "Epoch 265 Train Loss : 6210939.021484375 Test Loss : 220086908.26666668\n",
            "Epoch 266 Train Loss : 6437980.001953125 Test Loss : 206711309.33333334\n",
            "Epoch 267 Train Loss : 6424995.234375 Test Loss : 228615956.26666668\n",
            "Epoch 268 Train Loss : 6602877.74609375 Test Loss : 221667976.53333333\n",
            "Epoch 269 Train Loss : 6595417.078125 Test Loss : 203601072.53333333\n",
            "Epoch 270 Train Loss : 6305770.6572265625 Test Loss : 199288947.2\n",
            "Epoch 271 Train Loss : 6103856.779296875 Test Loss : 198767660.26666668\n",
            "Epoch 272 Train Loss : 6248876.6953125 Test Loss : 194854307.73333332\n",
            "Epoch 273 Train Loss : 5958068.40234375 Test Loss : 198109998.93333334\n",
            "Epoch 274 Train Loss : 5898425.078125 Test Loss : 197390119.2\n",
            "Epoch 275 Train Loss : 6091134.8349609375 Test Loss : 205479995.2\n",
            "Epoch 276 Train Loss : 6214401.4921875 Test Loss : 200685730.13333333\n",
            "Epoch 277 Train Loss : 5783155.466796875 Test Loss : 198932140.8\n",
            "Epoch 278 Train Loss : 5644001.8310546875 Test Loss : 201076310.93333334\n",
            "Epoch 279 Train Loss : 5787727.55859375 Test Loss : 210995385.6\n",
            "Epoch 280 Train Loss : 5669508.876953125 Test Loss : 204220217.06666666\n",
            "Epoch 281 Train Loss : 5639416.3203125 Test Loss : 200572245.86666667\n",
            "Epoch 282 Train Loss : 5603675.40234375 Test Loss : 198324728.26666668\n",
            "Epoch 283 Train Loss : 5559173.34765625 Test Loss : 199647252.26666668\n",
            "Epoch 284 Train Loss : 5968247.126953125 Test Loss : 199232624.53333333\n",
            "Epoch 285 Train Loss : 5968957.923828125 Test Loss : 203643437.33333334\n",
            "Epoch 286 Train Loss : 5504675.798828125 Test Loss : 203021556.26666668\n",
            "Epoch 287 Train Loss : 5590332.142578125 Test Loss : 200872512.53333333\n",
            "Epoch 288 Train Loss : 5595303.78515625 Test Loss : 200910052.0\n",
            "Epoch 289 Train Loss : 5384205.1865234375 Test Loss : 212860013.33333334\n",
            "Epoch 290 Train Loss : 5791924.39453125 Test Loss : 211186294.13333333\n",
            "Epoch 291 Train Loss : 5394596.23046875 Test Loss : 212548972.26666668\n",
            "Epoch 292 Train Loss : 5422182.267578125 Test Loss : 209084619.2\n",
            "Epoch 293 Train Loss : 5340527.9794921875 Test Loss : 209224142.93333334\n",
            "Epoch 294 Train Loss : 5157264.6484375 Test Loss : 211934614.93333334\n",
            "Epoch 295 Train Loss : 5205852.46484375 Test Loss : 209001197.86666667\n",
            "Epoch 296 Train Loss : 5184922.59765625 Test Loss : 213073479.46666667\n",
            "Epoch 297 Train Loss : 5100906.939453125 Test Loss : 206052913.06666666\n",
            "Epoch 298 Train Loss : 5112064.830078125 Test Loss : 208913986.66666666\n",
            "Epoch 299 Train Loss : 5308663.986328125 Test Loss : 206372578.66666666\n",
            "Epoch 300 Train Loss : 5188505.36328125 Test Loss : 207744485.86666667\n",
            "Epoch 301 Train Loss : 5246602.0625 Test Loss : 205805236.26666668\n",
            "Epoch 302 Train Loss : 5094789.98828125 Test Loss : 205949214.13333333\n",
            "Epoch 303 Train Loss : 5453143.248046875 Test Loss : 229285663.46666667\n",
            "Epoch 304 Train Loss : 5017394.814453125 Test Loss : 218325246.4\n",
            "Epoch 305 Train Loss : 5174444.1669921875 Test Loss : 216042714.66666666\n",
            "Epoch 306 Train Loss : 4899296.97265625 Test Loss : 208445474.13333333\n",
            "Epoch 307 Train Loss : 5522748.046875 Test Loss : 209609414.93333334\n",
            "Epoch 308 Train Loss : 5227562.9072265625 Test Loss : 205850293.2\n",
            "Epoch 309 Train Loss : 5031639.51953125 Test Loss : 213000147.2\n",
            "Epoch 310 Train Loss : 5633336.001953125 Test Loss : 206938256.53333333\n",
            "Epoch 311 Train Loss : 4975596.16015625 Test Loss : 212530830.4\n",
            "Epoch 312 Train Loss : 4858747.099609375 Test Loss : 210743575.46666667\n",
            "Epoch 313 Train Loss : 4821270.638671875 Test Loss : 215552428.8\n",
            "Epoch 314 Train Loss : 5995387.578125 Test Loss : 211807403.73333332\n",
            "Epoch 315 Train Loss : 5207082.310546875 Test Loss : 210435112.8\n",
            "Epoch 316 Train Loss : 5120060.017578125 Test Loss : 217827245.33333334\n",
            "Epoch 317 Train Loss : 4389196.9423828125 Test Loss : 215304500.8\n",
            "Epoch 318 Train Loss : 4601280.544921875 Test Loss : 215523107.2\n",
            "Epoch 319 Train Loss : 4906035.7109375 Test Loss : 214929322.13333333\n",
            "Epoch 320 Train Loss : 4591510.1279296875 Test Loss : 212746520.0\n",
            "Epoch 321 Train Loss : 4482337.087890625 Test Loss : 212958026.93333334\n",
            "Epoch 322 Train Loss : 4745414.904296875 Test Loss : 224599466.66666666\n",
            "Epoch 323 Train Loss : 4509076.7314453125 Test Loss : 226714362.66666666\n",
            "Epoch 324 Train Loss : 4924718.44140625 Test Loss : 213851913.06666666\n",
            "Epoch 325 Train Loss : 4574642.1806640625 Test Loss : 222207442.66666666\n",
            "Epoch 326 Train Loss : 4284799.955566406 Test Loss : 229581566.93333334\n",
            "Epoch 327 Train Loss : 4201913.4609375 Test Loss : 224335934.93333334\n",
            "Epoch 328 Train Loss : 4084832.1787109375 Test Loss : 214217044.26666668\n",
            "Epoch 329 Train Loss : 4012816.8107910156 Test Loss : 218826100.26666668\n",
            "Epoch 330 Train Loss : 4255875.2822265625 Test Loss : 215956326.93333334\n",
            "Epoch 331 Train Loss : 4286867.7919921875 Test Loss : 219547187.2\n",
            "Epoch 332 Train Loss : 4101570.5283203125 Test Loss : 218530625.06666666\n",
            "Epoch 333 Train Loss : 4104709.494140625 Test Loss : 230322573.33333334\n",
            "Epoch 334 Train Loss : 4047150.31640625 Test Loss : 214747918.13333333\n",
            "Epoch 335 Train Loss : 4007528.7421875 Test Loss : 221334401.6\n",
            "Epoch 336 Train Loss : 3948439.9296875 Test Loss : 220311319.46666667\n",
            "Epoch 337 Train Loss : 4298538.6416015625 Test Loss : 226102317.33333334\n",
            "Epoch 338 Train Loss : 4474275.15234375 Test Loss : 218953621.86666667\n",
            "Epoch 339 Train Loss : 4385909.99609375 Test Loss : 227187966.93333334\n",
            "Epoch 340 Train Loss : 3911502.482421875 Test Loss : 220403501.86666667\n",
            "Epoch 341 Train Loss : 3967769.09765625 Test Loss : 221972956.26666668\n",
            "Epoch 342 Train Loss : 4047893.544921875 Test Loss : 231482474.13333333\n",
            "Epoch 343 Train Loss : 3966806.5192871094 Test Loss : 220804265.6\n",
            "Epoch 344 Train Loss : 4145611.8544921875 Test Loss : 226864862.93333334\n",
            "Epoch 345 Train Loss : 3921995.083984375 Test Loss : 228035077.33333334\n",
            "Epoch 346 Train Loss : 3744759.6810302734 Test Loss : 227283840.0\n",
            "Epoch 347 Train Loss : 3848699.6958007812 Test Loss : 226387683.73333332\n",
            "Epoch 348 Train Loss : 3802528.048828125 Test Loss : 224119875.73333332\n",
            "Epoch 349 Train Loss : 3901476.6953125 Test Loss : 227474084.26666668\n",
            "Epoch 350 Train Loss : 4145749.7626953125 Test Loss : 224786497.6\n",
            "Epoch 351 Train Loss : 3750001.130859375 Test Loss : 224637133.86666667\n",
            "Epoch 352 Train Loss : 3818476.599609375 Test Loss : 238989159.46666667\n",
            "Epoch 353 Train Loss : 3571779.3251953125 Test Loss : 229788393.6\n",
            "Epoch 354 Train Loss : 3487589.8623046875 Test Loss : 236272563.2\n",
            "Epoch 355 Train Loss : 3567492.443359375 Test Loss : 233458246.93333334\n",
            "Epoch 356 Train Loss : 3605299.8740234375 Test Loss : 224266948.26666668\n",
            "Epoch 357 Train Loss : 3541590.83203125 Test Loss : 226291637.33333334\n",
            "Epoch 358 Train Loss : 3394912.765625 Test Loss : 231269684.26666668\n",
            "Epoch 359 Train Loss : 3443651.4873046875 Test Loss : 225066798.93333334\n",
            "Epoch 360 Train Loss : 3502087.3090820312 Test Loss : 228392036.8\n",
            "Epoch 361 Train Loss : 3538281.37890625 Test Loss : 228202857.06666666\n",
            "Epoch 362 Train Loss : 3454131.5380859375 Test Loss : 230037270.93333334\n",
            "Epoch 363 Train Loss : 3314166.0673828125 Test Loss : 228665249.06666666\n",
            "Epoch 364 Train Loss : 3241841.7241210938 Test Loss : 231689689.06666666\n",
            "Epoch 365 Train Loss : 3276869.3950195312 Test Loss : 226756076.0\n",
            "Epoch 366 Train Loss : 3212957.1982421875 Test Loss : 231486773.86666667\n",
            "Epoch 367 Train Loss : 3307931.94140625 Test Loss : 234030955.73333332\n",
            "Epoch 368 Train Loss : 3611957.58984375 Test Loss : 236244410.13333333\n",
            "Epoch 369 Train Loss : 3489163.0717773438 Test Loss : 231729604.26666668\n",
            "Epoch 370 Train Loss : 3161988.0361328125 Test Loss : 267848902.4\n",
            "Epoch 371 Train Loss : 3328185.083984375 Test Loss : 235123264.0\n",
            "Epoch 372 Train Loss : 3146944.3876953125 Test Loss : 234750792.0\n",
            "Epoch 373 Train Loss : 3656587.9521484375 Test Loss : 236971013.33333334\n",
            "Epoch 374 Train Loss : 3513922.072265625 Test Loss : 234879507.73333332\n",
            "Epoch 375 Train Loss : 3315892.65625 Test Loss : 232564186.13333333\n",
            "Epoch 376 Train Loss : 3031218.703125 Test Loss : 234377907.73333332\n",
            "Epoch 377 Train Loss : 3183383.689453125 Test Loss : 235839248.53333333\n",
            "Epoch 378 Train Loss : 3114789.2954101562 Test Loss : 238085011.2\n",
            "Epoch 379 Train Loss : 3150034.892578125 Test Loss : 245718866.13333333\n",
            "Epoch 380 Train Loss : 3091093.6298828125 Test Loss : 238202238.4\n",
            "Epoch 381 Train Loss : 3542152.595703125 Test Loss : 241752351.46666667\n",
            "Epoch 382 Train Loss : 2997903.4228515625 Test Loss : 238776530.13333333\n",
            "Epoch 383 Train Loss : 2978625.1259765625 Test Loss : 235884017.06666666\n",
            "Epoch 384 Train Loss : 2895093.2670898438 Test Loss : 240555496.0\n",
            "Epoch 385 Train Loss : 2924289.16015625 Test Loss : 244338574.4\n",
            "Epoch 386 Train Loss : 2904322.0834960938 Test Loss : 256949974.93333334\n",
            "Epoch 387 Train Loss : 2943287.599609375 Test Loss : 237075606.93333334\n",
            "Epoch 388 Train Loss : 2792854.6791992188 Test Loss : 246896698.13333333\n",
            "Epoch 389 Train Loss : 2922739.3334960938 Test Loss : 235405112.0\n",
            "Epoch 390 Train Loss : 2776689.2802734375 Test Loss : 240186044.8\n",
            "Epoch 391 Train Loss : 2726422.2548828125 Test Loss : 246664661.86666667\n",
            "Epoch 392 Train Loss : 2689349.0673828125 Test Loss : 246892235.2\n",
            "Epoch 393 Train Loss : 2831157.220703125 Test Loss : 239050883.2\n",
            "Epoch 394 Train Loss : 3074682.568359375 Test Loss : 238359091.2\n",
            "Epoch 395 Train Loss : 2984989.7646484375 Test Loss : 254679684.26666668\n",
            "Epoch 396 Train Loss : 3022588.642578125 Test Loss : 248967385.6\n",
            "Epoch 397 Train Loss : 2890701.8876953125 Test Loss : 239589357.33333334\n",
            "Epoch 398 Train Loss : 3100974.38671875 Test Loss : 242875461.33333334\n",
            "Epoch 399 Train Loss : 2589671.923828125 Test Loss : 244838102.4\n",
            "Epoch 400 Train Loss : 2529459.02734375 Test Loss : 241699473.06666666\n",
            "Epoch 401 Train Loss : 2604162.69140625 Test Loss : 248395132.8\n",
            "Epoch 402 Train Loss : 2741487.517578125 Test Loss : 240306939.2\n",
            "Epoch 403 Train Loss : 2575654.5620117188 Test Loss : 243047475.2\n",
            "Epoch 404 Train Loss : 2575280.6723632812 Test Loss : 240428826.66666666\n",
            "Epoch 405 Train Loss : 2728267.130859375 Test Loss : 252420350.4\n",
            "Epoch 406 Train Loss : 2586147.8779296875 Test Loss : 245349109.33333334\n",
            "Epoch 407 Train Loss : 2534417.665283203 Test Loss : 247216621.86666667\n",
            "Epoch 408 Train Loss : 2363919.4780273438 Test Loss : 244044915.2\n",
            "Epoch 409 Train Loss : 2409035.876953125 Test Loss : 253624153.86666667\n",
            "Epoch 410 Train Loss : 2477623.207763672 Test Loss : 246659874.13333333\n",
            "Epoch 411 Train Loss : 2342914.083984375 Test Loss : 245504172.26666668\n",
            "Epoch 412 Train Loss : 2384502.1181640625 Test Loss : 261878205.86666667\n",
            "Epoch 413 Train Loss : 2486249.244140625 Test Loss : 244149176.0\n",
            "Epoch 414 Train Loss : 2347837.521484375 Test Loss : 248556515.2\n",
            "Epoch 415 Train Loss : 2356238.8369140625 Test Loss : 251325009.06666666\n",
            "Epoch 416 Train Loss : 2281203.7587890625 Test Loss : 255367516.8\n",
            "Epoch 417 Train Loss : 2282515.0458984375 Test Loss : 247756335.46666667\n",
            "Epoch 418 Train Loss : 2319869.3477783203 Test Loss : 253651819.2\n",
            "Epoch 419 Train Loss : 2181941.8588867188 Test Loss : 246647924.8\n",
            "Epoch 420 Train Loss : 2379878.26171875 Test Loss : 250301100.8\n",
            "Epoch 421 Train Loss : 2158637.08203125 Test Loss : 260622138.13333333\n",
            "Epoch 422 Train Loss : 2584719.9169921875 Test Loss : 248925634.13333333\n",
            "Epoch 423 Train Loss : 2226468.640625 Test Loss : 252359164.8\n",
            "Epoch 424 Train Loss : 2629800.7744140625 Test Loss : 251251755.73333332\n",
            "Epoch 425 Train Loss : 2388730.267578125 Test Loss : 249407299.73333332\n",
            "Epoch 426 Train Loss : 2228723.9975585938 Test Loss : 257345188.8\n",
            "Epoch 427 Train Loss : 2180546.6540527344 Test Loss : 250528781.86666667\n",
            "Epoch 428 Train Loss : 2055551.40625 Test Loss : 256101146.66666666\n",
            "Epoch 429 Train Loss : 2036274.6225585938 Test Loss : 259344784.53333333\n",
            "Epoch 430 Train Loss : 2255462.21875 Test Loss : 252846338.13333333\n",
            "Epoch 431 Train Loss : 1997623.7727050781 Test Loss : 257753325.86666667\n",
            "Epoch 432 Train Loss : 2021220.4584960938 Test Loss : 255196690.13333333\n",
            "Epoch 433 Train Loss : 2036747.0634765625 Test Loss : 257611083.73333332\n",
            "Epoch 434 Train Loss : 1926954.5166015625 Test Loss : 253687779.73333332\n",
            "Epoch 435 Train Loss : 2127761.5112304688 Test Loss : 256591400.53333333\n",
            "Epoch 436 Train Loss : 2048166.8869628906 Test Loss : 251983277.33333334\n",
            "Epoch 437 Train Loss : 2027601.1791992188 Test Loss : 253020248.0\n",
            "Epoch 438 Train Loss : 1857102.5114746094 Test Loss : 255050977.06666666\n",
            "Epoch 439 Train Loss : 1911501.7670898438 Test Loss : 259568121.06666666\n",
            "Epoch 440 Train Loss : 1785243.0865478516 Test Loss : 266973267.2\n",
            "Epoch 441 Train Loss : 1974546.037109375 Test Loss : 262793724.8\n",
            "Epoch 442 Train Loss : 2007308.294921875 Test Loss : 257192171.2\n",
            "Epoch 443 Train Loss : 1946719.8442382812 Test Loss : 276439934.4\n",
            "Epoch 444 Train Loss : 1978593.6708984375 Test Loss : 266026585.6\n",
            "Epoch 445 Train Loss : 1985499.6538085938 Test Loss : 254798103.46666667\n",
            "Epoch 446 Train Loss : 1900617.8291015625 Test Loss : 254865077.86666667\n",
            "Epoch 447 Train Loss : 1875322.6744384766 Test Loss : 260123356.8\n",
            "Epoch 448 Train Loss : 1986812.3779296875 Test Loss : 259692701.86666667\n",
            "Epoch 449 Train Loss : 1854968.7626953125 Test Loss : 264924508.8\n",
            "Epoch 450 Train Loss : 1766360.2255859375 Test Loss : 261259773.86666667\n",
            "Epoch 451 Train Loss : 1694257.7092285156 Test Loss : 267951812.26666668\n",
            "Epoch 452 Train Loss : 1800272.326171875 Test Loss : 260774763.73333332\n",
            "Epoch 453 Train Loss : 1828036.5185546875 Test Loss : 259434126.93333334\n",
            "Epoch 454 Train Loss : 1669998.8598632812 Test Loss : 268602339.73333335\n",
            "Epoch 455 Train Loss : 1655235.6108398438 Test Loss : 258174103.46666667\n",
            "Epoch 456 Train Loss : 1837778.4951171875 Test Loss : 263025639.46666667\n",
            "Epoch 457 Train Loss : 1806205.8349609375 Test Loss : 257163745.6\n",
            "Epoch 458 Train Loss : 1618374.9926757812 Test Loss : 255063536.8\n",
            "Epoch 459 Train Loss : 1736842.6147460938 Test Loss : 263330749.33333334\n",
            "Epoch 460 Train Loss : 1627087.3154296875 Test Loss : 284026390.4\n",
            "Epoch 461 Train Loss : 1572477.4711914062 Test Loss : 298701674.6666667\n",
            "Epoch 462 Train Loss : 1592132.2014160156 Test Loss : 260073709.86666667\n",
            "Epoch 463 Train Loss : 1768301.435546875 Test Loss : 267597133.86666667\n",
            "Epoch 464 Train Loss : 1722124.1108398438 Test Loss : 260964820.26666668\n",
            "Epoch 465 Train Loss : 1730865.3149414062 Test Loss : 280962008.53333336\n",
            "Epoch 466 Train Loss : 1686187.3046875 Test Loss : 271309958.93333334\n",
            "Epoch 467 Train Loss : 1815939.3994140625 Test Loss : 263268316.26666668\n",
            "Epoch 468 Train Loss : 1567037.9233398438 Test Loss : 260034925.33333334\n",
            "Epoch 469 Train Loss : 1557432.2919921875 Test Loss : 267784009.06666666\n",
            "Epoch 470 Train Loss : 1493990.3642578125 Test Loss : 283009545.06666666\n",
            "Epoch 471 Train Loss : 1593133.07421875 Test Loss : 265306183.46666667\n",
            "Epoch 472 Train Loss : 1833207.205078125 Test Loss : 290352831.46666664\n",
            "Epoch 473 Train Loss : 1775481.759765625 Test Loss : 270857487.46666664\n",
            "Epoch 474 Train Loss : 1556554.3544921875 Test Loss : 275178275.73333335\n",
            "Epoch 475 Train Loss : 1622181.8569335938 Test Loss : 273146359.46666664\n",
            "Epoch 476 Train Loss : 1486358.09765625 Test Loss : 264525298.66666666\n",
            "Epoch 477 Train Loss : 1360173.9477539062 Test Loss : 270336762.6666667\n",
            "Epoch 478 Train Loss : 1460833.5385742188 Test Loss : 268892542.4\n",
            "Epoch 479 Train Loss : 1473002.591796875 Test Loss : 268827236.26666665\n",
            "Epoch 480 Train Loss : 1412652.9389648438 Test Loss : 266491872.0\n",
            "Epoch 481 Train Loss : 1355686.6481933594 Test Loss : 271944032.53333336\n",
            "Epoch 482 Train Loss : 1305150.2355957031 Test Loss : 285457319.46666664\n",
            "Epoch 483 Train Loss : 1386106.0283203125 Test Loss : 268050779.73333332\n",
            "Epoch 484 Train Loss : 1425193.5554199219 Test Loss : 270061297.06666666\n",
            "Epoch 485 Train Loss : 1289110.3754882812 Test Loss : 272719461.3333333\n",
            "Epoch 486 Train Loss : 1272318.9653320312 Test Loss : 272904219.73333335\n",
            "Epoch 487 Train Loss : 1361006.8100585938 Test Loss : 270052696.0\n",
            "Epoch 488 Train Loss : 1366081.4615478516 Test Loss : 270015553.06666666\n",
            "Epoch 489 Train Loss : 1300976.0014648438 Test Loss : 319842634.6666667\n",
            "Epoch 490 Train Loss : 1362408.8933105469 Test Loss : 292567012.26666665\n",
            "Epoch 491 Train Loss : 1286207.205078125 Test Loss : 268445610.1333333\n",
            "Epoch 492 Train Loss : 1309118.8017578125 Test Loss : 269575015.46666664\n",
            "Epoch 493 Train Loss : 1244113.83984375 Test Loss : 297096630.4\n",
            "Epoch 494 Train Loss : 1222997.8989257812 Test Loss : 269151990.93333334\n",
            "Epoch 495 Train Loss : 1389134.6196289062 Test Loss : 270995714.6666667\n",
            "Epoch 496 Train Loss : 1214978.5549316406 Test Loss : 278630187.73333335\n",
            "Epoch 497 Train Loss : 1252258.9846191406 Test Loss : 271012462.4\n",
            "Epoch 498 Train Loss : 1154115.998046875 Test Loss : 279133529.6\n",
            "Epoch 499 Train Loss : 1185511.3359375 Test Loss : 321409696.53333336\n",
            "Epoch 500 Train Loss : 1143582.5754394531 Test Loss : 276453795.73333335\n",
            "Epoch 501 Train Loss : 1137553.1362304688 Test Loss : 273363588.26666665\n",
            "Epoch 502 Train Loss : 1098276.2358398438 Test Loss : 273187826.1333333\n",
            "Epoch 503 Train Loss : 1134771.1787109375 Test Loss : 277388606.93333334\n",
            "Epoch 504 Train Loss : 1301634.228515625 Test Loss : 304287982.93333334\n",
            "Epoch 505 Train Loss : 1192056.8088378906 Test Loss : 275700788.26666665\n",
            "Epoch 506 Train Loss : 1145361.4067382812 Test Loss : 280626433.6\n",
            "Epoch 507 Train Loss : 1311833.673828125 Test Loss : 298556081.6\n",
            "Epoch 508 Train Loss : 1253843.6303710938 Test Loss : 276738145.06666666\n",
            "Epoch 509 Train Loss : 1449097.5888671875 Test Loss : 304432126.93333334\n",
            "Epoch 510 Train Loss : 1468302.4848632812 Test Loss : 277979834.6666667\n",
            "Epoch 511 Train Loss : 1051439.486694336 Test Loss : 300781111.46666664\n",
            "Epoch 512 Train Loss : 989216.8120117188 Test Loss : 275147910.4\n",
            "Epoch 513 Train Loss : 971652.0627441406 Test Loss : 274492724.26666665\n",
            "Epoch 514 Train Loss : 968455.9006347656 Test Loss : 278424276.26666665\n",
            "Epoch 515 Train Loss : 936760.9497680664 Test Loss : 278649322.6666667\n",
            "Epoch 516 Train Loss : 1031530.6474609375 Test Loss : 281411079.46666664\n",
            "Epoch 517 Train Loss : 937927.0593261719 Test Loss : 277988068.26666665\n",
            "Epoch 518 Train Loss : 1034843.2983398438 Test Loss : 290692627.73333335\n",
            "Epoch 519 Train Loss : 961288.6555175781 Test Loss : 281200209.06666666\n",
            "Epoch 520 Train Loss : 1028873.1358642578 Test Loss : 278010385.6\n",
            "Epoch 521 Train Loss : 1295115.8740234375 Test Loss : 345524341.3333333\n",
            "Epoch 522 Train Loss : 1132559.16015625 Test Loss : 282818233.6\n",
            "Epoch 523 Train Loss : 961185.5874023438 Test Loss : 283031702.4\n",
            "Epoch 524 Train Loss : 934529.8195800781 Test Loss : 292557336.0\n",
            "Epoch 525 Train Loss : 918224.1401367188 Test Loss : 280044464.53333336\n",
            "Epoch 526 Train Loss : 1059191.3930664062 Test Loss : 279233261.8666667\n",
            "Epoch 527 Train Loss : 867986.1430664062 Test Loss : 289818060.8\n",
            "Epoch 528 Train Loss : 854496.8251342773 Test Loss : 285521037.8666667\n",
            "Epoch 529 Train Loss : 904006.6235351562 Test Loss : 279760273.6\n",
            "Epoch 530 Train Loss : 948711.7875976562 Test Loss : 307683570.1333333\n",
            "Epoch 531 Train Loss : 906447.4348144531 Test Loss : 279802607.46666664\n",
            "Epoch 532 Train Loss : 910406.4721679688 Test Loss : 299513106.1333333\n",
            "Epoch 533 Train Loss : 823694.119140625 Test Loss : 277579276.8\n",
            "Epoch 534 Train Loss : 929131.3544921875 Test Loss : 279050284.8\n",
            "Epoch 535 Train Loss : 850344.3159179688 Test Loss : 287523525.3333333\n",
            "Epoch 536 Train Loss : 827693.6807861328 Test Loss : 287496890.1333333\n",
            "Epoch 537 Train Loss : 804182.2580566406 Test Loss : 283927338.6666667\n",
            "Epoch 538 Train Loss : 801848.3237304688 Test Loss : 294251172.8\n",
            "Epoch 539 Train Loss : 838282.236328125 Test Loss : 291619162.6666667\n",
            "Epoch 540 Train Loss : 750087.7658691406 Test Loss : 283124591.46666664\n",
            "Epoch 541 Train Loss : 739209.0319824219 Test Loss : 293520308.26666665\n",
            "Epoch 542 Train Loss : 756391.9797363281 Test Loss : 353653460.8\n",
            "Epoch 543 Train Loss : 810937.7971191406 Test Loss : 292735024.0\n",
            "Epoch 544 Train Loss : 722290.1440429688 Test Loss : 280937298.1333333\n",
            "Epoch 545 Train Loss : 710592.5935058594 Test Loss : 286582757.8666667\n",
            "Epoch 546 Train Loss : 662850.5537109375 Test Loss : 284933243.2\n",
            "Epoch 547 Train Loss : 772169.2504882812 Test Loss : 293331312.0\n",
            "Epoch 548 Train Loss : 790026.6462402344 Test Loss : 288814378.6666667\n",
            "Epoch 549 Train Loss : 889459.6176757812 Test Loss : 282207741.6\n",
            "Epoch 550 Train Loss : 715774.3527832031 Test Loss : 286481507.2\n",
            "Epoch 551 Train Loss : 737536.9421386719 Test Loss : 348241949.8666667\n",
            "Epoch 552 Train Loss : 716624.2286376953 Test Loss : 288481300.26666665\n",
            "Epoch 553 Train Loss : 733451.49609375 Test Loss : 290847829.3333333\n",
            "Epoch 554 Train Loss : 742477.2204589844 Test Loss : 289286601.6\n",
            "Epoch 555 Train Loss : 686866.6433105469 Test Loss : 302220951.46666664\n",
            "Epoch 556 Train Loss : 704239.5844726562 Test Loss : 282282096.8\n",
            "Epoch 557 Train Loss : 893011.9699707031 Test Loss : 300840610.1333333\n",
            "Epoch 558 Train Loss : 904179.4340820312 Test Loss : 288571904.0\n",
            "Epoch 559 Train Loss : 795073.8371582031 Test Loss : 293228989.8666667\n",
            "Epoch 560 Train Loss : 1092427.1225585938 Test Loss : 308319429.3333333\n",
            "Epoch 561 Train Loss : 714100.6828613281 Test Loss : 297703060.26666665\n",
            "Epoch 562 Train Loss : 619741.6442871094 Test Loss : 295636404.8\n",
            "Epoch 563 Train Loss : 597233.8781738281 Test Loss : 285146386.6666667\n",
            "Epoch 564 Train Loss : 586180.4262695312 Test Loss : 296875586.6666667\n",
            "Epoch 565 Train Loss : 581387.4094238281 Test Loss : 288364295.46666664\n",
            "Epoch 566 Train Loss : 606876.0913085938 Test Loss : 287657840.0\n",
            "Epoch 567 Train Loss : 565772.1743164062 Test Loss : 284809421.8666667\n",
            "Epoch 568 Train Loss : 541738.4670410156 Test Loss : 292450079.46666664\n",
            "Epoch 569 Train Loss : 534897.8991699219 Test Loss : 297315297.06666666\n",
            "Epoch 570 Train Loss : 588132.9047851562 Test Loss : 289077542.4\n",
            "Epoch 571 Train Loss : 558415.1284179688 Test Loss : 287939046.4\n",
            "Epoch 572 Train Loss : 497817.1218261719 Test Loss : 300132650.1333333\n",
            "Epoch 573 Train Loss : 475399.9216308594 Test Loss : 311866322.6666667\n",
            "Epoch 574 Train Loss : 515329.6384277344 Test Loss : 295524668.26666665\n",
            "Epoch 575 Train Loss : 515875.34533691406 Test Loss : 293359142.4\n",
            "Epoch 576 Train Loss : 522269.53857421875 Test Loss : 306274161.06666666\n",
            "Epoch 577 Train Loss : 509253.3310546875 Test Loss : 296423373.8666667\n",
            "Epoch 578 Train Loss : 469411.60888671875 Test Loss : 294300769.06666666\n",
            "Epoch 579 Train Loss : 488828.36572265625 Test Loss : 292625518.4\n",
            "Epoch 580 Train Loss : 482431.2204589844 Test Loss : 293856331.2\n",
            "Epoch 581 Train Loss : 454122.31036376953 Test Loss : 304420007.46666664\n",
            "Epoch 582 Train Loss : 524681.5354003906 Test Loss : 304043785.6\n",
            "Epoch 583 Train Loss : 563049.8310546875 Test Loss : 293640507.73333335\n",
            "Epoch 584 Train Loss : 854286.6655273438 Test Loss : 296677009.06666666\n",
            "Epoch 585 Train Loss : 540286.8659667969 Test Loss : 293502016.0\n",
            "Epoch 586 Train Loss : 476522.7595214844 Test Loss : 300697049.6\n",
            "Epoch 587 Train Loss : 430399.22119140625 Test Loss : 294216298.6666667\n",
            "Epoch 588 Train Loss : 494028.4033203125 Test Loss : 298380124.8\n",
            "Epoch 589 Train Loss : 503683.45446777344 Test Loss : 297483238.4\n",
            "Epoch 590 Train Loss : 425166.6130371094 Test Loss : 292993441.06666666\n",
            "Epoch 591 Train Loss : 462944.3522949219 Test Loss : 290706353.6\n",
            "Epoch 592 Train Loss : 481968.916015625 Test Loss : 291601478.4\n",
            "Epoch 593 Train Loss : 416019.45849609375 Test Loss : 301721897.06666666\n",
            "Epoch 594 Train Loss : 458508.1110839844 Test Loss : 294102252.26666665\n",
            "Epoch 595 Train Loss : 406129.9267578125 Test Loss : 293662064.0\n",
            "Epoch 596 Train Loss : 402322.0908203125 Test Loss : 297308810.6666667\n",
            "Epoch 597 Train Loss : 385806.5227050781 Test Loss : 308041138.1333333\n",
            "Epoch 598 Train Loss : 390067.63232421875 Test Loss : 300643255.46666664\n",
            "Epoch 599 Train Loss : 374142.76818847656 Test Loss : 296443490.1333333\n",
            "Epoch 600 Train Loss : 398699.42126464844 Test Loss : 292452228.0\n",
            "Epoch 601 Train Loss : 383032.5732421875 Test Loss : 294695694.93333334\n",
            "Epoch 602 Train Loss : 468032.34423828125 Test Loss : 297306071.46666664\n",
            "Epoch 603 Train Loss : 642294.93359375 Test Loss : 304064448.0\n",
            "Epoch 604 Train Loss : 395514.5895996094 Test Loss : 299015989.3333333\n",
            "Epoch 605 Train Loss : 374455.52795410156 Test Loss : 306136332.8\n",
            "Epoch 606 Train Loss : 408953.1804199219 Test Loss : 297389537.06666666\n",
            "Epoch 607 Train Loss : 478831.4875488281 Test Loss : 298145986.1333333\n",
            "Epoch 608 Train Loss : 450848.6647949219 Test Loss : 326003954.6666667\n",
            "Epoch 609 Train Loss : 455556.6046142578 Test Loss : 326622046.4\n",
            "Epoch 610 Train Loss : 413603.4782714844 Test Loss : 295308331.73333335\n",
            "Epoch 611 Train Loss : 459020.1486816406 Test Loss : 303057541.3333333\n",
            "Epoch 612 Train Loss : 634221.3737792969 Test Loss : 301792944.0\n",
            "Epoch 613 Train Loss : 581234.0368652344 Test Loss : 324590417.06666666\n",
            "Epoch 614 Train Loss : 428792.0275878906 Test Loss : 306168330.6666667\n",
            "Epoch 615 Train Loss : 424452.2561035156 Test Loss : 296051354.1333333\n",
            "Epoch 616 Train Loss : 385406.98779296875 Test Loss : 301058501.3333333\n",
            "Epoch 617 Train Loss : 322635.71154785156 Test Loss : 295583083.73333335\n",
            "Epoch 618 Train Loss : 318939.06677246094 Test Loss : 311468200.53333336\n",
            "Epoch 619 Train Loss : 378007.98974609375 Test Loss : 301080258.1333333\n",
            "Epoch 620 Train Loss : 328413.4455566406 Test Loss : 300295670.93333334\n",
            "Epoch 621 Train Loss : 315657.0764160156 Test Loss : 305127162.6666667\n",
            "Epoch 622 Train Loss : 276799.8992919922 Test Loss : 303740545.06666666\n",
            "Epoch 623 Train Loss : 295794.88037109375 Test Loss : 306770604.26666665\n",
            "Epoch 624 Train Loss : 390995.2541503906 Test Loss : 310588951.46666664\n",
            "Epoch 625 Train Loss : 324043.1444091797 Test Loss : 298201922.1333333\n",
            "Epoch 626 Train Loss : 307248.36474609375 Test Loss : 301506020.26666665\n",
            "Epoch 627 Train Loss : 442216.2391357422 Test Loss : 367880354.6666667\n",
            "Epoch 628 Train Loss : 380101.140625 Test Loss : 304269596.8\n",
            "Epoch 629 Train Loss : 343062.6151123047 Test Loss : 300682776.53333336\n",
            "Epoch 630 Train Loss : 361410.3662109375 Test Loss : 300055301.8666667\n",
            "Epoch 631 Train Loss : 305064.0096435547 Test Loss : 297700982.4\n",
            "Epoch 632 Train Loss : 309253.1379394531 Test Loss : 302746032.0\n",
            "Epoch 633 Train Loss : 368894.77490234375 Test Loss : 300534556.8\n",
            "Epoch 634 Train Loss : 335369.29248046875 Test Loss : 302579483.73333335\n",
            "Epoch 635 Train Loss : 322038.6484375 Test Loss : 297739814.4\n",
            "Epoch 636 Train Loss : 324814.2548828125 Test Loss : 304899756.8\n",
            "Epoch 637 Train Loss : 398333.0927734375 Test Loss : 300443472.0\n",
            "Epoch 638 Train Loss : 363918.2170410156 Test Loss : 324709095.46666664\n",
            "Epoch 639 Train Loss : 407057.3684082031 Test Loss : 314887565.8666667\n",
            "Epoch 640 Train Loss : 314411.8386230469 Test Loss : 299195584.53333336\n",
            "Epoch 641 Train Loss : 293880.92248535156 Test Loss : 315801317.3333333\n",
            "Epoch 642 Train Loss : 264286.7048339844 Test Loss : 310504337.6\n",
            "Epoch 643 Train Loss : 245912.3416748047 Test Loss : 312689460.26666665\n",
            "Epoch 644 Train Loss : 271242.4564819336 Test Loss : 300588491.2\n",
            "Epoch 645 Train Loss : 289004.68884277344 Test Loss : 307577474.1333333\n",
            "Epoch 646 Train Loss : 295831.2275390625 Test Loss : 305555142.4\n",
            "Epoch 647 Train Loss : 288845.4774169922 Test Loss : 309062260.26666665\n",
            "Epoch 648 Train Loss : 258154.40209960938 Test Loss : 308966664.53333336\n",
            "Epoch 649 Train Loss : 260357.71411132812 Test Loss : 305117163.73333335\n",
            "Epoch 650 Train Loss : 230029.35400390625 Test Loss : 302806626.1333333\n",
            "Epoch 651 Train Loss : 235703.69873046875 Test Loss : 309849469.8666667\n",
            "Epoch 652 Train Loss : 303013.9382324219 Test Loss : 306189562.1333333\n",
            "Epoch 653 Train Loss : 931885.2927246094 Test Loss : 301293513.6\n",
            "Epoch 654 Train Loss : 525466.3420410156 Test Loss : 303876032.0\n",
            "Epoch 655 Train Loss : 290793.37060546875 Test Loss : 304595644.8\n",
            "Epoch 656 Train Loss : 240055.2108154297 Test Loss : 306233456.0\n",
            "Epoch 657 Train Loss : 287360.8994140625 Test Loss : 303294041.6\n",
            "Epoch 658 Train Loss : 211719.45080566406 Test Loss : 308500336.0\n",
            "Epoch 659 Train Loss : 171931.0126953125 Test Loss : 308521785.6\n",
            "Epoch 660 Train Loss : 195133.31762695312 Test Loss : 306007809.06666666\n",
            "Epoch 661 Train Loss : 279387.171875 Test Loss : 301915755.73333335\n",
            "Epoch 662 Train Loss : 293750.0479736328 Test Loss : 309518074.6666667\n",
            "Epoch 663 Train Loss : 196807.75366210938 Test Loss : 310161310.93333334\n",
            "Epoch 664 Train Loss : 178141.86627197266 Test Loss : 313375655.46666664\n",
            "Epoch 665 Train Loss : 214338.22192382812 Test Loss : 312806877.8666667\n",
            "Epoch 666 Train Loss : 182595.57885742188 Test Loss : 330209141.8666667\n",
            "Epoch 667 Train Loss : 175669.0535888672 Test Loss : 316891896.53333336\n",
            "Epoch 668 Train Loss : 175960.14489746094 Test Loss : 326001324.8\n",
            "Epoch 669 Train Loss : 175784.99279785156 Test Loss : 323680825.6\n",
            "Epoch 670 Train Loss : 180314.02868652344 Test Loss : 304621469.8666667\n",
            "Epoch 671 Train Loss : 246214.24145507812 Test Loss : 311707048.53333336\n",
            "Epoch 672 Train Loss : 226203.58813476562 Test Loss : 304436366.93333334\n",
            "Epoch 673 Train Loss : 243426.1981201172 Test Loss : 315851837.8666667\n",
            "Epoch 674 Train Loss : 267629.34130859375 Test Loss : 320273586.1333333\n",
            "Epoch 675 Train Loss : 284641.10986328125 Test Loss : 311708930.1333333\n",
            "Epoch 676 Train Loss : 315793.4787597656 Test Loss : 308574260.8\n",
            "Epoch 677 Train Loss : 245700.35052490234 Test Loss : 318457623.46666664\n",
            "Epoch 678 Train Loss : 196517.6346435547 Test Loss : 313454525.3333333\n",
            "Epoch 679 Train Loss : 225184.52978515625 Test Loss : 305226774.1333333\n",
            "Epoch 680 Train Loss : 209444.2208251953 Test Loss : 310501159.46666664\n",
            "Epoch 681 Train Loss : 186021.38537597656 Test Loss : 313049606.4\n",
            "Epoch 682 Train Loss : 178145.7354736328 Test Loss : 310144691.73333335\n",
            "Epoch 683 Train Loss : 217087.73754882812 Test Loss : 321024075.73333335\n",
            "Epoch 684 Train Loss : 268376.44860839844 Test Loss : 306819196.8\n",
            "Epoch 685 Train Loss : 256468.94360351562 Test Loss : 310922107.2\n",
            "Epoch 686 Train Loss : 238074.45703125 Test Loss : 321288654.93333334\n",
            "Epoch 687 Train Loss : 204006.48852539062 Test Loss : 345271440.0\n",
            "Epoch 688 Train Loss : 211117.294921875 Test Loss : 335543866.6666667\n",
            "Epoch 689 Train Loss : 249844.48291015625 Test Loss : 315102059.73333335\n",
            "Epoch 690 Train Loss : 206765.72583007812 Test Loss : 336690819.73333335\n",
            "Epoch 691 Train Loss : 264612.8977050781 Test Loss : 319973936.0\n",
            "Epoch 692 Train Loss : 190863.49584960938 Test Loss : 327869188.26666665\n",
            "Epoch 693 Train Loss : 172774.58618164062 Test Loss : 336465598.4\n",
            "Epoch 694 Train Loss : 203363.20043945312 Test Loss : 309082030.93333334\n",
            "Epoch 695 Train Loss : 209904.83544921875 Test Loss : 314358187.2\n",
            "Epoch 696 Train Loss : 253498.46557617188 Test Loss : 310779632.53333336\n",
            "Epoch 697 Train Loss : 225340.17333984375 Test Loss : 325649902.93333334\n",
            "Epoch 698 Train Loss : 220229.4794921875 Test Loss : 331748354.1333333\n",
            "Epoch 699 Train Loss : 202423.275390625 Test Loss : 336347171.73333335\n",
            "Epoch 700 Train Loss : 176252.509765625 Test Loss : 313276469.3333333\n",
            "Epoch 701 Train Loss : 203350.41528320312 Test Loss : 318508737.6\n",
            "Epoch 702 Train Loss : 349515.9885864258 Test Loss : 310483214.93333334\n",
            "Epoch 703 Train Loss : 536816.5815429688 Test Loss : 306236596.26666665\n",
            "Epoch 704 Train Loss : 264973.39807128906 Test Loss : 318984394.6666667\n",
            "Epoch 705 Train Loss : 164872.31884765625 Test Loss : 318279289.6\n",
            "Epoch 706 Train Loss : 121230.96313476562 Test Loss : 312903891.2\n",
            "Epoch 707 Train Loss : 100059.64569091797 Test Loss : 319264949.3333333\n",
            "Epoch 708 Train Loss : 104563.64404296875 Test Loss : 310731814.4\n",
            "Epoch 709 Train Loss : 117488.61853027344 Test Loss : 314139214.93333334\n",
            "Epoch 710 Train Loss : 269897.5424194336 Test Loss : 318642973.3333333\n",
            "Epoch 711 Train Loss : 108710.18786621094 Test Loss : 310873787.73333335\n",
            "Epoch 712 Train Loss : 114444.56784057617 Test Loss : 313330099.2\n",
            "Epoch 713 Train Loss : 131834.19647216797 Test Loss : 313627112.53333336\n",
            "Epoch 714 Train Loss : 140774.54931640625 Test Loss : 322619607.46666664\n",
            "Epoch 715 Train Loss : 158110.0118408203 Test Loss : 319935292.8\n",
            "Epoch 716 Train Loss : 132330.52325439453 Test Loss : 331618273.06666666\n",
            "Epoch 717 Train Loss : 120247.90856933594 Test Loss : 322412931.2\n",
            "Epoch 718 Train Loss : 130016.09338378906 Test Loss : 311693202.1333333\n",
            "Epoch 719 Train Loss : 148248.5802001953 Test Loss : 312505951.46666664\n",
            "Epoch 720 Train Loss : 137501.2344970703 Test Loss : 321417379.2\n",
            "Epoch 721 Train Loss : 125265.05383300781 Test Loss : 310561966.93333334\n",
            "Epoch 722 Train Loss : 110690.62799072266 Test Loss : 323889011.2\n",
            "Epoch 723 Train Loss : 114529.88256835938 Test Loss : 316607802.6666667\n",
            "Epoch 724 Train Loss : 102538.42712402344 Test Loss : 313435605.3333333\n",
            "Epoch 725 Train Loss : 109977.95587158203 Test Loss : 317779099.2\n",
            "Epoch 726 Train Loss : 101317.23193359375 Test Loss : 313221498.6666667\n",
            "Epoch 727 Train Loss : 111464.41943359375 Test Loss : 312803939.2\n",
            "Epoch 728 Train Loss : 120831.59936523438 Test Loss : 327702646.4\n",
            "Epoch 729 Train Loss : 119747.9228515625 Test Loss : 311735387.73333335\n",
            "Epoch 730 Train Loss : 138352.64770507812 Test Loss : 311261730.6666667\n",
            "Epoch 731 Train Loss : 170314.30004882812 Test Loss : 311456277.3333333\n",
            "Epoch 732 Train Loss : 172426.7677001953 Test Loss : 338169221.8666667\n",
            "Epoch 733 Train Loss : 146513.7449951172 Test Loss : 333461849.6\n",
            "Epoch 734 Train Loss : 221451.4825439453 Test Loss : 321518947.2\n",
            "Epoch 735 Train Loss : 244041.91186523438 Test Loss : 320590898.1333333\n",
            "Epoch 736 Train Loss : 260132.90393066406 Test Loss : 322855497.6\n",
            "Epoch 737 Train Loss : 232780.36853027344 Test Loss : 314807835.2\n",
            "Epoch 738 Train Loss : 229212.8310546875 Test Loss : 315355026.1333333\n",
            "Epoch 739 Train Loss : 247790.02307128906 Test Loss : 318119912.53333336\n",
            "Epoch 740 Train Loss : 193567.82196044922 Test Loss : 311780385.06666666\n",
            "Epoch 741 Train Loss : 183623.0601196289 Test Loss : 324933389.8666667\n",
            "Epoch 742 Train Loss : 227360.087890625 Test Loss : 328562237.8666667\n",
            "Epoch 743 Train Loss : 206351.67639160156 Test Loss : 315810184.53333336\n",
            "Epoch 744 Train Loss : 201508.5965576172 Test Loss : 314375901.8666667\n",
            "Epoch 745 Train Loss : 199320.8953857422 Test Loss : 317235749.3333333\n",
            "Epoch 746 Train Loss : 242930.96142578125 Test Loss : 311106368.0\n",
            "Epoch 747 Train Loss : 182480.02124023438 Test Loss : 319382039.46666664\n",
            "Epoch 748 Train Loss : 157352.7080078125 Test Loss : 313774914.1333333\n",
            "Epoch 749 Train Loss : 162969.2944946289 Test Loss : 335257233.06666666\n",
            "Epoch 750 Train Loss : 180075.5955810547 Test Loss : 313641264.0\n",
            "Epoch 751 Train Loss : 199646.67626953125 Test Loss : 317293996.8\n",
            "Epoch 752 Train Loss : 146281.2667236328 Test Loss : 314888451.73333335\n",
            "Epoch 753 Train Loss : 109593.80639648438 Test Loss : 314720056.53333336\n",
            "Epoch 754 Train Loss : 118936.00799560547 Test Loss : 324434295.46666664\n",
            "Epoch 755 Train Loss : 116184.54895019531 Test Loss : 320551255.46666664\n",
            "Epoch 756 Train Loss : 119150.56719970703 Test Loss : 322368088.0\n",
            "Epoch 757 Train Loss : 127854.23645019531 Test Loss : 316172004.26666665\n",
            "Epoch 758 Train Loss : 120743.54388427734 Test Loss : 314673670.4\n",
            "Epoch 759 Train Loss : 92543.98388671875 Test Loss : 324161897.6\n",
            "Epoch 760 Train Loss : 101534.8374633789 Test Loss : 327291583.46666664\n",
            "Epoch 761 Train Loss : 93159.36145019531 Test Loss : 321819305.6\n",
            "Epoch 762 Train Loss : 91867.99291992188 Test Loss : 314570065.06666666\n",
            "Epoch 763 Train Loss : 106753.34228515625 Test Loss : 331354025.6\n",
            "Epoch 764 Train Loss : 144643.0037841797 Test Loss : 321645207.46666664\n",
            "Epoch 765 Train Loss : 116514.859375 Test Loss : 322364508.26666665\n",
            "Epoch 766 Train Loss : 177931.05078125 Test Loss : 318138345.6\n",
            "Epoch 767 Train Loss : 244205.14416503906 Test Loss : 319610645.3333333\n",
            "Epoch 768 Train Loss : 785203.7767333984 Test Loss : 320823689.6\n",
            "Epoch 769 Train Loss : 399424.7907714844 Test Loss : 336144864.0\n",
            "Epoch 770 Train Loss : 595793.5576171875 Test Loss : 316100659.73333335\n",
            "Epoch 771 Train Loss : 217402.45947265625 Test Loss : 317107934.93333334\n",
            "Epoch 772 Train Loss : 185390.15649414062 Test Loss : 318686701.8666667\n",
            "Epoch 773 Train Loss : 188047.80517578125 Test Loss : 327873784.53333336\n",
            "Epoch 774 Train Loss : 118661.48712158203 Test Loss : 318579591.46666664\n",
            "Epoch 775 Train Loss : 95367.64575195312 Test Loss : 315011188.26666665\n",
            "Epoch 776 Train Loss : 92755.69946289062 Test Loss : 321705747.2\n",
            "Epoch 777 Train Loss : 77182.16674804688 Test Loss : 318138615.46666664\n",
            "Epoch 778 Train Loss : 70572.30319213867 Test Loss : 324154149.3333333\n",
            "Epoch 779 Train Loss : 77194.24118041992 Test Loss : 323459445.8666667\n",
            "Epoch 780 Train Loss : 83394.01611328125 Test Loss : 314827944.53333336\n",
            "Epoch 781 Train Loss : 108567.04193115234 Test Loss : 320429489.06666666\n",
            "Epoch 782 Train Loss : 135205.02349853516 Test Loss : 319247072.0\n",
            "Epoch 783 Train Loss : 324864.40625 Test Loss : 349436116.26666665\n",
            "Epoch 784 Train Loss : 255330.35083007812 Test Loss : 313141521.6\n",
            "Epoch 785 Train Loss : 203636.8699951172 Test Loss : 328216578.1333333\n",
            "Epoch 786 Train Loss : 215202.40795898438 Test Loss : 320549395.2\n",
            "Epoch 787 Train Loss : 197418.70068359375 Test Loss : 323325834.6666667\n",
            "Epoch 788 Train Loss : 151005.0421142578 Test Loss : 316768481.06666666\n",
            "Epoch 789 Train Loss : 139741.09936523438 Test Loss : 329967622.4\n",
            "Epoch 790 Train Loss : 147715.8529663086 Test Loss : 317151700.26666665\n",
            "Epoch 791 Train Loss : 261526.6834716797 Test Loss : 319594563.2\n",
            "Epoch 792 Train Loss : 197176.97326660156 Test Loss : 322853328.0\n",
            "Epoch 793 Train Loss : 141494.84240722656 Test Loss : 321747253.3333333\n",
            "Epoch 794 Train Loss : 107741.4175415039 Test Loss : 318886724.26666665\n",
            "Epoch 795 Train Loss : 125946.31372070312 Test Loss : 331807250.1333333\n",
            "Epoch 796 Train Loss : 96229.38354492188 Test Loss : 316247363.2\n",
            "Epoch 797 Train Loss : 64252.48501586914 Test Loss : 315145395.2\n",
            "Epoch 798 Train Loss : 56303.35232543945 Test Loss : 335469092.26666665\n",
            "Epoch 799 Train Loss : 52999.19348144531 Test Loss : 327115960.53333336\n",
            "Epoch 800 Train Loss : 49750.16485595703 Test Loss : 328227011.2\n",
            "Epoch 801 Train Loss : 59014.948791503906 Test Loss : 323365928.53333336\n",
            "Epoch 802 Train Loss : 62774.39031982422 Test Loss : 349786606.93333334\n",
            "Epoch 803 Train Loss : 83953.58541870117 Test Loss : 325325833.6\n",
            "Epoch 804 Train Loss : 49087.18914794922 Test Loss : 326841568.0\n",
            "Epoch 805 Train Loss : 58966.53643798828 Test Loss : 321125781.8666667\n",
            "Epoch 806 Train Loss : 56682.24542236328 Test Loss : 315385194.6666667\n",
            "Epoch 807 Train Loss : 76333.32727050781 Test Loss : 396112995.2\n",
            "Epoch 808 Train Loss : 102734.80017089844 Test Loss : 327553312.0\n",
            "Epoch 809 Train Loss : 97037.53552246094 Test Loss : 327229308.8\n",
            "Epoch 810 Train Loss : 84181.83898925781 Test Loss : 316055653.3333333\n",
            "Epoch 811 Train Loss : 76987.76245117188 Test Loss : 326265253.3333333\n",
            "Epoch 812 Train Loss : 74500.1948852539 Test Loss : 316027878.4\n",
            "Epoch 813 Train Loss : 142844.32055664062 Test Loss : 321398162.1333333\n",
            "Epoch 814 Train Loss : 162751.60180664062 Test Loss : 325671720.53333336\n",
            "Epoch 815 Train Loss : 155809.80834960938 Test Loss : 323441925.3333333\n",
            "Epoch 816 Train Loss : 131715.97106933594 Test Loss : 392031481.6\n",
            "Epoch 817 Train Loss : 136484.05932617188 Test Loss : 325283038.93333334\n",
            "Epoch 818 Train Loss : 171760.3525390625 Test Loss : 329426092.8\n",
            "Epoch 819 Train Loss : 248470.42321777344 Test Loss : 393344869.3333333\n",
            "Epoch 820 Train Loss : 311623.26708984375 Test Loss : 317931443.2\n",
            "Epoch 821 Train Loss : 318749.6931152344 Test Loss : 320855953.06666666\n",
            "Epoch 822 Train Loss : 326726.2421875 Test Loss : 319875724.8\n",
            "Epoch 823 Train Loss : 311579.63818359375 Test Loss : 338440153.06666666\n",
            "Epoch 824 Train Loss : 205880.50537109375 Test Loss : 320448496.0\n",
            "Epoch 825 Train Loss : 155341.2452392578 Test Loss : 404178403.2\n",
            "Epoch 826 Train Loss : 97782.3970336914 Test Loss : 321189784.53333336\n",
            "Epoch 827 Train Loss : 76064.84460449219 Test Loss : 320314304.0\n",
            "Epoch 828 Train Loss : 47551.974182128906 Test Loss : 319209610.6666667\n",
            "Epoch 829 Train Loss : 31563.78286743164 Test Loss : 316349762.6666667\n",
            "Epoch 830 Train Loss : 32018.477340698242 Test Loss : 316007453.8666667\n",
            "Epoch 831 Train Loss : 28845.55451965332 Test Loss : 319327302.4\n",
            "Epoch 832 Train Loss : 32537.473907470703 Test Loss : 323380541.3333333\n",
            "Epoch 833 Train Loss : 50149.62515258789 Test Loss : 332645491.2\n",
            "Epoch 834 Train Loss : 60635.948974609375 Test Loss : 332430764.26666665\n",
            "Epoch 835 Train Loss : 59089.09582519531 Test Loss : 316882105.06666666\n",
            "Epoch 836 Train Loss : 60420.454193115234 Test Loss : 333273140.26666665\n",
            "Epoch 837 Train Loss : 59840.40335083008 Test Loss : 321020490.6666667\n",
            "Epoch 838 Train Loss : 59106.942321777344 Test Loss : 336259811.2\n",
            "Epoch 839 Train Loss : 74930.90716552734 Test Loss : 321852114.1333333\n",
            "Epoch 840 Train Loss : 88996.98846435547 Test Loss : 318340724.26666665\n",
            "Epoch 841 Train Loss : 119464.04681396484 Test Loss : 325690563.2\n",
            "Epoch 842 Train Loss : 165666.2755126953 Test Loss : 321896597.3333333\n",
            "Epoch 843 Train Loss : 119691.34436035156 Test Loss : 318173312.53333336\n",
            "Epoch 844 Train Loss : 173129.1993408203 Test Loss : 325232531.2\n",
            "Epoch 845 Train Loss : 228327.94262695312 Test Loss : 326877188.26666665\n",
            "Epoch 846 Train Loss : 200623.46520996094 Test Loss : 325350754.1333333\n",
            "Epoch 847 Train Loss : 175816.00329589844 Test Loss : 329629280.0\n",
            "Epoch 848 Train Loss : 181795.05102539062 Test Loss : 323756596.26666665\n",
            "Epoch 849 Train Loss : 134420.15551757812 Test Loss : 323542123.73333335\n",
            "Epoch 850 Train Loss : 117452.11212158203 Test Loss : 322875829.3333333\n",
            "Epoch 851 Train Loss : 108143.96362304688 Test Loss : 318261032.0\n",
            "Epoch 852 Train Loss : 86249.4931640625 Test Loss : 319034429.8666667\n",
            "Epoch 853 Train Loss : 122527.38464355469 Test Loss : 326172369.06666666\n",
            "Epoch 854 Train Loss : 126977.2622680664 Test Loss : 321774705.06666666\n",
            "Epoch 855 Train Loss : 112695.56079101562 Test Loss : 329030953.6\n",
            "Epoch 856 Train Loss : 110222.57678222656 Test Loss : 327309558.4\n",
            "Epoch 857 Train Loss : 171894.33325195312 Test Loss : 399973270.93333334\n",
            "Epoch 858 Train Loss : 154540.62231445312 Test Loss : 323942776.53333336\n",
            "Epoch 859 Train Loss : 148031.14794921875 Test Loss : 320568618.1333333\n",
            "Epoch 860 Train Loss : 150156.75549316406 Test Loss : 323844188.8\n",
            "Epoch 861 Train Loss : 128146.39672851562 Test Loss : 323472693.3333333\n",
            "Epoch 862 Train Loss : 110837.84649658203 Test Loss : 326324496.0\n",
            "Epoch 863 Train Loss : 106768.20532226562 Test Loss : 319020139.73333335\n",
            "Epoch 864 Train Loss : 105618.48547363281 Test Loss : 319286422.4\n",
            "Epoch 865 Train Loss : 96600.95721435547 Test Loss : 340393433.6\n",
            "Epoch 866 Train Loss : 75238.46905517578 Test Loss : 329267510.4\n",
            "Epoch 867 Train Loss : 78524.6771850586 Test Loss : 319245588.26666665\n",
            "Epoch 868 Train Loss : 95291.10723876953 Test Loss : 321910337.06666666\n",
            "Epoch 869 Train Loss : 128991.45434570312 Test Loss : 321232864.53333336\n",
            "Epoch 870 Train Loss : 177908.4041748047 Test Loss : 409965134.93333334\n",
            "Epoch 871 Train Loss : 205998.65356445312 Test Loss : 389965924.8\n",
            "Epoch 872 Train Loss : 209861.27416992188 Test Loss : 324109086.93333334\n",
            "Epoch 873 Train Loss : 218063.27209472656 Test Loss : 323951602.1333333\n",
            "Epoch 874 Train Loss : 344323.05603027344 Test Loss : 321725352.53333336\n",
            "Epoch 875 Train Loss : 401200.61474609375 Test Loss : 319441330.1333333\n",
            "Epoch 876 Train Loss : 329668.7042236328 Test Loss : 328160957.3333333\n",
            "Epoch 877 Train Loss : 286357.0939941406 Test Loss : 397176821.3333333\n",
            "Epoch 878 Train Loss : 185276.1436767578 Test Loss : 318192863.46666664\n",
            "Epoch 879 Train Loss : 122762.75036621094 Test Loss : 346013280.0\n",
            "Epoch 880 Train Loss : 102405.46368408203 Test Loss : 324453008.0\n",
            "Epoch 881 Train Loss : 80101.66436767578 Test Loss : 316917857.06666666\n",
            "Epoch 882 Train Loss : 70093.0514831543 Test Loss : 321454564.26666665\n",
            "Epoch 883 Train Loss : 55022.22573852539 Test Loss : 316802531.73333335\n",
            "Epoch 884 Train Loss : 47327.54528808594 Test Loss : 324889620.26666665\n",
            "Epoch 885 Train Loss : 46414.48794555664 Test Loss : 325637834.6666667\n",
            "Epoch 886 Train Loss : 47109.41586303711 Test Loss : 322302979.2\n",
            "Epoch 887 Train Loss : 72203.34252929688 Test Loss : 324757630.4\n",
            "Epoch 888 Train Loss : 76131.72521972656 Test Loss : 321832235.2\n",
            "Epoch 889 Train Loss : 83535.17877197266 Test Loss : 317145290.6666667\n",
            "Epoch 890 Train Loss : 116710.78146362305 Test Loss : 326979936.0\n",
            "Epoch 891 Train Loss : 125262.08102416992 Test Loss : 316389725.8666667\n",
            "Epoch 892 Train Loss : 171659.16748046875 Test Loss : 324502505.06666666\n",
            "Epoch 893 Train Loss : 64682.25231933594 Test Loss : 326226563.2\n",
            "Epoch 894 Train Loss : 98395.35043334961 Test Loss : 398689953.06666666\n",
            "Epoch 895 Train Loss : 83818.98791503906 Test Loss : 322622886.4\n",
            "Epoch 896 Train Loss : 120221.46853637695 Test Loss : 319791348.26666665\n",
            "Epoch 897 Train Loss : 131049.38842773438 Test Loss : 322745988.8\n",
            "Epoch 898 Train Loss : 107513.5835571289 Test Loss : 315620358.93333334\n",
            "Epoch 899 Train Loss : 114464.2734375 Test Loss : 323266914.1333333\n",
            "Epoch 900 Train Loss : 135596.93560791016 Test Loss : 324890206.93333334\n",
            "Epoch 901 Train Loss : 186009.26171875 Test Loss : 322609658.6666667\n",
            "Epoch 902 Train Loss : 237507.10803222656 Test Loss : 326488507.73333335\n",
            "Epoch 903 Train Loss : 517821.93310546875 Test Loss : 341883576.53333336\n",
            "Epoch 904 Train Loss : 594007.0805664062 Test Loss : 318038922.6666667\n",
            "Epoch 905 Train Loss : 490062.51708984375 Test Loss : 324565282.1333333\n",
            "Epoch 906 Train Loss : 267353.4987792969 Test Loss : 328732142.93333334\n",
            "Epoch 907 Train Loss : 132484.6346435547 Test Loss : 320114314.6666667\n",
            "Epoch 908 Train Loss : 90525.75384521484 Test Loss : 319153328.0\n",
            "Epoch 909 Train Loss : 67307.3120727539 Test Loss : 324781414.4\n",
            "Epoch 910 Train Loss : 49337.90087890625 Test Loss : 321730234.6666667\n",
            "Epoch 911 Train Loss : 34819.336837768555 Test Loss : 322664801.6\n",
            "Epoch 912 Train Loss : 50387.72430419922 Test Loss : 331053353.6\n",
            "Epoch 913 Train Loss : 63412.779693603516 Test Loss : 326051354.1333333\n",
            "Epoch 914 Train Loss : 58525.957458496094 Test Loss : 321322770.1333333\n",
            "Epoch 915 Train Loss : 75877.83898925781 Test Loss : 328171772.8\n",
            "Epoch 916 Train Loss : 104450.15856933594 Test Loss : 321701636.26666665\n",
            "Epoch 917 Train Loss : 131295.62353515625 Test Loss : 325126159.46666664\n",
            "Epoch 918 Train Loss : 137729.50213623047 Test Loss : 328463982.93333334\n",
            "Epoch 919 Train Loss : 97563.3264465332 Test Loss : 345897365.3333333\n",
            "Epoch 920 Train Loss : 72884.42572021484 Test Loss : 328100200.53333336\n",
            "Epoch 921 Train Loss : 74234.9091796875 Test Loss : 322177193.6\n",
            "Epoch 922 Train Loss : 85667.7730102539 Test Loss : 318320731.73333335\n",
            "Epoch 923 Train Loss : 107225.01361083984 Test Loss : 323695152.0\n",
            "Epoch 924 Train Loss : 81929.0484008789 Test Loss : 320325468.8\n",
            "Epoch 925 Train Loss : 72261.80975341797 Test Loss : 348403260.8\n",
            "Epoch 926 Train Loss : 84684.67584228516 Test Loss : 317948029.3333333\n",
            "Epoch 927 Train Loss : 109905.23510742188 Test Loss : 340480692.26666665\n",
            "Epoch 928 Train Loss : 144012.90161132812 Test Loss : 324305147.73333335\n",
            "Epoch 929 Train Loss : 136025.9259033203 Test Loss : 329142478.4\n",
            "Epoch 930 Train Loss : 145484.45684814453 Test Loss : 320859219.2\n",
            "Epoch 931 Train Loss : 164283.3905029297 Test Loss : 319830242.1333333\n",
            "Epoch 932 Train Loss : 173099.64880371094 Test Loss : 322961360.0\n",
            "Epoch 933 Train Loss : 200014.5643310547 Test Loss : 322852861.3333333\n",
            "Epoch 934 Train Loss : 218343.46826171875 Test Loss : 391819638.4\n",
            "Epoch 935 Train Loss : 233734.14416503906 Test Loss : 327659700.26666665\n",
            "Epoch 936 Train Loss : 270036.9567871094 Test Loss : 320321368.53333336\n",
            "Epoch 937 Train Loss : 217801.84423828125 Test Loss : 319133356.8\n",
            "Epoch 938 Train Loss : 204088.1358642578 Test Loss : 315781404.26666665\n",
            "Epoch 939 Train Loss : 217711.9073486328 Test Loss : 320732030.4\n",
            "Epoch 940 Train Loss : 251082.49267578125 Test Loss : 318883837.3333333\n",
            "Epoch 941 Train Loss : 160033.98596191406 Test Loss : 345213442.6666667\n",
            "Epoch 942 Train Loss : 120359.13317871094 Test Loss : 318829314.6666667\n",
            "Epoch 943 Train Loss : 127708.03869628906 Test Loss : 323311529.6\n",
            "Epoch 944 Train Loss : 126469.66864013672 Test Loss : 318602003.73333335\n",
            "Epoch 945 Train Loss : 110569.9906616211 Test Loss : 326527484.8\n",
            "Epoch 946 Train Loss : 102293.05657958984 Test Loss : 323050608.0\n",
            "Epoch 947 Train Loss : 116556.80169677734 Test Loss : 320799662.93333334\n",
            "Epoch 948 Train Loss : 97499.08374023438 Test Loss : 337649937.06666666\n",
            "Epoch 949 Train Loss : 115946.19519042969 Test Loss : 326603369.6\n",
            "Epoch 950 Train Loss : 126973.67694091797 Test Loss : 348558167.46666664\n",
            "Epoch 951 Train Loss : 121614.3369140625 Test Loss : 317614220.26666665\n",
            "Epoch 952 Train Loss : 177122.02368164062 Test Loss : 326659251.2\n",
            "Epoch 953 Train Loss : 191218.33770751953 Test Loss : 325363571.2\n",
            "Epoch 954 Train Loss : 171414.6064453125 Test Loss : 319489700.26666665\n",
            "Epoch 955 Train Loss : 117990.39422607422 Test Loss : 331891920.0\n",
            "Epoch 956 Train Loss : 96372.93984985352 Test Loss : 328559925.3333333\n",
            "Epoch 957 Train Loss : 116161.51477050781 Test Loss : 329369609.6\n",
            "Epoch 958 Train Loss : 166906.2872314453 Test Loss : 334211699.2\n",
            "Epoch 959 Train Loss : 147845.16284179688 Test Loss : 316470358.93333334\n",
            "Epoch 960 Train Loss : 127919.52758789062 Test Loss : 317296048.0\n",
            "Epoch 961 Train Loss : 86361.57421875 Test Loss : 333290906.6666667\n",
            "Epoch 962 Train Loss : 76992.95281982422 Test Loss : 319319672.53333336\n",
            "Epoch 963 Train Loss : 81472.63525390625 Test Loss : 338516871.46666664\n",
            "Epoch 964 Train Loss : 81636.22888183594 Test Loss : 323825962.1333333\n",
            "Epoch 965 Train Loss : 194017.52557373047 Test Loss : 319225953.6\n",
            "Epoch 966 Train Loss : 958091.5213623047 Test Loss : 320434030.6666667\n",
            "Epoch 967 Train Loss : 243810.119140625 Test Loss : 344539801.06666666\n",
            "Epoch 968 Train Loss : 130500.54516601562 Test Loss : 319154061.8666667\n",
            "Epoch 969 Train Loss : 81470.74481201172 Test Loss : 323877707.73333335\n",
            "Epoch 970 Train Loss : 59543.37139892578 Test Loss : 319781154.1333333\n",
            "Epoch 971 Train Loss : 38857.467681884766 Test Loss : 335806967.46666664\n",
            "Epoch 972 Train Loss : 38198.1953125 Test Loss : 321144957.8666667\n",
            "Epoch 973 Train Loss : 28880.180114746094 Test Loss : 318377665.6\n",
            "Epoch 974 Train Loss : 26408.994079589844 Test Loss : 315676556.1333333\n",
            "Epoch 975 Train Loss : 28965.11489868164 Test Loss : 321635810.1333333\n",
            "Epoch 976 Train Loss : 37383.63360595703 Test Loss : 323291436.8\n",
            "Epoch 977 Train Loss : 65075.03012084961 Test Loss : 322750971.73333335\n",
            "Epoch 978 Train Loss : 195656.1166381836 Test Loss : 348258554.1333333\n",
            "Epoch 979 Train Loss : 254054.84454345703 Test Loss : 340539352.53333336\n",
            "Epoch 980 Train Loss : 205408.15490722656 Test Loss : 321395451.73333335\n",
            "Epoch 981 Train Loss : 131015.01623535156 Test Loss : 319059380.26666665\n",
            "Epoch 982 Train Loss : 91629.93389892578 Test Loss : 322488809.6\n",
            "Epoch 983 Train Loss : 79248.896484375 Test Loss : 324165869.8666667\n",
            "Epoch 984 Train Loss : 66402.83905029297 Test Loss : 323205166.93333334\n",
            "Epoch 985 Train Loss : 63698.92071533203 Test Loss : 319912465.06666666\n",
            "Epoch 986 Train Loss : 68143.29916381836 Test Loss : 324256408.0\n",
            "Epoch 987 Train Loss : 223408.40844726562 Test Loss : 322552394.6666667\n",
            "Epoch 988 Train Loss : 177700.68780517578 Test Loss : 325269402.1333333\n",
            "Epoch 989 Train Loss : 127656.08679199219 Test Loss : 329050362.6666667\n",
            "Epoch 990 Train Loss : 111497.78326416016 Test Loss : 320822175.46666664\n",
            "Epoch 991 Train Loss : 106570.86218261719 Test Loss : 326362183.46666664\n",
            "Epoch 992 Train Loss : 111123.23626708984 Test Loss : 406526109.8666667\n",
            "Epoch 993 Train Loss : 133020.1796875 Test Loss : 333892954.6666667\n",
            "Epoch 994 Train Loss : 132056.41033935547 Test Loss : 327768488.0\n",
            "Epoch 995 Train Loss : 117886.66845703125 Test Loss : 321956478.93333334\n",
            "Epoch 996 Train Loss : 153073.3446044922 Test Loss : 326758600.0\n",
            "Epoch 997 Train Loss : 162913.46307373047 Test Loss : 321582839.46666664\n",
            "Epoch 998 Train Loss : 260993.38159179688 Test Loss : 350040229.3333333\n",
            "Epoch 999 Train Loss : 378034.1696777344 Test Loss : 323474033.06666666\n",
            "Epoch 1000 Train Loss : 1086993.3862304688 Test Loss : 318338773.8666667\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# 학습 횟수 만큼 반복\n",
        "for epoch in range(1000):\n",
        "\n",
        "    # 모델 학습(학습데이터)\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 모델 평가 (평가데이터)\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Train Loss : {train_loss} Test Loss : {test_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4a5d236e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_training_results(train_losses, test_losses)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
