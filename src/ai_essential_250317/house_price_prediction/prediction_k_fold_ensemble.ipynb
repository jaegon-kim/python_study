{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "075f4649",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaegon-kim/python_study/blob/main/src/ai_essential_250317/house_price_prediction/prediction_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C3HNZJH5LuqI",
      "metadata": {
        "id": "C3HNZJH5LuqI"
      },
      "source": [
        "# House Price Prediction - K fold 를 잘 못 사용한 예제\n",
        "- **목표**\n",
        "  - 이 워크샵은 주어진 데이터셋을 이용해 심층신경망 모델을 학습시켜 주택의 최종 판매 가격(SalePrice)을 예측하는 것이 최종 목표입니다.\n",
        "\n",
        "- **데이터셋 정보**\n",
        "  - 데이터셋은 총 79개의 설명 변수와 타겟 변수인 주택 가격(SalePrice)로 구성됩니다.\n",
        "  - 설명 변수는 주택의 다양한 특성(예: 건축 연도, 면적, 위치, 방 개수 등)을 포함합니다.\n",
        "  - 데이터는 판매 가격이 포함된 학습용 데이터인 `X`, `y` 와 판매 가격이 포함되지 않은 평가용 데이터인 `TEST`파일로 나뉘며, 각각 모델 학습 및 평가에 사용됩니다.\n",
        "    - 평가용 데이터 `TEST`의 판매 가격(SalePrice)를 예측 후 리더보드로 제출하여 평가합니다.\n",
        "\n",
        "- **문제 유형**\n",
        "  - 이 워크샵은 회귀 문제로 연속형 변수를 예측하는 것이 목표입니다. 모델의 성능은 `Mean Absolute Error`로 측정됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FerWbWa8ML9S",
      "metadata": {
        "id": "FerWbWa8ML9S"
      },
      "source": [
        "## 1. 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fbebc02c",
      "metadata": {
        "id": "fbebc02c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install JAEN -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2b192c23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b192c23",
        "outputId": "3b67d51c-657f-4350-e45e-7e7982dc84c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sdn/anaconda3/envs/python311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 그대로 실행하세요.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "from JAEN.utils import plot_training_results\n",
        "from torch.utils.data import DataLoader, Subset \n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OSiIE4tdPcSV",
      "metadata": {
        "id": "OSiIE4tdPcSV"
      },
      "source": [
        "## 2. 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Cwo9d1i-ON3u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwo9d1i-ON3u",
        "outputId": "99de5569-2e0f-48da-ea5d-7e0900be621c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1460, 79]), torch.Size([1460, 1]), torch.Size([1459, 79]))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from JAEN.datasets import load_house_price\n",
        "X, y, TEST = load_house_price()\n",
        "X.shape, y.shape, TEST.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55745e97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55745e97",
        "outputId": "d01bacac-ce8d-4817-f220-5b3d7c3b2222"
      },
      "outputs": [],
      "source": [
        "#column_mean = torch.mean(X, dim=0)\n",
        "#print(\"Column Mean:\", column_mean, column_mean.shape)\n",
        "#column_std = torch.std(X, dim=0)\n",
        "#print(\"Column Std:\", column_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f7b6fa57",
      "metadata": {
        "id": "f7b6fa57"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "    for data, labels in train_loader: # 미니 배치 별 파라미터 업데이트 수행\n",
        "        data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "        # 순전파\n",
        "        outputs = model(data)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 손실 누적\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    # - len(train_loader): 평균 Loss\n",
        "    return running_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3bb36d73",
      "metadata": {
        "id": "3bb36d73"
      },
      "outputs": [],
      "source": [
        "# 평가 함수 정의\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "    with torch.no_grad():  # 평가 중에는 기울기 계산을 하지 않음\n",
        "        for data, labels in test_loader: # 미니 배치 별 손실 계산\n",
        "            data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "            # 순전파\n",
        "            outputs = model(data)\n",
        "\n",
        "            # 손실 계산\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 손실 누적\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    return running_loss / len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Ozw8bwM2h-Ya",
      "metadata": {
        "id": "Ozw8bwM2h-Ya"
      },
      "outputs": [],
      "source": [
        "# DNN 모델 구성\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(79, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)  # 10%의 드롭아웃 적용\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.bn2(self.fc2(x)))\n",
        "        #x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "#model = DNN().to(device)\n",
        "#summary(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d2377505",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataLoader 생성\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "#train_loader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True, drop_last=True)\n",
        "dataset = TensorDataset(X, y)\n",
        "k_folds = 5\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef294ad2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Fold 1 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 38,884,970,075.178085 Test Loss : 38,962,954,563.36842\n",
            "Save model Epoch 2 Train Loss : 38,883,493,944.10959 Test Loss : 38,960,828,416.0\n",
            "Save model Epoch 3 Train Loss : 38,881,581,673.20548 Test Loss : 38,958,415,548.63158\n",
            "Save model Epoch 4 Train Loss : 38,878,964,188.9315 Test Loss : 38,956,293,389.47369\n",
            "Save model Epoch 5 Train Loss : 38,875,434,811.61644 Test Loss : 38,952,309,167.1579\n",
            "Save model Epoch 6 Train Loss : 38,871,293,755.61644 Test Loss : 38,947,997,157.052635\n",
            "Save model Epoch 7 Train Loss : 38,865,801,833.20548 Test Loss : 38,941,366,056.42105\n",
            "Save model Epoch 8 Train Loss : 38,860,042,071.671234 Test Loss : 38,934,002,202.947365\n",
            "Save model Epoch 9 Train Loss : 38,853,205,062.136986 Test Loss : 38,927,009,037.47369\n",
            "Save model Epoch 10 Train Loss : 38,845,919,091.72603 Test Loss : 38,922,017,414.73684\n",
            "Save model Epoch 11 Train Loss : 38,837,543,950.0274 Test Loss : 38,911,423,865.26316\n",
            "Save model Epoch 12 Train Loss : 38,829,220,849.9726 Test Loss : 38,906,395,917.47369\n",
            "Save model Epoch 13 Train Loss : 38,819,915,635.72603 Test Loss : 38,899,672,117.89474\n",
            "Save model Epoch 14 Train Loss : 38,808,817,972.60274 Test Loss : 38,890,283,546.947365\n",
            "Save model Epoch 15 Train Loss : 38,798,836,329.20548 Test Loss : 38,873,985,886.31579\n",
            "Save model Epoch 16 Train Loss : 38,787,155,154.41096 Test Loss : 38,860,423,491.36842\n",
            "Save model Epoch 17 Train Loss : 38,775,294,513.095894 Test Loss : 38,852,424,865.68421\n",
            "Save model Epoch 18 Train Loss : 38,762,146,465.31507 Test Loss : 38,834,547,981.47369\n",
            "Save model Epoch 20 Train Loss : 38,734,161,050.30137 Test Loss : 38,826,618,772.210526\n",
            "Save model Epoch 21 Train Loss : 38,720,475,949.58904 Test Loss : 38,802,475,762.52631\n",
            "Save model Epoch 22 Train Loss : 38,704,868,309.91781 Test Loss : 38,785,401,047.57895\n",
            "Save model Epoch 23 Train Loss : 38,689,673,664.87671 Test Loss : 38,777,080,239.1579\n",
            "Save model Epoch 24 Train Loss : 38,673,188,948.16438 Test Loss : 38,757,663,582.31579\n",
            "Save model Epoch 25 Train Loss : 38,658,144,648.76712 Test Loss : 38,730,974,585.26316\n",
            "Save model Epoch 26 Train Loss : 38,638,026,695.89041 Test Loss : 38,704,952,266.10526\n",
            "Save model Epoch 29 Train Loss : 38,580,289,031.0137 Test Loss : 38,669,115,392.0\n",
            "Save model Epoch 31 Train Loss : 38,546,787,370.08219 Test Loss : 38,617,776,343.57895\n",
            "Save model Epoch 32 Train Loss : 38,528,611,734.79452 Test Loss : 38,597,609,579.789474\n",
            "Save model Epoch 33 Train Loss : 38,506,077,927.45206 Test Loss : 38,592,546,384.8421\n",
            "Save model Epoch 34 Train Loss : 38,485,239,976.328766 Test Loss : 38,568,846,174.31579\n",
            "Save model Epoch 35 Train Loss : 38,463,633,969.095894 Test Loss : 38,551,560,299.789474\n",
            "Save model Epoch 36 Train Loss : 38,440,844,021.479454 Test Loss : 38,528,442,260.210526\n",
            "Save model Epoch 37 Train Loss : 38,416,991,610.73972 Test Loss : 38,512,117,544.42105\n",
            "Save model Epoch 38 Train Loss : 38,392,907,355.178085 Test Loss : 38,495,516,672.0\n",
            "Save model Epoch 39 Train Loss : 38,371,725,031.45206 Test Loss : 38,433,192,259.36842\n",
            "Save model Epoch 41 Train Loss : 38,322,978,507.39726 Test Loss : 38,403,539,590.73684\n",
            "Save model Epoch 42 Train Loss : 38,304,693,851.178085 Test Loss : 38,373,220,244.210526\n",
            "Save model Epoch 44 Train Loss : 38,247,196,784.21918 Test Loss : 38,345,785,236.210526\n",
            "Save model Epoch 45 Train Loss : 38,227,477,798.57534 Test Loss : 38,275,837,413.052635\n",
            "Save model Epoch 46 Train Loss : 38,195,719,462.57534 Test Loss : 38,273,987,422.31579\n",
            "Save model Epoch 48 Train Loss : 38,147,630,767.34247 Test Loss : 38,233,000,259.36842\n",
            "Save model Epoch 49 Train Loss : 38,123,290,539.83562 Test Loss : 38,214,160,707.36842\n",
            "Save model Epoch 50 Train Loss : 38,089,425,709.58904 Test Loss : 38,201,269,517.47369\n",
            "Save model Epoch 51 Train Loss : 38,056,860,798.246574 Test Loss : 38,134,036,803.36842\n",
            "Save model Epoch 53 Train Loss : 38,007,454,986.520546 Test Loss : 38,117,299,038.31579\n",
            "Save model Epoch 54 Train Loss : 37,976,535,236.38356 Test Loss : 38,070,642,688.0\n",
            "Save model Epoch 55 Train Loss : 37,945,596,942.0274 Test Loss : 38,058,919,936.0\n",
            "Save model Epoch 56 Train Loss : 37,921,058,647.671234 Test Loss : 37,958,284,557.47369\n",
            "Save model Epoch 58 Train Loss : 37,863,884,042.520546 Test Loss : 37,943,838,720.0\n",
            "Save model Epoch 59 Train Loss : 37,834,329,578.9589 Test Loss : 37,911,666,903.57895\n",
            "Save model Epoch 60 Train Loss : 37,796,242,446.0274 Test Loss : 37,899,646,221.47369\n",
            "Save model Epoch 61 Train Loss : 37,760,049,881.42466 Test Loss : 37,814,711,565.47369\n",
            "Save model Epoch 63 Train Loss : 37,694,423,573.0411 Test Loss : 37,751,461,995.789474\n",
            "Save model Epoch 64 Train Loss : 37,658,339,973.26028 Test Loss : 37,743,258,246.73684\n",
            "Save model Epoch 65 Train Loss : 37,634,995,719.0137 Test Loss : 37,662,758,049.68421\n",
            "Save model Epoch 66 Train Loss : 37,608,047,714.19178 Test Loss : 37,603,903,056.8421\n",
            "Save model Epoch 71 Train Loss : 37,427,131,321.863014 Test Loss : 37,565,971,078.73684\n",
            "Save model Epoch 72 Train Loss : 37,388,808,893.369865 Test Loss : 37,433,469,035.789474\n",
            "Save model Epoch 73 Train Loss : 37,363,014,922.520546 Test Loss : 37,420,710,211.36842\n",
            "Save model Epoch 74 Train Loss : 37,328,514,090.08219 Test Loss : 37,392,691,415.57895\n",
            "Save model Epoch 77 Train Loss : 37,227,083,761.9726 Test Loss : 37,330,731,008.0\n",
            "Save model Epoch 78 Train Loss : 37,171,754,250.520546 Test Loss : 37,172,843,250.52631\n",
            "Save model Epoch 79 Train Loss : 37,143,453,022.68493 Test Loss : 37,162,749,736.42105\n",
            "Save model Epoch 81 Train Loss : 37,056,728,049.9726 Test Loss : 37,097,838,160.8421\n",
            "Save model Epoch 82 Train Loss : 37,006,841,351.0137 Test Loss : 37,055,271,019.789474\n",
            "Save model Epoch 84 Train Loss : 36,947,240,651.39726 Test Loss : 37,036,602,960.8421\n",
            "Save model Epoch 85 Train Loss : 36,931,494,785.753426 Test Loss : 36,922,477,406.31579\n",
            "Save model Epoch 88 Train Loss : 36,787,320,158.68493 Test Loss : 36,875,654,736.8421\n",
            "Save model Epoch 92 Train Loss : 36,637,392,475.178085 Test Loss : 36,853,473,603.36842\n",
            "Save model Epoch 93 Train Loss : 36,559,808,147.287674 Test Loss : 36,705,417,216.0\n",
            "Save model Epoch 94 Train Loss : 36,573,782,997.91781 Test Loss : 36,664,947,550.31579\n",
            "Save model Epoch 95 Train Loss : 36,497,751,530.9589 Test Loss : 36,598,405,766.73684\n",
            "Save model Epoch 96 Train Loss : 36,459,525,470.68493 Test Loss : 36,496,161,414.73684\n",
            "Epoch 100 Train Loss : 36,295,987,957.479454 Test Loss : 36,429,460,318.31579 in fold 0\n",
            "Save model Epoch 100 Train Loss : 36,295,987,957.479454 Test Loss : 36,429,460,318.31579\n",
            "Save model Epoch 101 Train Loss : 36,217,497,964.712326 Test Loss : 36,344,803,759.1579\n",
            "Save model Epoch 102 Train Loss : 36,169,868,358.136986 Test Loss : 36,325,094,777.26316\n",
            "Save model Epoch 104 Train Loss : 36,111,163,420.054794 Test Loss : 36,114,342,534.73684\n",
            "Save model Epoch 111 Train Loss : 35,778,634,513.53425 Test Loss : 35,978,716,537.26316\n",
            "Save model Epoch 112 Train Loss : 35,750,971,448.10959 Test Loss : 35,927,174,197.89474\n",
            "Save model Epoch 113 Train Loss : 35,711,117,592.54794 Test Loss : 35,879,601,852.63158\n",
            "Save model Epoch 114 Train Loss : 35,618,523,276.27397 Test Loss : 35,746,739,146.10526\n",
            "Save model Epoch 115 Train Loss : 35,599,221,521.53425 Test Loss : 35,642,221,190.73684\n",
            "Save model Epoch 119 Train Loss : 35,403,448,053.479454 Test Loss : 35,373,012,021.89474\n",
            "Save model Epoch 121 Train Loss : 35,302,448,506.73972 Test Loss : 35,219,149,985.68421\n",
            "Save model Epoch 127 Train Loss : 34,974,810,420.60274 Test Loss : 34,947,179,897.26316\n",
            "Save model Epoch 131 Train Loss : 34,800,856,470.79452 Test Loss : 34,923,656,353.68421\n",
            "Save model Epoch 134 Train Loss : 34,626,755,822.46575 Test Loss : 34,912,590,362.947365\n",
            "Save model Epoch 136 Train Loss : 34,507,524,039.89041 Test Loss : 34,598,663,114.10526\n",
            "Save model Epoch 139 Train Loss : 34,361,035,172.821915 Test Loss : 34,361,948,698.947365\n",
            "Save model Epoch 145 Train Loss : 34,005,891,464.767124 Test Loss : 34,141,380,284.63158\n",
            "Save model Epoch 149 Train Loss : 33,775,520,669.80822 Test Loss : 33,998,249,229.473682\n",
            "Save model Epoch 150 Train Loss : 33,749,451,032.547947 Test Loss : 33,855,426,560.0\n",
            "Save model Epoch 151 Train Loss : 33,687,727,651.068493 Test Loss : 33,732,924,254.31579\n",
            "Save model Epoch 153 Train Loss : 33,539,750,533.260273 Test Loss : 33,443,759,912.42105\n",
            "Save model Epoch 159 Train Loss : 33,251,108,920.10959 Test Loss : 33,087,188,992.0\n",
            "Save model Epoch 160 Train Loss : 33,159,264,059.61644 Test Loss : 33,026,652,591.157894\n",
            "Save model Epoch 164 Train Loss : 32,882,637,347.068493 Test Loss : 32,930,180,365.473682\n",
            "Save model Epoch 169 Train Loss : 32,609,177,571.945206 Test Loss : 32,684,747,398.736843\n",
            "Save model Epoch 173 Train Loss : 32,382,723,338.52055 Test Loss : 32,339,206,251.789474\n",
            "Save model Epoch 180 Train Loss : 31,894,072,642.63014 Test Loss : 32,295,875,098.94737\n",
            "Save model Epoch 181 Train Loss : 31,856,788,171.39726 Test Loss : 32,035,396,230.736843\n",
            "Save model Epoch 182 Train Loss : 31,782,856,086.79452 Test Loss : 31,920,312,320.0\n",
            "Save model Epoch 187 Train Loss : 31,410,530,724.82192 Test Loss : 31,613,685,760.0\n",
            "Save model Epoch 188 Train Loss : 31,369,387,470.90411 Test Loss : 31,408,125,520.842106\n"
          ]
        }
      ],
      "source": [
        "fold_models = []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
        "    print(f'== Fold {fold+1} / {k_folds} ==')\n",
        "\n",
        "    train_subset = Subset(dataset, train_idx)\n",
        "    test_subset = Subset(dataset, test_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
        "    test_loader = DataLoader(test_subset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Early Stopping을 위한 변수 설정\n",
        "    best_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    patience = 200\n",
        "    counter = 0\n",
        "     \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    # Fold 마다 Model 초기화\n",
        "    model = DNN().to(device)\n",
        "\n",
        "    # 손실함수 및 옵티마이저 설정\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "    # 학습 횟수 만큼 반복\n",
        "    for epoch in range(2000):\n",
        "\n",
        "        # 모델 학습(학습데이터)\n",
        "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # 모델 평가 (평가데이터)\n",
        "        test_loss = evaluate(model, test_loader, criterion, device)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch {epoch+1} Train Loss : {train_loss:,} Test Loss : {test_loss:,} in fold {fold}')\n",
        "\n",
        "        # Early Stopping 조건 체크\n",
        "        if test_loss < best_loss:\n",
        "            best_loss = test_loss\n",
        "            counter = 0\n",
        "            best_epoch = epoch\n",
        "            print(f'Save model Epoch {epoch+1} Train Loss : {train_loss:,} Test Loss : {test_loss:,}')\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            counter += 1\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(f'Early stopping trigerred at epoch {epoch+1} Best Loss: {best_loss:,} in epoch {best_epoch}')\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "    plot_training_results(train_losses, test_losses)\n",
        "    plot_training_results(train_losses[2000:], test_losses[2000:])\n",
        "\n",
        "    fold_models.append(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ecf5dbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ecf5dbd",
        "outputId": "74672144-6629-40cd-98eb-caf93ae5a3a7"
      },
      "outputs": [],
      "source": [
        "# 사용자명을 입력하세요. (이름이 아니여도 괜찮습니다.)\n",
        "username = \"김재곤\"\n",
        "assert username, \"username 변수에 값이 설정되지 않았습니다.\"\n",
        "\n",
        "# 그대로 실행하세요.\n",
        "from JAEN.competition import Competition\n",
        "comp = Competition(\n",
        "    username=username,\n",
        "    course_name='AI Essential',\n",
        "    course_round='0317(1)',\n",
        "    competition_name='House Price Prediction'\n",
        ")\n",
        "\n",
        "# 학습된 모델의 TEST 예측\n",
        "for m in fold_models:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(TEST)\n",
        "    avg_outputs = torch.mean(torch.stack(outputs), dim=0)\n",
        "\n",
        "#model.eval()\n",
        "#TEST2 = TEST.to(device)\n",
        "#with torch.no_grad():\n",
        "#    outputs = model(TEST2)\n",
        "#outputs\n",
        "\n",
        "# comp.prediction에 TEST 예측 결과 대입\n",
        "#comp.prediction = outputs\n",
        "comp.prediction = avg_outputs\n",
        "comp.prediction\n",
        "\n",
        "# 제출\n",
        "#comp.submit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
