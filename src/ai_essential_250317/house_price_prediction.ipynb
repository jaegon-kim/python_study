{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaegon-kim/python_study/blob/main/src/ai_essential_250317/house_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C3HNZJH5LuqI",
      "metadata": {
        "id": "C3HNZJH5LuqI"
      },
      "source": [
        "# House Price Prediction\n",
        "- **목표**\n",
        "  - 이 워크샵은 주어진 데이터셋을 이용해 심층신경망 모델을 학습시켜 주택의 최종 판매 가격(SalePrice)을 예측하는 것이 최종 목표입니다.\n",
        "\n",
        "- **데이터셋 정보**\n",
        "  - 데이터셋은 총 79개의 설명 변수와 타겟 변수인 주택 가격(SalePrice)로 구성됩니다.\n",
        "  - 설명 변수는 주택의 다양한 특성(예: 건축 연도, 면적, 위치, 방 개수 등)을 포함합니다.\n",
        "  - 데이터는 판매 가격이 포함된 학습용 데이터인 `X`, `y` 와 판매 가격이 포함되지 않은 평가용 데이터인 `TEST`파일로 나뉘며, 각각 모델 학습 및 평가에 사용됩니다.\n",
        "    - 평가용 데이터 `TEST`의 판매 가격(SalePrice)를 예측 후 리더보드로 제출하여 평가합니다.\n",
        "\n",
        "- **문제 유형**\n",
        "  - 이 워크샵은 회귀 문제로 연속형 변수를 예측하는 것이 목표입니다. 모델의 성능은 `Mean Absolute Error`로 측정됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FerWbWa8ML9S",
      "metadata": {
        "id": "FerWbWa8ML9S"
      },
      "source": [
        "## 1. 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fbebc02c",
      "metadata": {
        "id": "fbebc02c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install JAEN -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2b192c23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b192c23",
        "outputId": "fd9b6098-5f31-4179-92f6-894bb7b2873f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 그대로 실행하세요.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4144477e",
      "metadata": {
        "id": "4144477e"
      },
      "outputs": [],
      "source": [
        "# 사용자명을 입력하세요. (이름이 아니여도 괜찮습니다.)\n",
        "username = \"김재곤\"\n",
        "assert username, \"username 변수에 값이 설정되지 않았습니다.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "MkvHc266MWva",
      "metadata": {
        "id": "MkvHc266MWva",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# 그대로 실행하세요.\n",
        "from JAEN.competition import Competition\n",
        "comp = Competition(\n",
        "    username=username,\n",
        "    course_name='AI Essential',\n",
        "    course_round='0317(1)',\n",
        "    competition_name='House Price Prediction'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OSiIE4tdPcSV",
      "metadata": {
        "id": "OSiIE4tdPcSV"
      },
      "source": [
        "## 2. 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Cwo9d1i-ON3u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwo9d1i-ON3u",
        "outputId": "8ce0cf32-3d9f-48aa-90f9-3bcfb370cc5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1460, 79]), torch.Size([1460, 1]), torch.Size([1459, 79]))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from JAEN.datasets import load_house_price\n",
        "X, y, TEST = load_house_price()\n",
        "X.shape, y.shape, TEST.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I_Vc3a22PgBm",
      "metadata": {
        "id": "I_Vc3a22PgBm"
      },
      "source": [
        "## 3. 제출 예시 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "933af893",
      "metadata": {
        "id": "933af893"
      },
      "outputs": [],
      "source": [
        "# TEST의 예측값 대입 (지금은 0으로 채워진 값 대입)\n",
        "#comp.prediction =  torch.zeros(1459)\n",
        "#comp.prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "wvkBKJUbKsW9",
      "metadata": {
        "id": "wvkBKJUbKsW9"
      },
      "outputs": [],
      "source": [
        "# 제출\n",
        "#comp.submit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4wNCB3ATlBe4",
      "metadata": {
        "id": "4wNCB3ATlBe4"
      },
      "source": [
        "## 4. 심층신경망 모델을 구성하고 학습하여 TEST를 예측해보세요.\n",
        "- TEST의 예측 결과는 `comp.prediction`에 대입해주세요. **torch.tensor** 형태여야합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "KtwmpH1EibaM",
      "metadata": {
        "id": "KtwmpH1EibaM"
      },
      "outputs": [],
      "source": [
        "# DataLoader 생성\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "train_loader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "-37EJXcZibcK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-37EJXcZibcK",
        "outputId": "9b6f3b9b-67f0-4b89-c999-c630b8df3486"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "DNN                                      --\n",
              "├─Linear: 1-1                            10,240\n",
              "├─BatchNorm1d: 1-2                       256\n",
              "├─Linear: 1-3                            8,256\n",
              "├─Linear: 1-4                            65\n",
              "├─ReLU: 1-5                              --\n",
              "├─Dropout: 1-6                           --\n",
              "=================================================================\n",
              "Total params: 18,817\n",
              "Trainable params: 18,817\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DNN 모델 구성\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(79, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)  # 10%의 드롭아웃 적용\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = DNN().to(device)\n",
        "summary(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a2237784",
      "metadata": {
        "id": "a2237784"
      },
      "outputs": [],
      "source": [
        "# 손실함수 및 옵티마이저 설정\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "VPKYlayzU0Dt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPKYlayzU0Dt",
        "outputId": "4d39f3d8-96b1-4d55-f6fb-6c2e1d182e77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1460, 79])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3e1ded1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e1ded1c",
        "outputId": "8640c845-495b-41fe-e363-e8e1c076c579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1000], Loss: 38969745897.73913\n",
            "Save model at epoch 1\n",
            "Save model at epoch 2\n",
            "Save model at epoch 3\n",
            "Save model at epoch 5\n",
            "Save model at epoch 6\n",
            "Save model at epoch 7\n",
            "Save model at epoch 8\n",
            "Save model at epoch 9\n",
            "Save model at epoch 10\n",
            "Epoch [11/1000], Loss: 36763102831.304344\n",
            "Save model at epoch 11\n",
            "Save model at epoch 12\n",
            "Save model at epoch 13\n",
            "Save model at epoch 14\n",
            "Save model at epoch 15\n",
            "Save model at epoch 16\n",
            "Save model at epoch 17\n",
            "Save model at epoch 18\n",
            "Save model at epoch 19\n",
            "Save model at epoch 20\n",
            "Epoch [21/1000], Loss: 25475204541.217392\n",
            "Save model at epoch 21\n",
            "Save model at epoch 22\n",
            "Save model at epoch 23\n",
            "Save model at epoch 24\n",
            "Save model at epoch 25\n",
            "Save model at epoch 26\n",
            "Save model at epoch 27\n",
            "Save model at epoch 28\n",
            "Save model at epoch 29\n",
            "Save model at epoch 30\n",
            "Epoch [31/1000], Loss: 11297944976.695652\n",
            "Save model at epoch 31\n",
            "Save model at epoch 32\n",
            "Save model at epoch 33\n",
            "Save model at epoch 34\n",
            "Save model at epoch 35\n",
            "Save model at epoch 36\n",
            "Save model at epoch 37\n",
            "Save model at epoch 38\n",
            "Save model at epoch 39\n",
            "Save model at epoch 40\n",
            "Epoch [41/1000], Loss: 3189056299.130435\n",
            "Save model at epoch 41\n",
            "Save model at epoch 42\n",
            "Save model at epoch 43\n",
            "Save model at epoch 44\n",
            "Save model at epoch 45\n",
            "Save model at epoch 46\n",
            "Save model at epoch 47\n",
            "Save model at epoch 48\n",
            "Save model at epoch 49\n",
            "Epoch [51/1000], Loss: 1345470794.4347825\n",
            "Save model at epoch 51\n",
            "Save model at epoch 52\n",
            "Save model at epoch 55\n",
            "Epoch [61/1000], Loss: 1202459001.0434783\n",
            "Save model at epoch 70\n",
            "Epoch [71/1000], Loss: 1140736708.8695652\n",
            "Epoch [81/1000], Loss: 962077968.3478261\n",
            "Epoch [91/1000], Loss: 1009577418.4347826\n",
            "Save model at epoch 94\n",
            "Save model at epoch 100\n",
            "Epoch [101/1000], Loss: 1016678866.7826087\n",
            "Epoch [111/1000], Loss: 922546989.9130435\n",
            "Save model at epoch 117\n",
            "Epoch [121/1000], Loss: 949381702.9565217\n",
            "Save model at epoch 128\n",
            "Epoch [131/1000], Loss: 834466457.0434783\n",
            "Save model at epoch 131\n",
            "Epoch [141/1000], Loss: 950605814.2608696\n",
            "Epoch [151/1000], Loss: 944551197.9130435\n",
            "Epoch [161/1000], Loss: 872199330.7826087\n",
            "Save model at epoch 162\n",
            "Epoch [171/1000], Loss: 924455620.8695652\n",
            "Epoch [181/1000], Loss: 894848888.0\n",
            "Epoch [191/1000], Loss: 781291355.826087\n",
            "Save model at epoch 191\n",
            "Epoch [201/1000], Loss: 781747470.9565217\n",
            "Epoch [211/1000], Loss: 819558651.1304348\n",
            "Save model at epoch 218\n",
            "Epoch [221/1000], Loss: 893132180.173913\n",
            "Save model at epoch 226\n",
            "Epoch [231/1000], Loss: 822656835.1304348\n",
            "Epoch [241/1000], Loss: 773994669.9130435\n",
            "Epoch [251/1000], Loss: 910308455.6521739\n",
            "Epoch [261/1000], Loss: 772728831.3043479\n",
            "Epoch [271/1000], Loss: 827554121.0434783\n",
            "Save model at epoch 279\n",
            "Epoch [281/1000], Loss: 804327355.826087\n",
            "Epoch [291/1000], Loss: 826791346.0869565\n",
            "Epoch [301/1000], Loss: 884585662.2608696\n",
            "Epoch [311/1000], Loss: 861866022.9565217\n",
            "Epoch [321/1000], Loss: 750451479.6521739\n",
            "Epoch [331/1000], Loss: 754300432.6956521\n",
            "Epoch [341/1000], Loss: 726308726.2608696\n",
            "Epoch [351/1000], Loss: 849252339.4782609\n",
            "Save model at epoch 352\n",
            "Save model at epoch 358\n",
            "Epoch [361/1000], Loss: 686675605.5652174\n",
            "Save model at epoch 361\n",
            "Epoch [371/1000], Loss: 905252259.4782609\n",
            "Epoch [381/1000], Loss: 822630633.0434783\n",
            "Epoch [391/1000], Loss: 908331256.3478261\n",
            "Save model at epoch 396\n",
            "Epoch [401/1000], Loss: 716833924.173913\n",
            "Epoch [411/1000], Loss: 723827771.4782609\n",
            "Epoch [421/1000], Loss: 778404461.9130435\n",
            "Epoch [431/1000], Loss: 729092842.4347826\n",
            "Epoch [441/1000], Loss: 768969058.0869565\n",
            "Epoch [451/1000], Loss: 765587161.0434783\n",
            "Epoch [461/1000], Loss: 825488756.173913\n",
            "Save model at epoch 467\n",
            "Epoch [471/1000], Loss: 802767039.3043479\n",
            "Epoch [481/1000], Loss: 655489630.2608696\n",
            "Epoch [491/1000], Loss: 835372330.4347826\n",
            "Epoch [501/1000], Loss: 786056726.2608696\n",
            "Epoch [511/1000], Loss: 729483061.5652174\n",
            "Epoch [521/1000], Loss: 782690731.1304348\n",
            "Epoch [531/1000], Loss: 753200748.173913\n",
            "Epoch [541/1000], Loss: 819794402.7826087\n",
            "Epoch [551/1000], Loss: 737017720.6956521\n",
            "Epoch [561/1000], Loss: 721925870.2608696\n",
            "Early stopping at epoch 567\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델 학습 과정 구현\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 100\n",
        "counter = 0\n",
        "\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss}')\n",
        "\n",
        "    if train_loss < best_loss:\n",
        "        best_loss = train_loss\n",
        "        counter = 0\n",
        "        print(f'Save model at epoch {epoch+1}')\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "    else:\n",
        "        counter += 1\n",
        "\n",
        "    if counter >= patience:\n",
        "        print(f'Early stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c1439106",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1439106",
        "outputId": "e7dd53cf-29d3-4021-c695-a6cfb8262a68"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEST\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m outputs\n",
            "File \u001b[0;32m~/anaconda3/envs/python311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/python311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mDNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#x = self.relu(self.fc1(x))\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
            "File \u001b[0;32m~/anaconda3/envs/python311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/python311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/python311/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
          ]
        }
      ],
      "source": [
        "# 학습된 모델의 TEST 예측\n",
        "#model.eval()\n",
        "#with torch.no_grad():\n",
        "#    outputs = model(TEST)\n",
        "#outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb7b590",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fb7b590",
        "outputId": "d344e0ca-1a16-4a92-da92-46eda7a58eda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[114947.7891],\n",
              "        [203401.7344],\n",
              "        [179515.8750],\n",
              "        ...,\n",
              "        [167293.5000],\n",
              "        [125959.5938],\n",
              "        [242594.2812]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# comp.prediction에 TEST 예측 결과 대입\n",
        "#comp.prediction = outputs\n",
        "#comp.prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd1d918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd1d918",
        "outputId": "95898e4b-a892-4370-da30-70b926e7f296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[House Price Prediction 평가 결과]\n",
            " AI Essential 0317(1) 과정 김재곤님의 점수는 17397.93359375 입니다."
          ]
        }
      ],
      "source": [
        "# 제출\n",
        "#comp.submit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
