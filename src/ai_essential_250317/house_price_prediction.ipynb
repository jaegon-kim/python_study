{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaegon-kim/python_study/blob/main/src/ai_essential_250317/house_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C3HNZJH5LuqI",
      "metadata": {
        "id": "C3HNZJH5LuqI"
      },
      "source": [
        "# House Price Prediction\n",
        "- **목표**\n",
        "  - 이 워크샵은 주어진 데이터셋을 이용해 심층신경망 모델을 학습시켜 주택의 최종 판매 가격(SalePrice)을 예측하는 것이 최종 목표입니다.\n",
        "\n",
        "- **데이터셋 정보**\n",
        "  - 데이터셋은 총 79개의 설명 변수와 타겟 변수인 주택 가격(SalePrice)로 구성됩니다.\n",
        "  - 설명 변수는 주택의 다양한 특성(예: 건축 연도, 면적, 위치, 방 개수 등)을 포함합니다.\n",
        "  - 데이터는 판매 가격이 포함된 학습용 데이터인 `X`, `y` 와 판매 가격이 포함되지 않은 평가용 데이터인 `TEST`파일로 나뉘며, 각각 모델 학습 및 평가에 사용됩니다.\n",
        "    - 평가용 데이터 `TEST`의 판매 가격(SalePrice)를 예측 후 리더보드로 제출하여 평가합니다.\n",
        "\n",
        "- **문제 유형**\n",
        "  - 이 워크샵은 회귀 문제로 연속형 변수를 예측하는 것이 목표입니다. 모델의 성능은 `Mean Absolute Error`로 측정됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FerWbWa8ML9S",
      "metadata": {
        "id": "FerWbWa8ML9S"
      },
      "source": [
        "## 1. 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fbebc02c",
      "metadata": {
        "id": "fbebc02c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install JAEN -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2b192c23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b192c23",
        "outputId": "7531a17c-5d26-4a92-9312-de15d8a699a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# 그대로 실행하세요.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4144477e",
      "metadata": {
        "id": "4144477e"
      },
      "outputs": [],
      "source": [
        "# 사용자명을 입력하세요. (이름이 아니여도 괜찮습니다.)\n",
        "username = \"김재곤\"\n",
        "assert username, \"username 변수에 값이 설정되지 않았습니다.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "MkvHc266MWva",
      "metadata": {
        "id": "MkvHc266MWva",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# 그대로 실행하세요.\n",
        "from JAEN.competition import Competition\n",
        "comp = Competition(\n",
        "    username=username,\n",
        "    course_name='AI Essential',\n",
        "    course_round='0317(1)',\n",
        "    competition_name='House Price Prediction'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OSiIE4tdPcSV",
      "metadata": {
        "id": "OSiIE4tdPcSV"
      },
      "source": [
        "## 2. 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Cwo9d1i-ON3u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwo9d1i-ON3u",
        "outputId": "24dc93c8-934b-4072-b870-a183cb25f31a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1460, 79]), torch.Size([1460, 1]), torch.Size([1459, 79]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from JAEN.datasets import load_house_price\n",
        "X, y, TEST = load_house_price()\n",
        "X.shape, y.shape, TEST.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I_Vc3a22PgBm",
      "metadata": {
        "id": "I_Vc3a22PgBm"
      },
      "source": [
        "## 3. 제출 예시 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "933af893",
      "metadata": {
        "id": "933af893"
      },
      "outputs": [],
      "source": [
        "# TEST의 예측값 대입 (지금은 0으로 채워진 값 대입)\n",
        "#comp.prediction =  torch.zeros(1459)\n",
        "#comp.prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "wvkBKJUbKsW9",
      "metadata": {
        "id": "wvkBKJUbKsW9"
      },
      "outputs": [],
      "source": [
        "# 제출\n",
        "#comp.submit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4wNCB3ATlBe4",
      "metadata": {
        "id": "4wNCB3ATlBe4"
      },
      "source": [
        "## 4. 심층신경망 모델을 구성하고 학습하여 TEST를 예측해보세요.\n",
        "- TEST의 예측 결과는 `comp.prediction`에 대입해주세요. **torch.tensor** 형태여야합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "    for data, labels in train_loader: # 미니 배치 별 파라미터 업데이트 수행\n",
        "        data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "        # 순전파\n",
        "        outputs = model(data)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 손실 누적\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    # - len(train_loader): 평균 Loss\n",
        "    return running_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "WFaYIgOTYhZz"
      },
      "id": "WFaYIgOTYhZz",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "    with torch.no_grad():  # 평가 중에는 기울기 계산을 하지 않음\n",
        "        for data, labels in test_loader: # 미니 배치 별 손실 계산\n",
        "            data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "            # 순전파\n",
        "            outputs = model(data)\n",
        "\n",
        "            # 손실 계산\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 손실 누적\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    return running_loss / len(test_loader)"
      ],
      "metadata": {
        "id": "WFCtYxB8YhEs"
      },
      "id": "WFCtYxB8YhEs",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "KtwmpH1EibaM",
      "metadata": {
        "id": "KtwmpH1EibaM"
      },
      "outputs": [],
      "source": [
        "# DataLoader 생성\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "#train_loader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)\n",
        "dataset = TensorDataset(X, y)\n",
        "k_folds = 5\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "-37EJXcZibcK",
      "metadata": {
        "id": "-37EJXcZibcK"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(79, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)  # 10%의 드롭아웃 적용\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.bn2(self.fc2(x)))\n",
        "        #x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "'''\n",
        "#'''\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc_1 = nn.Linear(79, 512)\n",
        "        self.bn_1 = nn.BatchNorm1d(512)\n",
        "        #self.fc0 = nn.Linear(79, 256)\n",
        "        self.fc0 = nn.Linear(512, 256)\n",
        "        self.bn0 = nn.BatchNorm1d(256)\n",
        "        #self.fc1 = nn.Linear(79, 128)\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)  # 10%의 드롭아웃 적용\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.bn_1(self.fc_1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.bn0(self.fc0(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "#'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "VPKYlayzU0Dt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPKYlayzU0Dt",
        "outputId": "c49edd97-a129-4a55-e81b-ae222e15d576"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1460, 79])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3e1ded1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e1ded1c",
        "outputId": "a028bfec-d470-46a3-93a0-7a6182302288",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Fold 1 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 38,880,394,506.520546 Test Loss : 38,942,361,923.36842\n",
            "Save model Epoch 2 Train Loss : 38,841,523,045.69863 Test Loss : 38,879,658,738.52631\n",
            "Save model Epoch 3 Train Loss : 38,734,124,214.35616 Test Loss : 38,705,786,987.789474\n",
            "Save model Epoch 4 Train Loss : 38,514,588,938.520546 Test Loss : 38,399,868,820.210526\n",
            "Save model Epoch 5 Train Loss : 38,134,085,295.34247 Test Loss : 37,841,750,016.0\n",
            "Save model Epoch 6 Train Loss : 37,555,337,272.10959 Test Loss : 37,235,211,964.63158\n",
            "Save model Epoch 7 Train Loss : 36,755,295,105.753426 Test Loss : 36,552,837,443.36842\n",
            "Save model Epoch 8 Train Loss : 35,735,474,049.753426 Test Loss : 35,301,953,428.210526\n",
            "Save model Epoch 9 Train Loss : 34,440,560,808.328766 Test Loss : 34,163,452,227.36842\n",
            "Save model Epoch 10 Train Loss : 32,850,486,903.232876 Test Loss : 32,369,901,568.0\n",
            "Save model Epoch 11 Train Loss : 31,112,604,405.47945 Test Loss : 29,805,573,820.63158\n",
            "Save model Epoch 12 Train Loss : 29,124,312,639.123287 Test Loss : 28,399,238,305.68421\n",
            "Save model Epoch 13 Train Loss : 26,934,950,757.69863 Test Loss : 26,325,008,114.526318\n",
            "Save model Epoch 14 Train Loss : 24,656,171,232.438354 Test Loss : 23,766,121,579.789474\n",
            "Save model Epoch 15 Train Loss : 22,261,478,414.027397 Test Loss : 22,380,159,676.63158\n",
            "Save model Epoch 16 Train Loss : 20,008,473,417.643837 Test Loss : 19,890,095,912.42105\n",
            "Save model Epoch 17 Train Loss : 17,648,551,515.17808 Test Loss : 15,641,354,940.631578\n",
            "Save model Epoch 18 Train Loss : 15,394,803,550.68493 Test Loss : 15,290,703,386.947369\n",
            "Save model Epoch 19 Train Loss : 13,142,201,073.972603 Test Loss : 13,132,130,654.31579\n",
            "Save model Epoch 20 Train Loss : 11,271,319,615.123287 Test Loss : 10,698,845,345.68421\n",
            "Save model Epoch 21 Train Loss : 9,739,390,186.958904 Test Loss : 8,777,117,345.68421\n",
            "Save model Epoch 22 Train Loss : 8,224,815,261.808219 Test Loss : 7,955,499,843.368421\n",
            "Save model Epoch 23 Train Loss : 6,852,693,156.821918 Test Loss : 7,612,702,693.052631\n",
            "Save model Epoch 24 Train Loss : 5,955,672,018.410959 Test Loss : 5,318,946,613.894737\n",
            "Save model Epoch 25 Train Loss : 4,874,586,497.753425 Test Loss : 4,991,310,127.157895\n",
            "Save model Epoch 26 Train Loss : 4,347,776,486.575342 Test Loss : 3,874,954,394.9473686\n",
            "Save model Epoch 27 Train Loss : 3,630,382,822.5753427 Test Loss : 3,182,092,523.7894735\n",
            "Save model Epoch 28 Train Loss : 3,340,728,701.369863 Test Loss : 2,702,990,625.6842103\n",
            "Save model Epoch 30 Train Loss : 2,987,413,643.39726 Test Loss : 2,274,344,990.3157897\n",
            "Save model Epoch 32 Train Loss : 2,591,404,696.109589 Test Loss : 2,164,454,007.5789475\n",
            "Save model Epoch 33 Train Loss : 2,366,064,127.5616436 Test Loss : 1,945,427,103.1578948\n",
            "Save model Epoch 34 Train Loss : 2,033,053,336.5479453 Test Loss : 1,404,036,585.2631578\n",
            "Save model Epoch 36 Train Loss : 1,585,700,583.890411 Test Loss : 1,188,455,838.3157895\n",
            "Save model Epoch 38 Train Loss : 1,230,293,896.328767 Test Loss : 1,107,356,165.0526316\n",
            "Save model Epoch 39 Train Loss : 1,444,024,756.1643836 Test Loss : 1,046,976,494.7368422\n",
            "Save model Epoch 40 Train Loss : 1,238,911,186.1917808 Test Loss : 989,508,556.2105263\n",
            "Save model Epoch 45 Train Loss : 1,015,594,466.1917808 Test Loss : 866,638,656.4210526\n",
            "Early stopping trigerred at epoch 50 Best Loss: 866,638,656.4210526 in epoch 44\n",
            "== Fold 2 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 38,445,305,491.287674 Test Loss : 40,886,310,373.052635\n",
            "Save model Epoch 2 Train Loss : 38,407,694,784.87671 Test Loss : 40,825,082,179.36842\n",
            "Save model Epoch 3 Train Loss : 38,300,234,850.19178 Test Loss : 40,662,522,718.31579\n",
            "Save model Epoch 4 Train Loss : 38,080,709,730.19178 Test Loss : 40,350,271,488.0\n",
            "Save model Epoch 5 Train Loss : 37,711,139,236.821915 Test Loss : 39,809,746,836.210526\n",
            "Save model Epoch 6 Train Loss : 37,165,026,149.69863 Test Loss : 38,969,800,704.0\n",
            "Save model Epoch 7 Train Loss : 36,402,507,691.83562 Test Loss : 38,403,235,840.0\n",
            "Save model Epoch 8 Train Loss : 35,378,213,677.58904 Test Loss : 37,534,685,938.52631\n",
            "Save model Epoch 9 Train Loss : 34,119,552,547.068493 Test Loss : 35,703,206,965.89474\n",
            "Save model Epoch 10 Train Loss : 32,659,159,376.657536 Test Loss : 33,988,863,461.05263\n",
            "Save model Epoch 11 Train Loss : 30,918,427,157.041096 Test Loss : 31,606,870,986.105263\n",
            "Save model Epoch 12 Train Loss : 28,979,929,915.61644 Test Loss : 29,814,316,409.263157\n",
            "Save model Epoch 13 Train Loss : 26,842,991,980.71233 Test Loss : 27,996,899,328.0\n",
            "Save model Epoch 14 Train Loss : 24,714,613,549.589043 Test Loss : 25,457,776,640.0\n",
            "Save model Epoch 15 Train Loss : 22,303,611,216.657536 Test Loss : 22,885,088,902.736843\n",
            "Save model Epoch 16 Train Loss : 19,968,472,134.136986 Test Loss : 19,416,659,105.68421\n",
            "Save model Epoch 17 Train Loss : 17,650,238,456.9863 Test Loss : 16,857,790,625.68421\n",
            "Save model Epoch 18 Train Loss : 15,501,320,192.0 Test Loss : 15,413,127,545.263159\n",
            "Save model Epoch 19 Train Loss : 13,542,740,290.630136 Test Loss : 13,881,786,691.368422\n",
            "Save model Epoch 20 Train Loss : 11,397,504,091.178082 Test Loss : 10,888,674,088.421053\n",
            "Save model Epoch 21 Train Loss : 9,906,313,889.31507 Test Loss : 10,184,474,758.736841\n",
            "Save model Epoch 22 Train Loss : 8,628,462,914.630136 Test Loss : 8,859,827,146.105263\n",
            "Save model Epoch 23 Train Loss : 7,345,016,099.068493 Test Loss : 5,621,476,419.368421\n",
            "Save model Epoch 25 Train Loss : 5,401,818,985.20548 Test Loss : 4,771,328,916.210526\n",
            "Save model Epoch 26 Train Loss : 4,597,517,469.808219 Test Loss : 4,073,125,497.263158\n",
            "Save model Epoch 27 Train Loss : 3,792,962,731.8356166 Test Loss : 3,886,302,228.2105265\n",
            "Save model Epoch 28 Train Loss : 3,384,559,934.2465754 Test Loss : 3,381,172,412.631579\n",
            "Save model Epoch 30 Train Loss : 2,894,968,118.3561645 Test Loss : 2,544,579,550.3157897\n",
            "Save model Epoch 31 Train Loss : 2,804,543,183.780822 Test Loss : 2,210,805,711.1578946\n",
            "Save model Epoch 32 Train Loss : 2,632,257,181.808219 Test Loss : 2,066,757,840.0\n",
            "Save model Epoch 35 Train Loss : 2,102,032,722.8493152 Test Loss : 1,597,738,120.4210527\n",
            "Save model Epoch 36 Train Loss : 1,772,159,190.7945206 Test Loss : 1,555,042,680.4210527\n",
            "Save model Epoch 38 Train Loss : 1,422,830,209.3150685 Test Loss : 1,491,408,472.8421052\n",
            "Save model Epoch 39 Train Loss : 1,343,319,284.8219178 Test Loss : 1,260,719,684.631579\n",
            "Save model Epoch 41 Train Loss : 1,211,653,951.671233 Test Loss : 1,053,321,474.1052631\n",
            "Early stopping trigerred at epoch 46 Best Loss: 1,053,321,474.1052631 in epoch 40\n",
            "== Fold 3 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 39,296,871,269.69863 Test Loss : 37,313,376,687.1579\n",
            "Save model Epoch 2 Train Loss : 39,262,258,246.136986 Test Loss : 37,251,850,671.1579\n",
            "Save model Epoch 3 Train Loss : 39,163,037,808.21918 Test Loss : 37,103,959,848.42105\n",
            "Save model Epoch 4 Train Loss : 38,962,067,231.561646 Test Loss : 36,867,437,621.89474\n",
            "Save model Epoch 5 Train Loss : 38,623,287,464.328766 Test Loss : 36,353,382,938.947365\n",
            "Save model Epoch 6 Train Loss : 38,110,428,566.79452 Test Loss : 35,842,970,893.47369\n",
            "Save model Epoch 7 Train Loss : 37,388,776,602.30137 Test Loss : 34,850,635,021.47369\n",
            "Save model Epoch 8 Train Loss : 36,427,489,897.20548 Test Loss : 34,165,275,162.94737\n",
            "Save model Epoch 9 Train Loss : 35,246,694,876.9315 Test Loss : 32,907,065,559.57895\n",
            "Save model Epoch 10 Train Loss : 33,803,194,339.945206 Test Loss : 31,903,834,650.94737\n",
            "Save model Epoch 11 Train Loss : 32,185,455,237.260273 Test Loss : 29,504,747,088.842106\n",
            "Save model Epoch 12 Train Loss : 30,352,439,632.657536 Test Loss : 26,809,838,645.894737\n",
            "Save model Epoch 13 Train Loss : 28,322,269,618.849316 Test Loss : 25,514,565,955.36842\n",
            "Save model Epoch 14 Train Loss : 26,214,007,373.150684 Test Loss : 23,919,508,965.05263\n",
            "Save model Epoch 15 Train Loss : 23,932,128,936.328766 Test Loss : 21,073,870,309.05263\n",
            "Save model Epoch 16 Train Loss : 21,676,340,728.9863 Test Loss : 19,186,615,296.0\n",
            "Save model Epoch 17 Train Loss : 19,331,188,848.219177 Test Loss : 18,333,242,314.105263\n",
            "Save model Epoch 18 Train Loss : 17,032,347,963.616438 Test Loss : 15,718,021,551.157894\n",
            "Save model Epoch 19 Train Loss : 14,814,678,268.49315 Test Loss : 14,381,230,618.947369\n",
            "Save model Epoch 20 Train Loss : 12,986,671,994.739725 Test Loss : 12,407,328,821.894737\n",
            "Save model Epoch 21 Train Loss : 11,106,318,202.739725 Test Loss : 10,307,651,287.578947\n",
            "Save model Epoch 22 Train Loss : 9,702,442,176.876713 Test Loss : 9,255,439,737.263159\n",
            "Save model Epoch 23 Train Loss : 8,213,594,360.986301 Test Loss : 8,559,656,043.789474\n",
            "Save model Epoch 24 Train Loss : 6,829,029,435.616438 Test Loss : 7,827,977,431.578947\n",
            "Save model Epoch 25 Train Loss : 5,879,696,499.7260275 Test Loss : 6,173,305,909.894737\n",
            "Save model Epoch 26 Train Loss : 4,994,815,484.493151 Test Loss : 5,965,502,720.0\n",
            "Save model Epoch 27 Train Loss : 4,385,343,719.452055 Test Loss : 5,471,682,708.210526\n",
            "Save model Epoch 28 Train Loss : 3,980,389,652.1643834 Test Loss : 5,194,672,047.157895\n",
            "Save model Epoch 30 Train Loss : 3,386,746,787.9452057 Test Loss : 4,733,304,414.315789\n",
            "Save model Epoch 33 Train Loss : 2,472,238,348.2739725 Test Loss : 4,534,513,591.578947\n",
            "Early stopping trigerred at epoch 38 Best Loss: 4,534,513,591.578947 in epoch 32\n",
            "== Fold 4 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 39,187,087,360.0 Test Loss : 38,243,634,553.26316\n",
            "Save model Epoch 2 Train Loss : 39,146,603,674.30137 Test Loss : 38,182,866,836.210526\n",
            "Save model Epoch 3 Train Loss : 39,028,064,536.54794 Test Loss : 37,989,049,505.68421\n",
            "Save model Epoch 4 Train Loss : 38,791,873,157.26028 Test Loss : 37,751,015,316.210526\n",
            "Save model Epoch 5 Train Loss : 38,374,127,447.671234 Test Loss : 37,266,325,504.0\n",
            "Save model Epoch 6 Train Loss : 37,757,190,873.42466 Test Loss : 36,436,384,175.1579\n",
            "Save model Epoch 7 Train Loss : 36,876,692,465.9726 Test Loss : 35,550,012,362.10526\n",
            "Save model Epoch 8 Train Loss : 35,770,673,572.821915 Test Loss : 34,505,621,611.789474\n",
            "Save model Epoch 9 Train Loss : 34,354,822,775.232876 Test Loss : 33,334,804,372.210526\n",
            "Save model Epoch 10 Train Loss : 32,651,270,705.09589 Test Loss : 31,074,117,416.42105\n",
            "Save model Epoch 11 Train Loss : 30,750,610,053.260273 Test Loss : 29,005,574,251.789474\n",
            "Save model Epoch 12 Train Loss : 28,587,790,378.08219 Test Loss : 27,879,634,728.42105\n",
            "Save model Epoch 13 Train Loss : 26,316,623,352.9863 Test Loss : 25,888,751,292.63158\n",
            "Save model Epoch 14 Train Loss : 23,938,805,002.52055 Test Loss : 22,450,293,598.31579\n",
            "Save model Epoch 15 Train Loss : 21,507,965,390.90411 Test Loss : 19,435,718,224.842106\n",
            "Save model Epoch 16 Train Loss : 19,008,326,915.50685 Test Loss : 16,871,831,713.68421\n",
            "Save model Epoch 17 Train Loss : 16,577,470,870.794521 Test Loss : 15,143,648,202.105263\n",
            "Save model Epoch 18 Train Loss : 14,272,085,363.726027 Test Loss : 11,586,664,852.210526\n",
            "Save model Epoch 19 Train Loss : 12,332,898,970.30137 Test Loss : 11,437,014,204.631578\n",
            "Save model Epoch 20 Train Loss : 10,381,211,833.863014 Test Loss : 10,871,347,738.947369\n",
            "Save model Epoch 21 Train Loss : 8,768,432,583.890411 Test Loss : 7,957,837,231.157895\n",
            "Save model Epoch 22 Train Loss : 7,400,343,909.69863 Test Loss : 6,705,727,636.210526\n",
            "Save model Epoch 23 Train Loss : 6,209,740,920.986301 Test Loss : 5,135,099,162.947369\n",
            "Save model Epoch 24 Train Loss : 5,170,661,283.068493 Test Loss : 4,728,171,196.631579\n",
            "Save model Epoch 25 Train Loss : 4,556,431,604.602739 Test Loss : 3,878,422,312.4210525\n",
            "Save model Epoch 26 Train Loss : 4,011,706,440.767123 Test Loss : 3,791,962,078.3157897\n",
            "Save model Epoch 27 Train Loss : 3,663,435,102.6849313 Test Loss : 2,740,136,730.9473686\n",
            "Save model Epoch 28 Train Loss : 3,056,208,544.8767123 Test Loss : 2,475,373,050.9473686\n",
            "Save model Epoch 29 Train Loss : 3,126,006,388.60274 Test Loss : 2,171,323,934.3157897\n",
            "Save model Epoch 30 Train Loss : 2,783,900,486.1369863 Test Loss : 1,933,479,488.0\n",
            "Save model Epoch 31 Train Loss : 2,372,174,729.6438355 Test Loss : 1,583,130,068.2105262\n",
            "Save model Epoch 32 Train Loss : 2,138,230,993.0958905 Test Loss : 1,516,310,986.9473684\n",
            "Save model Epoch 33 Train Loss : 1,990,149,805.589041 Test Loss : 1,055,090,597.0526316\n",
            "Early stopping trigerred at epoch 38 Best Loss: 1,055,090,597.0526316 in epoch 32\n",
            "== Fold 5 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 39,361,135,798.35616 Test Loss : 38,919,176,946.52631\n",
            "Save model Epoch 2 Train Loss : 39,318,987,565.58904 Test Loss : 38,849,104,842.10526\n",
            "Save model Epoch 3 Train Loss : 39,204,712,910.904106 Test Loss : 38,714,859,843.36842\n",
            "Save model Epoch 4 Train Loss : 38,968,882,695.0137 Test Loss : 38,394,116,527.1579\n",
            "Save model Epoch 5 Train Loss : 38,568,999,893.91781 Test Loss : 37,860,518,319.1579\n",
            "Save model Epoch 6 Train Loss : 37,963,488,073.64384 Test Loss : 37,273,538,667.789474\n",
            "Save model Epoch 7 Train Loss : 37,109,815,716.821915 Test Loss : 36,428,840,960.0\n",
            "Save model Epoch 8 Train Loss : 35,986,657,841.095894 Test Loss : 35,206,257,933.47369\n",
            "Save model Epoch 9 Train Loss : 34,634,627,268.38356 Test Loss : 33,659,448,697.263157\n",
            "Save model Epoch 10 Train Loss : 32,971,738,406.575344 Test Loss : 32,191,461,591.57895\n",
            "Save model Epoch 11 Train Loss : 31,037,806,423.671234 Test Loss : 30,192,810,846.31579\n",
            "Save model Epoch 12 Train Loss : 29,004,043,993.424656 Test Loss : 27,901,679,400.42105\n",
            "Save model Epoch 13 Train Loss : 26,638,373,214.684933 Test Loss : 25,600,983,794.526318\n",
            "Save model Epoch 14 Train Loss : 24,314,928,310.356163 Test Loss : 22,139,011,287.57895\n",
            "Save model Epoch 15 Train Loss : 21,883,135,957.91781 Test Loss : 21,056,997,376.0\n",
            "Save model Epoch 16 Train Loss : 19,440,367,910.575344 Test Loss : 17,562,970,004.210526\n",
            "Save model Epoch 17 Train Loss : 17,068,701,071.780823 Test Loss : 15,168,997,537.68421\n",
            "Save model Epoch 18 Train Loss : 14,950,190,430.68493 Test Loss : 13,582,548,992.0\n",
            "Save model Epoch 19 Train Loss : 12,815,671,204.821918 Test Loss : 10,311,168,592.842106\n",
            "Save model Epoch 20 Train Loss : 10,925,168,696.109589 Test Loss : 9,863,732,574.31579\n",
            "Save model Epoch 21 Train Loss : 9,223,224,120.109589 Test Loss : 7,918,696,717.473684\n",
            "Save model Epoch 22 Train Loss : 7,881,174,815.561644 Test Loss : 7,456,131,988.210526\n",
            "Save model Epoch 23 Train Loss : 6,616,930,368.876713 Test Loss : 6,533,361,273.263158\n",
            "Save model Epoch 24 Train Loss : 5,825,241,822.684932 Test Loss : 3,932,823,511.5789475\n",
            "Save model Epoch 27 Train Loss : 3,575,358,086.1369863 Test Loss : 3,100,520,832.0\n",
            "Save model Epoch 28 Train Loss : 3,337,513,675.39726 Test Loss : 2,021,696,410.9473684\n",
            "Save model Epoch 30 Train Loss : 2,713,805,693.369863 Test Loss : 1,334,311,220.2105262\n",
            "Save model Epoch 34 Train Loss : 1,897,229,910.7945206 Test Loss : 1,197,277,790.3157895\n",
            "Save model Epoch 35 Train Loss : 1,930,247,531.1780822 Test Loss : 609,543,712.8421053\n",
            "Early stopping trigerred at epoch 40 Best Loss: 609,543,712.8421053 in epoch 34\n"
          ]
        }
      ],
      "source": [
        "fold_models = []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
        "    print(f'== Fold {fold+1} / {k_folds} ==')\n",
        "\n",
        "    train_subset = Subset(dataset, train_idx)\n",
        "    test_subset = Subset(dataset, test_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
        "    test_loader = DataLoader(test_subset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Early Stopping을 위한 변수 설정\n",
        "    best_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    patience = 3\n",
        "    counter = 0\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    # Fold 마다 Model 초기화\n",
        "    model = DNN().to(device)\n",
        "\n",
        "    # 손실함수 및 옵티마이저 설정\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "    # 학습 횟수 만큼 반복\n",
        "    for epoch in range(2000):\n",
        "\n",
        "        # 모델 학습(학습데이터)\n",
        "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # 모델 평가 (평가데이터)\n",
        "        test_loss = evaluate(model, test_loader, criterion, device)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch {epoch+1} Train Loss : {train_loss:,} Test Loss : {test_loss:,} in fold {fold}')\n",
        "\n",
        "        # Early Stopping 조건 체크\n",
        "        if test_loss < best_loss:\n",
        "            best_loss = test_loss\n",
        "            counter = 0\n",
        "            best_epoch = epoch\n",
        "            print(f'Save model Epoch {epoch+1} Train Loss : {train_loss:,} Test Loss : {test_loss:,}')\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            counter += 1\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(f'Early stopping trigerred at epoch {epoch+1} Best Loss: {best_loss:,} in epoch {best_epoch}')\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "    fold_models.append(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "m_cnt = 0\n",
        "for m in fold_models:\n",
        "    m_name = 'best_model' + str(m_cnt) + '.pt'\n",
        "    torch.save(model.state_dict(), m_name)\n",
        "    m_cnt += 1\n",
        "'''"
      ],
      "metadata": {
        "id": "lqwc-OLS8Ctj",
        "outputId": "ab185299-ca70-432c-b844-df8a679d5846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "id": "lqwc-OLS8Ctj",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nm_cnt = 0\\nfor m in fold_models:\\n    m_name = 'best_model' + str(m_cnt) + '.pt'\\n    torch.save(model.state_dict(), m_name)\\n    m_cnt += 1\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c1439106",
      "metadata": {
        "id": "c1439106"
      },
      "outputs": [],
      "source": [
        "# 학습된 모델의 TEST 예측\n",
        "\n",
        "outputs = []\n",
        "for model in fold_models:\n",
        "    with torch.no_grad():\n",
        "        output = model(TEST)\n",
        "        outputs.append(output)\n",
        "    avg_outputs = torch.mean(torch.stack(outputs), dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8fb7b590",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fb7b590",
        "outputId": "2551ede7-98f9-44da-8ed5-376420b37873"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[126773.5000],\n",
              "        [190604.6875],\n",
              "        [180696.6250],\n",
              "        ...,\n",
              "        [157227.8906],\n",
              "        [ 61226.1758],\n",
              "        [222542.9219]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# comp.prediction에 TEST 예측 결과 대입\n",
        "#comp.prediction = outputs\n",
        "comp.prediction = avg_outputs\n",
        "comp.prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ddd1d918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd1d918",
        "outputId": "4aa64386-7431-401b-cca6-d0838c915960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[House Price Prediction 평가 결과]\n",
            " AI Essential 0317(1) 과정 김재곤님의 점수는 16423.8359375 입니다."
          ]
        }
      ],
      "source": [
        "# 제출\n",
        "comp.submit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}