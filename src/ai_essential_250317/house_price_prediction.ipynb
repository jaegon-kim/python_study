{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaegon-kim/python_study/blob/main/src/ai_essential_250317/house_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C3HNZJH5LuqI",
      "metadata": {
        "id": "C3HNZJH5LuqI"
      },
      "source": [
        "# House Price Prediction\n",
        "- **목표**\n",
        "  - 이 워크샵은 주어진 데이터셋을 이용해 심층신경망 모델을 학습시켜 주택의 최종 판매 가격(SalePrice)을 예측하는 것이 최종 목표입니다.\n",
        "\n",
        "- **데이터셋 정보**\n",
        "  - 데이터셋은 총 79개의 설명 변수와 타겟 변수인 주택 가격(SalePrice)로 구성됩니다.\n",
        "  - 설명 변수는 주택의 다양한 특성(예: 건축 연도, 면적, 위치, 방 개수 등)을 포함합니다.\n",
        "  - 데이터는 판매 가격이 포함된 학습용 데이터인 `X`, `y` 와 판매 가격이 포함되지 않은 평가용 데이터인 `TEST`파일로 나뉘며, 각각 모델 학습 및 평가에 사용됩니다.\n",
        "    - 평가용 데이터 `TEST`의 판매 가격(SalePrice)를 예측 후 리더보드로 제출하여 평가합니다.\n",
        "\n",
        "- **문제 유형**\n",
        "  - 이 워크샵은 회귀 문제로 연속형 변수를 예측하는 것이 목표입니다. 모델의 성능은 `Mean Absolute Error`로 측정됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FerWbWa8ML9S",
      "metadata": {
        "id": "FerWbWa8ML9S"
      },
      "source": [
        "## 1. 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fbebc02c",
      "metadata": {
        "id": "fbebc02c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install JAEN -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2b192c23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b192c23",
        "outputId": "ad7d48fc-2d89-4c0c-9051-c2af3479b0af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# 그대로 실행하세요.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4144477e",
      "metadata": {
        "id": "4144477e"
      },
      "outputs": [],
      "source": [
        "# 사용자명을 입력하세요. (이름이 아니여도 괜찮습니다.)\n",
        "username = \"김재곤\"\n",
        "assert username, \"username 변수에 값이 설정되지 않았습니다.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "MkvHc266MWva",
      "metadata": {
        "id": "MkvHc266MWva",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# 그대로 실행하세요.\n",
        "from JAEN.competition import Competition\n",
        "comp = Competition(\n",
        "    username=username,\n",
        "    course_name='AI Essential',\n",
        "    course_round='0317(1)',\n",
        "    competition_name='House Price Prediction'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OSiIE4tdPcSV",
      "metadata": {
        "id": "OSiIE4tdPcSV"
      },
      "source": [
        "## 2. 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Cwo9d1i-ON3u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwo9d1i-ON3u",
        "outputId": "0ba5ba8a-5fca-4a51-a179-8e671fd3517f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1460, 79]), torch.Size([1460, 1]), torch.Size([1459, 79]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from JAEN.datasets import load_house_price\n",
        "X, y, TEST = load_house_price()\n",
        "X.shape, y.shape, TEST.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I_Vc3a22PgBm",
      "metadata": {
        "id": "I_Vc3a22PgBm"
      },
      "source": [
        "## 3. 제출 예시 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "933af893",
      "metadata": {
        "id": "933af893"
      },
      "outputs": [],
      "source": [
        "# TEST의 예측값 대입 (지금은 0으로 채워진 값 대입)\n",
        "#comp.prediction =  torch.zeros(1459)\n",
        "#comp.prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "wvkBKJUbKsW9",
      "metadata": {
        "id": "wvkBKJUbKsW9"
      },
      "outputs": [],
      "source": [
        "# 제출\n",
        "#comp.submit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4wNCB3ATlBe4",
      "metadata": {
        "id": "4wNCB3ATlBe4"
      },
      "source": [
        "## 4. 심층신경망 모델을 구성하고 학습하여 TEST를 예측해보세요.\n",
        "- TEST의 예측 결과는 `comp.prediction`에 대입해주세요. **torch.tensor** 형태여야합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "    for data, labels in train_loader: # 미니 배치 별 파라미터 업데이트 수행\n",
        "        data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "        # 순전파\n",
        "        outputs = model(data)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 손실 누적\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    # - len(train_loader): 평균 Loss\n",
        "    return running_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "WFaYIgOTYhZz"
      },
      "id": "WFaYIgOTYhZz",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "    with torch.no_grad():  # 평가 중에는 기울기 계산을 하지 않음\n",
        "        for data, labels in test_loader: # 미니 배치 별 손실 계산\n",
        "            data, labels = data.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "            # 순전파\n",
        "            outputs = model(data)\n",
        "\n",
        "            # 손실 계산\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 손실 누적\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    return running_loss / len(test_loader)"
      ],
      "metadata": {
        "id": "WFCtYxB8YhEs"
      },
      "id": "WFCtYxB8YhEs",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "KtwmpH1EibaM",
      "metadata": {
        "id": "KtwmpH1EibaM"
      },
      "outputs": [],
      "source": [
        "# DataLoader 생성\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "#train_loader = DataLoader(TensorDataset(X, y), batch_size=32, shuffle=True)\n",
        "dataset = TensorDataset(X, y)\n",
        "k_folds = 5\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "-37EJXcZibcK",
      "metadata": {
        "id": "-37EJXcZibcK"
      },
      "outputs": [],
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(79, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)  # 10%의 드롭아웃 적용\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.bn2(self.fc2(x)))\n",
        "        #x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "model = DNN().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a2237784",
      "metadata": {
        "id": "a2237784"
      },
      "outputs": [],
      "source": [
        "# 손실함수 및 옵티마이저 설정\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "VPKYlayzU0Dt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPKYlayzU0Dt",
        "outputId": "3fd2ded7-8cd4-496d-83f3-f9dff36f4d32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1460, 79])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3e1ded1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e1ded1c",
        "outputId": "ed122522-1394-4ca3-ed98-9a659e734bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Fold 1 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 38,885,161,044.16438 Test Loss : 38,963,217,893.052635\n",
            "Save model Epoch 2 Train Loss : 38,883,832,789.91781 Test Loss : 38,961,669,173.89474\n",
            "Save model Epoch 3 Train Loss : 38,881,920,715.39726 Test Loss : 38,958,476,665.26316\n",
            "Save model Epoch 4 Train Loss : 38,879,225,463.23288 Test Loss : 38,955,360,040.42105\n",
            "Save model Epoch 5 Train Loss : 38,875,662,223.78082 Test Loss : 38,952,808,555.789474\n",
            "Save model Epoch 6 Train Loss : 38,871,251,000.10959 Test Loss : 38,945,937,839.1579\n",
            "Save model Epoch 7 Train Loss : 38,866,297,084.49315 Test Loss : 38,944,328,757.89474\n",
            "Save model Epoch 8 Train Loss : 38,860,016,625.9726 Test Loss : 38,934,180,702.31579\n",
            "Save model Epoch 9 Train Loss : 38,853,615,475.72603 Test Loss : 38,932,080,532.210526\n",
            "Save model Epoch 10 Train Loss : 38,846,551,194.30137 Test Loss : 38,923,616,040.42105\n",
            "Save model Epoch 11 Train Loss : 38,838,155,544.54794 Test Loss : 38,910,785,536.0\n",
            "Save model Epoch 12 Train Loss : 38,829,337,109.0411 Test Loss : 38,908,162,048.0\n",
            "Save model Epoch 13 Train Loss : 38,819,928,793.42466 Test Loss : 38,893,982,881.68421\n",
            "Save model Epoch 14 Train Loss : 38,809,141,079.671234 Test Loss : 38,887,338,738.52631\n",
            "Save model Epoch 15 Train Loss : 38,799,076,646.57534 Test Loss : 38,875,708,685.47369\n",
            "Save model Epoch 16 Train Loss : 38,787,363,489.31507 Test Loss : 38,858,625,131.789474\n",
            "Save model Epoch 17 Train Loss : 38,774,135,737.863014 Test Loss : 38,844,063,959.57895\n",
            "Save model Epoch 18 Train Loss : 38,762,772,255.561646 Test Loss : 38,841,954,950.73684\n",
            "Save model Epoch 19 Train Loss : 38,749,882,227.72603 Test Loss : 38,823,638,285.47369\n",
            "Save model Epoch 20 Train Loss : 38,736,388,404.60274 Test Loss : 38,816,335,117.47369\n",
            "Save model Epoch 21 Train Loss : 38,721,904,794.30137 Test Loss : 38,813,750,110.31579\n",
            "Save model Epoch 22 Train Loss : 38,706,159,700.16438 Test Loss : 38,786,392,064.0\n",
            "Save model Epoch 23 Train Loss : 38,690,819,478.79452 Test Loss : 38,766,606,982.73684\n",
            "Save model Epoch 24 Train Loss : 38,674,352,240.21918 Test Loss : 38,734,811,674.947365\n",
            "Save model Epoch 26 Train Loss : 38,642,170,753.753426 Test Loss : 38,731,889,286.73684\n",
            "Save model Epoch 27 Train Loss : 38,622,639,230.246574 Test Loss : 38,683,416,576.0\n",
            "Save model Epoch 29 Train Loss : 38,583,769,031.89041 Test Loss : 38,663,168,970.10526\n",
            "Save model Epoch 30 Train Loss : 38,569,163,313.095894 Test Loss : 38,652,238,578.52631\n",
            "Save model Epoch 31 Train Loss : 38,547,204,460.712326 Test Loss : 38,616,631,619.36842\n",
            "Save model Epoch 32 Train Loss : 38,531,201,921.753426 Test Loss : 38,610,855,936.0\n",
            "Save model Epoch 33 Train Loss : 38,504,814,984.76712 Test Loss : 38,586,739,765.89474\n",
            "Save model Epoch 34 Train Loss : 38,485,903,724.712326 Test Loss : 38,580,874,293.89474\n",
            "Save model Epoch 35 Train Loss : 38,463,893,784.54794 Test Loss : 38,538,112,700.63158\n",
            "Save model Epoch 36 Train Loss : 38,439,992,165.69863 Test Loss : 38,528,248,778.10526\n",
            "Save model Epoch 37 Train Loss : 38,417,225,784.10959 Test Loss : 38,526,823,046.73684\n",
            "Save model Epoch 38 Train Loss : 38,396,889,705.20548 Test Loss : 38,462,967,484.63158\n",
            "Save model Epoch 40 Train Loss : 38,346,366,527.12329 Test Loss : 38,409,815,632.8421\n",
            "Save model Epoch 41 Train Loss : 38,325,646,546.41096 Test Loss : 38,397,232,074.10526\n",
            "Save model Epoch 43 Train Loss : 38,277,982,909.369865 Test Loss : 38,321,374,261.89474\n",
            "Save model Epoch 46 Train Loss : 38,203,033,908.60274 Test Loss : 38,295,093,679.1579\n",
            "Save model Epoch 47 Train Loss : 38,171,302,421.0411 Test Loss : 38,246,699,546.947365\n",
            "Save model Epoch 48 Train Loss : 38,143,885,620.60274 Test Loss : 38,178,884,769.68421\n",
            "Save model Epoch 50 Train Loss : 38,094,014,968.9863 Test Loss : 38,165,666,330.947365\n",
            "Save model Epoch 51 Train Loss : 38,064,994,332.054794 Test Loss : 38,158,791,733.89474\n",
            "Save model Epoch 52 Train Loss : 38,025,304,260.38356 Test Loss : 38,137,034,536.42105\n",
            "Save model Epoch 53 Train Loss : 38,003,557,446.136986 Test Loss : 38,092,907,250.52631\n",
            "Save model Epoch 54 Train Loss : 37,977,727,200.438354 Test Loss : 38,045,879,565.47369\n",
            "Save model Epoch 55 Train Loss : 37,955,388,920.9863 Test Loss : 38,012,519,801.26316\n",
            "Save model Epoch 57 Train Loss : 37,886,482,375.89041 Test Loss : 37,976,691,334.73684\n",
            "Save model Epoch 58 Train Loss : 37,855,388,545.753426 Test Loss : 37,837,873,906.52631\n",
            "Save model Epoch 63 Train Loss : 37,695,292,654.46575 Test Loss : 37,748,189,722.947365\n",
            "Save model Epoch 64 Train Loss : 37,668,868,741.26028 Test Loss : 37,737,104,653.47369\n",
            "Save model Epoch 67 Train Loss : 37,562,728,784.65753 Test Loss : 37,651,884,139.789474\n",
            "Save model Epoch 69 Train Loss : 37,492,887,832.54794 Test Loss : 37,593,693,022.31579\n",
            "Save model Epoch 71 Train Loss : 37,426,059,460.38356 Test Loss : 37,534,229,018.947365\n",
            "Save model Epoch 72 Train Loss : 37,392,343,096.10959 Test Loss : 37,460,993,293.47369\n",
            "Save model Epoch 74 Train Loss : 37,317,708,000.438354 Test Loss : 37,405,471,690.10526\n",
            "Save model Epoch 78 Train Loss : 37,181,202,235.61644 Test Loss : 37,304,458,725.052635\n",
            "Save model Epoch 79 Train Loss : 37,146,184,549.69863 Test Loss : 37,226,902,905.26316\n",
            "Save model Epoch 81 Train Loss : 37,063,750,136.9863 Test Loss : 37,176,745,876.210526\n",
            "Save model Epoch 83 Train Loss : 36,997,896,977.53425 Test Loss : 37,030,585,290.10526\n",
            "Save model Epoch 87 Train Loss : 36,831,803,588.38356 Test Loss : 36,934,021,551.1579\n",
            "Save model Epoch 90 Train Loss : 36,700,333,462.79452 Test Loss : 36,923,142,790.73684\n",
            "Save model Epoch 92 Train Loss : 36,606,226,460.054794 Test Loss : 36,651,317,248.0\n",
            "Save model Epoch 96 Train Loss : 36,472,075,923.287674 Test Loss : 36,587,356,698.947365\n",
            "Save model Epoch 97 Train Loss : 36,410,160,366.46575 Test Loss : 36,570,819,422.31579\n",
            "Save model Epoch 99 Train Loss : 36,329,464,284.9315 Test Loss : 36,519,162,179.36842\n",
            "Save model Epoch 100 Train Loss : 36,274,095,959.671234 Test Loss : 36,512,316,038.73684\n",
            "Save model Epoch 102 Train Loss : 36,206,259,915.39726 Test Loss : 36,369,634,789.052635\n",
            "Save model Epoch 103 Train Loss : 36,155,277,788.9315 Test Loss : 36,278,407,922.52631\n",
            "Save model Epoch 105 Train Loss : 36,053,402,946.630135 Test Loss : 36,063,394,762.10526\n",
            "Save model Epoch 106 Train Loss : 36,013,316,600.9863 Test Loss : 36,045,586,432.0\n",
            "Save model Epoch 107 Train Loss : 35,955,434,201.42466 Test Loss : 35,931,625,579.789474\n",
            "Save model Epoch 110 Train Loss : 35,832,048,233.20548 Test Loss : 35,812,763,324.63158\n",
            "Save model Epoch 113 Train Loss : 35,673,196,964.821915 Test Loss : 35,794,535,154.52631\n",
            "Save model Epoch 114 Train Loss : 35,660,407,429.26028 Test Loss : 35,711,396,378.947365\n",
            "Save model Epoch 119 Train Loss : 35,380,217,014.35616 Test Loss : 35,660,365,392.8421\n",
            "Save model Epoch 120 Train Loss : 35,340,310,864.65753 Test Loss : 35,356,114,620.63158\n",
            "Save model Epoch 123 Train Loss : 35,200,804,892.054794 Test Loss : 35,316,794,421.89474\n",
            "Save model Epoch 125 Train Loss : 35,099,544,730.30137 Test Loss : 35,028,193,387.789474\n",
            "Save model Epoch 130 Train Loss : 34,858,566,950.57534 Test Loss : 34,992,584,488.42105\n",
            "Save model Epoch 132 Train Loss : 34,731,641,028.38356 Test Loss : 34,783,188,668.63158\n",
            "Save model Epoch 135 Train Loss : 34,570,376,290.19178 Test Loss : 34,618,653,103.1579\n",
            "Save model Epoch 138 Train Loss : 34,389,884,703.561646 Test Loss : 34,611,474,108.63158\n",
            "Save model Epoch 139 Train Loss : 34,334,353,660.49315 Test Loss : 34,595,712,485.052635\n",
            "Save model Epoch 141 Train Loss : 34,208,949,682.849316 Test Loss : 34,337,641,202.526318\n",
            "Save model Epoch 145 Train Loss : 34,004,357,835.39726 Test Loss : 34,071,727,912.42105\n",
            "Save model Epoch 146 Train Loss : 33,969,260,403.72603 Test Loss : 33,867,507,388.63158\n",
            "Save model Epoch 152 Train Loss : 33,609,015,632.657536 Test Loss : 33,495,244,692.210526\n",
            "Save model Epoch 155 Train Loss : 33,453,811,515.61644 Test Loss : 33,405,817,586.526318\n",
            "Save model Epoch 161 Train Loss : 33,075,556,099.50685 Test Loss : 33,204,227,125.894737\n",
            "Save model Epoch 162 Train Loss : 33,057,540,180.164383 Test Loss : 33,115,056,559.157894\n",
            "Save model Epoch 164 Train Loss : 32,934,662,480.657536 Test Loss : 32,824,557,244.63158\n",
            "Save model Epoch 170 Train Loss : 32,479,991,022.46575 Test Loss : 32,660,990,167.57895\n",
            "Save model Epoch 171 Train Loss : 32,496,699,616.438354 Test Loss : 32,519,751,141.05263\n",
            "Save model Epoch 174 Train Loss : 32,242,082,549.47945 Test Loss : 32,073,557,261.473682\n",
            "Save model Epoch 179 Train Loss : 31,936,812,551.0137 Test Loss : 31,904,836,769.68421\n",
            "Save model Epoch 185 Train Loss : 31,608,298,818.63014 Test Loss : 31,591,893,962.105263\n",
            "Save model Epoch 190 Train Loss : 31,295,848,209.53425 Test Loss : 31,450,862,645.894737\n",
            "Save model Epoch 193 Train Loss : 31,025,495,979.835617 Test Loss : 31,025,994,374.736843\n",
            "Save model Epoch 194 Train Loss : 31,020,154,571.39726 Test Loss : 30,995,878,103.57895\n",
            "Save model Epoch 200 Train Loss : 30,648,052,974.46575 Test Loss : 30,906,837,207.57895\n",
            "Save model Epoch 201 Train Loss : 30,562,328,351.561646 Test Loss : 30,673,649,017.263157\n",
            "Save model Epoch 202 Train Loss : 30,440,966,115.945206 Test Loss : 30,448,062,571.789474\n",
            "Save model Epoch 209 Train Loss : 29,946,711,502.90411 Test Loss : 30,212,301,446.736843\n",
            "Save model Epoch 210 Train Loss : 29,892,898,395.17808 Test Loss : 29,802,427,014.736843\n",
            "Save model Epoch 218 Train Loss : 29,412,614,144.0 Test Loss : 29,530,871,161.263157\n",
            "Save model Epoch 219 Train Loss : 29,303,060,620.27397 Test Loss : 29,328,708,823.57895\n",
            "Save model Epoch 222 Train Loss : 29,140,766,888.328766 Test Loss : 28,503,390,261.894737\n",
            "Save model Epoch 230 Train Loss : 28,469,449,096.767124 Test Loss : 28,414,419,213.473682\n",
            "Save model Epoch 236 Train Loss : 28,094,044,693.041096 Test Loss : 28,086,105,141.894737\n",
            "Save model Epoch 238 Train Loss : 27,872,919,748.38356 Test Loss : 27,862,547,671.57895\n",
            "Save model Epoch 245 Train Loss : 27,439,359,677.36986 Test Loss : 27,749,227,897.263157\n",
            "Save model Epoch 246 Train Loss : 27,289,051,206.136986 Test Loss : 27,619,899,068.63158\n",
            "Save model Epoch 247 Train Loss : 27,214,376,020.164383 Test Loss : 27,514,463,232.0\n",
            "Save model Epoch 250 Train Loss : 27,102,507,975.89041 Test Loss : 27,323,696,559.157894\n",
            "Save model Epoch 251 Train Loss : 26,959,986,421.47945 Test Loss : 27,296,318,733.473682\n",
            "Save model Epoch 257 Train Loss : 26,589,938,982.575344 Test Loss : 27,283,204,904.42105\n",
            "Save model Epoch 258 Train Loss : 26,465,455,370.52055 Test Loss : 26,593,780,628.210526\n",
            "Save model Epoch 261 Train Loss : 26,223,308,126.684933 Test Loss : 26,102,691,354.94737\n",
            "Save model Epoch 266 Train Loss : 25,769,081,280.876713 Test Loss : 26,096,044,463.157894\n",
            "Save model Epoch 267 Train Loss : 25,725,449,580.71233 Test Loss : 26,041,758,450.526318\n",
            "Save model Epoch 268 Train Loss : 25,716,238,812.931507 Test Loss : 25,460,952,495.157894\n",
            "Save model Epoch 272 Train Loss : 25,386,112,028.054794 Test Loss : 25,134,186,819.36842\n",
            "Save model Epoch 279 Train Loss : 24,811,763,964.49315 Test Loss : 25,026,561,562.94737\n",
            "Save model Epoch 288 Train Loss : 24,078,204,521.20548 Test Loss : 25,005,275,998.31579\n",
            "Save model Epoch 289 Train Loss : 24,058,294,706.849316 Test Loss : 24,450,591,797.894737\n",
            "Save model Epoch 291 Train Loss : 23,855,572,725.47945 Test Loss : 24,190,716,928.0\n",
            "Save model Epoch 292 Train Loss : 23,766,552,646.136986 Test Loss : 23,971,253,625.263157\n",
            "Save model Epoch 293 Train Loss : 23,737,870,826.958904 Test Loss : 23,881,445,483.789474\n",
            "Save model Epoch 295 Train Loss : 23,625,947,893.47945 Test Loss : 23,462,728,973.473682\n",
            "Save model Epoch 298 Train Loss : 23,346,624,399.780823 Test Loss : 23,371,226,974.31579\n",
            "Save model Epoch 302 Train Loss : 23,060,965,460.164383 Test Loss : 23,317,231,184.842106\n",
            "Save model Epoch 305 Train Loss : 22,967,425,795.50685 Test Loss : 23,079,234,721.68421\n",
            "Save model Epoch 307 Train Loss : 22,713,359,850.958904 Test Loss : 23,062,126,592.0\n",
            "Save model Epoch 310 Train Loss : 22,421,138,095.342464 Test Loss : 21,876,061,184.0\n",
            "Save model Epoch 314 Train Loss : 22,084,188,707.068493 Test Loss : 21,782,385,448.42105\n",
            "Save model Epoch 318 Train Loss : 21,696,296,328.767124 Test Loss : 21,696,736,633.263157\n",
            "Save model Epoch 325 Train Loss : 21,285,729,630.684933 Test Loss : 21,265,136,801.68421\n",
            "Save model Epoch 326 Train Loss : 21,209,032,002.63014 Test Loss : 21,211,487,070.31579\n",
            "Save model Epoch 330 Train Loss : 20,772,733,937.972603 Test Loss : 21,031,812,042.105263\n",
            "Save model Epoch 334 Train Loss : 20,629,270,850.63014 Test Loss : 20,711,095,242.105263\n",
            "Save model Epoch 337 Train Loss : 20,233,975,976.328766 Test Loss : 20,342,337,482.105263\n",
            "Save model Epoch 338 Train Loss : 20,029,191,336.328766 Test Loss : 19,974,283,695.157894\n",
            "Save model Epoch 343 Train Loss : 19,816,144,657.53425 Test Loss : 19,600,999,855.157894\n",
            "Save model Epoch 345 Train Loss : 19,564,896,789.041096 Test Loss : 19,223,842,762.105263\n",
            "Save model Epoch 353 Train Loss : 18,910,580,371.28767 Test Loss : 18,713,243,270.736843\n",
            "Save model Epoch 358 Train Loss : 18,599,203,832.9863 Test Loss : 18,380,699,270.736843\n",
            "Save model Epoch 360 Train Loss : 18,437,012,999.0137 Test Loss : 17,940,567,417.263157\n",
            "Save model Epoch 367 Train Loss : 17,847,104,694.356163 Test Loss : 17,527,725,487.157894\n",
            "Save model Epoch 375 Train Loss : 17,273,715,410.410957 Test Loss : 17,141,668,217.263159\n",
            "Save model Epoch 379 Train Loss : 17,106,789,368.986301 Test Loss : 16,508,583,289.263159\n",
            "Save model Epoch 386 Train Loss : 16,479,008,599.671232 Test Loss : 16,453,493,005.473684\n",
            "Save model Epoch 389 Train Loss : 15,965,088,115.726027 Test Loss : 16,436,978,041.263159\n",
            "Save model Epoch 391 Train Loss : 15,990,290,137.424658 Test Loss : 15,625,724,604.631578\n",
            "Save model Epoch 396 Train Loss : 15,767,378,579.287672 Test Loss : 15,318,093,931.789474\n",
            "Save model Epoch 397 Train Loss : 15,655,698,474.082191 Test Loss : 15,075,414,770.526316\n",
            "Save model Epoch 404 Train Loss : 14,906,651,276.273973 Test Loss : 14,659,537,866.105263\n",
            "Save model Epoch 406 Train Loss : 14,911,405,743.342466 Test Loss : 14,432,099,058.526316\n",
            "Save model Epoch 408 Train Loss : 14,809,942,149.260275 Test Loss : 13,817,409,266.526316\n",
            "Save model Epoch 421 Train Loss : 13,728,762,788.821918 Test Loss : 13,592,927,770.947369\n",
            "Save model Epoch 431 Train Loss : 12,911,376,047.342466 Test Loss : 13,398,151,087.157894\n",
            "Save model Epoch 435 Train Loss : 12,736,780,512.438356 Test Loss : 13,008,744,582.736841\n",
            "Save model Epoch 438 Train Loss : 12,543,125,230.465754 Test Loss : 12,830,056,124.631578\n",
            "Save model Epoch 439 Train Loss : 12,562,490,087.452055 Test Loss : 12,556,054,689.68421\n",
            "Save model Epoch 441 Train Loss : 12,245,077,062.136986 Test Loss : 12,256,112,343.578947\n",
            "Save model Epoch 447 Train Loss : 11,875,990,542.027397 Test Loss : 11,275,950,511.157894\n",
            "Save model Epoch 462 Train Loss : 10,913,323,905.753426 Test Loss : 11,205,510,036.210526\n",
            "Save model Epoch 467 Train Loss : 10,519,781,533.80822 Test Loss : 10,227,985,461.894737\n",
            "Save model Epoch 470 Train Loss : 10,373,360,994.19178 Test Loss : 10,093,116,119.578947\n",
            "Save model Epoch 476 Train Loss : 9,915,959,488.876713 Test Loss : 9,751,871,568.842106\n",
            "Save model Epoch 481 Train Loss : 9,853,627,861.917809 Test Loss : 9,360,081,542.736841\n",
            "Save model Epoch 484 Train Loss : 9,436,557,697.753426 Test Loss : 9,210,726,319.157894\n",
            "Save model Epoch 485 Train Loss : 9,496,738,135.671232 Test Loss : 8,626,199,066.947369\n",
            "Save model Epoch 496 Train Loss : 8,764,475,356.931507 Test Loss : 8,354,458,839.578947\n",
            "Save model Epoch 502 Train Loss : 8,432,093,222.575342 Test Loss : 8,184,231,019.789474\n",
            "Save model Epoch 510 Train Loss : 7,978,353,439.561644 Test Loss : 7,345,361,987.368421\n",
            "Save model Epoch 522 Train Loss : 7,284,112,327.890411 Test Loss : 7,245,268,547.368421\n",
            "Save model Epoch 528 Train Loss : 6,924,644,139.835616 Test Loss : 7,040,356,176.842105\n",
            "Save model Epoch 529 Train Loss : 6,858,091,453.369863 Test Loss : 7,038,056,192.0\n",
            "Save model Epoch 530 Train Loss : 6,991,576,291.945206 Test Loss : 5,969,350,656.0\n",
            "Save model Epoch 543 Train Loss : 6,292,117,675.835616 Test Loss : 5,299,126,056.421053\n",
            "Save model Epoch 550 Train Loss : 5,758,929,081.863013 Test Loss : 5,021,395,307.789474\n",
            "Save model Epoch 552 Train Loss : 5,987,733,111.232877 Test Loss : 4,814,502,952.421053\n",
            "Save model Epoch 573 Train Loss : 5,021,805,277.808219 Test Loss : 4,726,275,226.947369\n",
            "Save model Epoch 576 Train Loss : 4,812,999,772.931507 Test Loss : 4,063,124,917.894737\n",
            "Save model Epoch 590 Train Loss : 4,392,865,918.246575 Test Loss : 4,028,891,344.8421054\n",
            "Save model Epoch 594 Train Loss : 4,438,587,424.438356 Test Loss : 3,675,358,733.4736843\n",
            "Save model Epoch 597 Train Loss : 4,395,571,141.260274 Test Loss : 3,377,279,225.263158\n",
            "Save model Epoch 602 Train Loss : 4,014,546,306.630137 Test Loss : 3,236,517,733.0526314\n",
            "Save model Epoch 619 Train Loss : 3,782,151,298.630137 Test Loss : 3,003,114,091.7894735\n",
            "Save model Epoch 627 Train Loss : 3,075,978,546.849315 Test Loss : 2,971,895,090.5263157\n",
            "Save model Epoch 629 Train Loss : 3,378,101,802.9589043 Test Loss : 2,953,683,136.0\n",
            "Save model Epoch 632 Train Loss : 3,089,750,848.8767123 Test Loss : 2,836,309,150.3157897\n",
            "Save model Epoch 633 Train Loss : 2,791,213,377.3150687 Test Loss : 2,599,960,545.6842103\n",
            "Save model Epoch 634 Train Loss : 2,764,187,677.808219 Test Loss : 2,560,917,352.4210525\n",
            "Save model Epoch 637 Train Loss : 2,762,893,480.767123 Test Loss : 2,218,335,717.0526314\n",
            "Save model Epoch 655 Train Loss : 2,384,909,954.630137 Test Loss : 2,130,949,389.4736843\n",
            "Save model Epoch 657 Train Loss : 2,206,700,709.6986303 Test Loss : 2,037,013,881.2631578\n",
            "Save model Epoch 665 Train Loss : 2,114,615,008.0 Test Loss : 1,949,246,265.2631578\n",
            "Save model Epoch 670 Train Loss : 1,766,662,406.9041095 Test Loss : 1,668,937,980.631579\n",
            "Save model Epoch 671 Train Loss : 1,867,071,468.0547945 Test Loss : 1,658,966,103.5789473\n",
            "Save model Epoch 676 Train Loss : 1,724,874,681.4246576 Test Loss : 1,648,564,269.4736843\n",
            "Save model Epoch 681 Train Loss : 1,822,561,098.739726 Test Loss : 1,534,940,380.631579\n",
            "Save model Epoch 683 Train Loss : 1,615,602,372.3835616 Test Loss : 1,321,073,684.2105262\n",
            "Save model Epoch 706 Train Loss : 1,363,484,371.7260275 Test Loss : 1,167,287,909.0526316\n",
            "Save model Epoch 720 Train Loss : 1,307,434,969.8630137 Test Loss : 1,071,528,391.5789474\n",
            "Save model Epoch 728 Train Loss : 1,324,184,249.4246576 Test Loss : 1,061,694,233.2631578\n",
            "Save model Epoch 739 Train Loss : 1,289,304,692.3835616 Test Loss : 1,044,191,204.2105263\n",
            "Save model Epoch 747 Train Loss : 1,077,016,521.8630137 Test Loss : 1,032,749,218.5263158\n",
            "Save model Epoch 748 Train Loss : 1,143,406,584.9863014 Test Loss : 1,001,159,772.6315789\n",
            "Save model Epoch 764 Train Loss : 1,064,554,854.7945205 Test Loss : 940,656,805.8947369\n",
            "Save model Epoch 773 Train Loss : 1,114,242,037.69863 Test Loss : 924,587,350.7368422\n",
            "Save model Epoch 806 Train Loss : 1,127,391,895.2328768 Test Loss : 915,018,488.0\n",
            "Save model Epoch 810 Train Loss : 959,646,915.5068493 Test Loss : 854,063,062.3157895\n",
            "Save model Epoch 837 Train Loss : 1,033,568,814.9041096 Test Loss : 842,579,541.4736842\n",
            "Save model Epoch 913 Train Loss : 1,111,763,424.4383562 Test Loss : 839,956,354.5263158\n",
            "Save model Epoch 922 Train Loss : 1,006,271,798.3561643 Test Loss : 816,921,634.9473684\n",
            "Save model Epoch 1051 Train Loss : 990,432,573.369863 Test Loss : 802,715,060.6315789\n",
            "Save model Epoch 1061 Train Loss : 1,124,832,481.3150685 Test Loss : 801,231,903.1578947\n",
            "Early stopping trigerred at epoch 1261 Best Loss 801,231,903.1578947\n",
            "== Fold 2 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 981,026,263.2328767 Test Loss : 115,383,476.21052632\n",
            "Save model Epoch 3 Train Loss : 1,120,653,383.890411 Test Loss : 81,644,212.21052632\n",
            "Early stopping trigerred at epoch 203 Best Loss 81,644,212.21052632\n",
            "== Fold 3 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 1,189,148,867.5068493 Test Loss : 220,124,990.52631578\n",
            "Save model Epoch 2 Train Loss : 1,127,728,065.7534246 Test Loss : 148,772,346.10526314\n",
            "Save model Epoch 6 Train Loss : 1,158,055,774.9041095 Test Loss : 126,346,615.68421052\n",
            "Early stopping trigerred at epoch 206 Best Loss 126,346,615.68421052\n",
            "== Fold 4 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 987,037,495.0136986 Test Loss : 156,564,367.36842105\n",
            "Save model Epoch 17 Train Loss : 1,180,564,394.958904 Test Loss : 135,797,632.84210527\n",
            "Early stopping trigerred at epoch 217 Best Loss 135,797,632.84210527\n",
            "== Fold 5 / 5 ==\n",
            "Save model Epoch 1 Train Loss : 1,198,597,791.7808218 Test Loss : 97,486,267.36842105\n",
            "Early stopping trigerred at epoch 201 Best Loss 97,486,267.36842105\n"
          ]
        }
      ],
      "source": [
        "# 모델 학습 과정 구현\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
        "    print(f'== Fold {fold+1} / {k_folds} ==')\n",
        "\n",
        "    train_subset = Subset(dataset, train_idx)\n",
        "    test_subset = Subset(dataset, test_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
        "    test_loader = DataLoader(test_subset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Early Stopping을 위한 변수 설정\n",
        "    best_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    patience = 200\n",
        "    counter = 0\n",
        "\n",
        "    # 학습 횟수 만큼 반복\n",
        "    for epoch in range(2000):\n",
        "\n",
        "        # 모델 학습(학습데이터)\n",
        "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # 모델 평가 (평가데이터)\n",
        "        test_loss = evaluate(model, test_loader, criterion, device)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        #print(f'Epoch {epoch+1} Train Loss : {train_loss} Test Loss : {test_loss}')\n",
        "\n",
        "        # Early Stopping 조건 체크\n",
        "        if test_loss < best_loss:\n",
        "            best_loss = test_loss\n",
        "        #if train_loss < best_loss:\n",
        "        #    best_loss = train_loss\n",
        "            counter = 0\n",
        "            print(f'Save model Epoch {epoch+1} Train Loss : {train_loss:,} Test Loss : {test_loss:,}')\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            counter += 1\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(f'Early stopping trigerred at epoch {epoch+1} Best Loss {best_loss:,}')\n",
        "            #model.load_state_dict(torch.load('best_model.pt'))\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_model.pt'))"
      ],
      "metadata": {
        "id": "HDtdVa9cY4U6",
        "outputId": "606931da-5b45-49b8-861f-6cef1dd439d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HDtdVa9cY4U6",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kkZl2Id4Y6tF"
      },
      "id": "kkZl2Id4Y6tF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test1) 512/4FC, 2000 epoch Save model Epoch 1893 Train Loss : 238649820.6956522 (Score: 15933)"
      ],
      "metadata": {
        "id": "g-VgNImbJpne"
      },
      "id": "g-VgNImbJpne"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c1439106",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1439106",
        "outputId": "d66229e6-5aaf-404f-99e5-405072be6624"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[129985.2891],\n",
              "        [157321.1094],\n",
              "        [186561.4531],\n",
              "        ...,\n",
              "        [166528.7188],\n",
              "        [149886.1094],\n",
              "        [237987.2656]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# 학습된 모델의 TEST 예측\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(TEST)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8fb7b590",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fb7b590",
        "outputId": "afd8de0d-047d-4289-c0fc-ceda887f1ffe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[129985.2891],\n",
              "        [157321.1094],\n",
              "        [186561.4531],\n",
              "        ...,\n",
              "        [166528.7188],\n",
              "        [149886.1094],\n",
              "        [237987.2656]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# comp.prediction에 TEST 예측 결과 대입\n",
        "comp.prediction = outputs\n",
        "comp.prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ddd1d918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd1d918",
        "outputId": "751dd90c-bb6f-419a-f381-3245ee4e5700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[House Price Prediction 평가 결과]\n",
            " AI Essential 0317(1) 과정 김재곤님의 점수는 17389.94140625 입니다."
          ]
        }
      ],
      "source": [
        "# 제출\n",
        "comp.submit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}