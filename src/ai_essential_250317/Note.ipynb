{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOLMQLiooEfysoF6k7pJV9w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaegon-kim/python_study/blob/main/src/ai_essential_250317/Note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Essential 교육\n",
        "---"
      ],
      "metadata": {
        "id": "-2xX4N_Gov1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "강사 : 이우엽\n",
        "## 수업 자료 채널\n",
        "자료: https://mm.jaen.kr/ai-essential-67/channels/off-topic\n"
      ],
      "metadata": {
        "id": "YfcIGc7oyaZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # PyTorch 라이브러리 임포트"
      ],
      "metadata": {
        "id": "XdTHSodk0cxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python\n",
        "---\n",
        "### import\n",
        "- import를 한다고 해서 package에 포함된 module 전체가 로딩되는 것이 아니다.\n",
        "  - 맨 밑에 까지 연결이 안될 수 있다.\n",
        "\n",
        "### from mymodule import welcome\n",
        "- 'from' 키워드를 사용하여, 명시된 module 전체를 로딩할 수 있다.\n",
        "\n",
        "### Python의 인덱싱\n",
        "- 양수 인덱싱 (왼쪽 부터 시작하는 인덱싱)\n",
        "- 음수 인덱싱 (오른쪽 부터 시작하는 인덱싱)\n",
        "\n",
        "---\n",
        "> tensor_2d[0, :] = 100' 를 쓰는 것과 ;tensor_2d[0][:] = 100 의 차이\n",
        "> - 뒤에 것은 쓰지 않는 것이 좋다.\n",
        "> - intemerdiate 처리하는 과정에서 copy가 발생할 수 있다.\n",
        "---"
      ],
      "metadata": {
        "id": "V--FSutm4tBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 함수"
      ],
      "metadata": {
        "id": "gNTT-gGz6d4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a, b):\n",
        "    return a + b\n",
        "print(f'{add(1, 2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BODC8RMS5vCw",
        "outputId": "63b09074-a6bd-4470-b7e4-512db2eb2845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  가변인자"
      ],
      "metadata": {
        "id": "W4E3Clqj6UDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func_1(*args):\n",
        "    for arg in args:\n",
        "        print(f'{arg}')\n",
        "\n",
        "func_1(1, 2, 3, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy6LGi1U6H8b",
        "outputId": "0f949578-e167-4477-ead2-3666fd96e4bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 키워드 가변인자"
      ],
      "metadata": {
        "id": "RE4JgKR06jfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func_2(**kwargs):\n",
        "    for k, v in kwargs.items():\n",
        "        print(f'k={k}, v={v}')\n",
        "\n",
        "func_2(a=1, b=2, hello='world')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOrG319g6m3k",
        "outputId": "cebebb50-492b-4954-845e-430317c962a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k=a, v=1\n",
            "k=b, v=2\n",
            "k=hello, v=world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_numbers(numbers):\n",
        "    for number in numbers:\n",
        "        if number % 2 == 0:\n",
        "            print(f'{number} is even')\n",
        "        else:\n",
        "            print(f'{number} is odd')\n",
        "\n",
        "numbers = {1, 2, 3, 4}\n",
        "check_numbers(numbers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3rEbNTW40t6",
        "outputId": "b615a755-8fc7-486b-e294-b66dddf98e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 is odd\n",
            "2 is even\n",
            "3 is odd\n",
            "4 is even\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = {1, 2, 3, 4}\n",
        "for number in numbers:\n",
        "    print(f'{number}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InvQu1pb5XFE",
        "outputId": "986cb60d-0aa6-4d99-82cb-f26575db66f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for number in range(0, 10):\n",
        "   print(f'{number}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuS3Cghe5h9K",
        "outputId": "d8874522-c24a-4279-9be7-0ff6081dd514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tensor Flow, Pytorch\n",
        "---\n",
        "- Tensor Flow\n",
        "  - 정적 그래프 (성능이 조금 더 좋음)\n",
        "  - 지금은 거의 사용하지 않는다.\n",
        "  - 사용자가 최적화에 대한 고민을 좀 덜한다. (저점이 높다.)\n",
        "    - 다중화/Multi Core등에 대해서는 고민을 좀 덜한다.\n",
        "- Pytorch\n",
        "  - 동적 그래프 (사용성이 더 좋다)\n"
      ],
      "metadata": {
        "id": "Po8zIoAyyxxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch\n",
        "---\n",
        "## Tensor\n",
        "- 다차원의 배열(행렬)\n",
        "- Pytorch Tensor는\n",
        "  - 자동 미분을 지원한다.\n",
        "  - GPU(CUDA)를 지원하여 연산을 가속할 수 있다. (장치를 쉽게 이동힐 수 있다.)"
      ],
      "metadata": {
        "id": "dq5x04n0o0u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Type 변경 (Device, Data Type)"
      ],
      "metadata": {
        "id": "nttXTPsA1DB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1d = torch.tensor([1.0, 2.0, 3.0])\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # GPU 사용 가능 여부 확인\n",
        "tensor_1d = tensor_1d.to(device)  # 1D 텐서 GPU로 이동\n",
        "tensor_1d.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "016p7vtX0Tst",
        "outputId": "b875aa0e-5c2c-4389-90ab-0fdd122b76aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1d = torch.tensor([1, 2, 3], dtype=torch.float64)\n",
        "tensor_1d = tensor_1d.to(torch.int64)\n",
        "tensor_1d"
      ],
      "metadata": {
        "id": "IvRLWWE_0sT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9b4d2f-6d38-4ddf-c560-25eb6fff3b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Indexing"
      ],
      "metadata": {
        "id": "3Ih41Yv1709z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1d = torch.tensor([10, 20, 30, 40])\n",
        "print(tensor_1d[0])\n",
        "print(tensor_1d[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiouYOiq7qJO",
        "outputId": "44383b53-efbd-4b3e-ae3b-928c43e17e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10)\n",
            "tensor(40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2d = torch.tensor(\n",
        "    [\n",
        "        [1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "        [7, 8, 9]\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(tensor_2d[0, 1])\n",
        "print(tensor_2d[-1, -1])"
      ],
      "metadata": {
        "id": "7skjRTu67z-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Slicing"
      ],
      "metadata": {
        "id": "GIoJzlUW8A-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_id = torch.tensor([10, 20, 30, 40, 50])\n",
        "tensor_id[:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkAtWOFL78jf",
        "outputId": "bc39cbe3-29cb-467e-ff74-8846342dd5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10, 20, 30, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2d = torch.tensor(\n",
        "    [\n",
        "        [1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "        [7, 8, 9]\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(tensor_2d[0, :]) # --> 1, 2, 3\n",
        "\n",
        "print(tensor_2d[:, 0]) # --> 1, 4, 7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JuRQAfE8M8E",
        "outputId": "199fce32-5101-420c-da61-26a4b524e937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([1, 4, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([10, 20, 30, 40, 50])\n",
        "tensor[0] = 100\n",
        "print(tensor)\n",
        "\n",
        "tensor[1:4] = torch.tensor([200, 300, 400])\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fgrcn6X8UsI",
        "outputId": "0d1d5e79-9c98-4f13-a84f-b213194036b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([100,  20,  30,  40,  50])\n",
            "tensor([100, 200, 300, 400,  50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2d = torch.tensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9]\n",
        "])\n",
        "tensor_2d[0, :] = 100\n",
        "print(tensor_2d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0enWb9ts8e6t",
        "outputId": "ff754552-91ad-402e-e308-407f5b47102c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[100, 100, 100],\n",
            "        [  4,   5,   6],\n",
            "        [  7,   8,   9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2d = torch.tensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9]\n",
        "])\n",
        "\n",
        "tensor_2d[0, :] = 0\n",
        "print(tensor_2d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDa4MA5w8t93",
        "outputId": "736cafd1-d5e3-403a-9257-c59f34597f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor 덧셈"
      ],
      "metadata": {
        "id": "8uzLQ6T284Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 x N 행렬들 간 덧셈\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "add_rslt = x + y\n",
        "print(add_rslt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8ceI9g283cs",
        "outputId": "7640dcc4-676c-4165-bac4-edceb0d88bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 7, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Sum"
      ],
      "metadata": {
        "id": "KOnNUo0d954L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(\n",
        "    [\n",
        "        [7, 1, 3],\n",
        "        [2, 8, 6],\n",
        "        [4, 5, 6]\n",
        "    ]\n",
        ")\n",
        "print(torch.sum(x))\n",
        "print(torch.sum(x, dim=0))\n",
        "print(torch.sum(x, dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lCHBioe94yg",
        "outputId": "07fbdb37-5afa-4026-8d1c-449458ac59ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(42)\n",
            "tensor([13, 14, 15])\n",
            "tensor([11, 16, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Max"
      ],
      "metadata": {
        "id": "nQU0tyQT-PBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([\n",
        "    [7, 1, 3],\n",
        "    [2, 8, 6],\n",
        "    [4, 5, 9]])\n",
        "print(torch.max(x), '\\n')\n",
        "print(torch.max(x, dim=1), '\\n')\n",
        "print(torch.max(x[0], x[1]), '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fCC0v__-Pg-",
        "outputId": "267eb052-813d-463d-ef91-3b623b428d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9) \n",
            "\n",
            "torch.return_types.max(\n",
            "values=tensor([7, 8, 9]),\n",
            "indices=tensor([0, 1, 2])) \n",
            "\n",
            "tensor([7, 8, 6]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tesnor 재구조화"
      ],
      "metadata": {
        "id": "_Y3Du_dI1MLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reshape"
      ],
      "metadata": {
        "id": "KAvfzjFv1kiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(10, 64, 7, 7) # 4차원 텐서\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sis8e7_I1O9w",
        "outputId": "c2e24dbf-0729-42b5-8f8b-d31b2994025d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 64, 7, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.reshape(-1)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V1IpCtn3dIZ",
        "outputId": "b2962191-f93f-4c78-be49-9a0584c32aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([31360])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.reshape(x.shape[0], -1)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md0r0PV_1nwb",
        "outputId": "a45d512e-7315-4a66-8b9d-7b2ec2d5e70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3136])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFOEgxla2Lfk",
        "outputId": "4af62cb1-deda-49c2-a759-b07caa6bb127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 64, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번 째 파라미터로 -1을 쓰면 2번째 파리미터로 나누어 첫번째 디멘젼을 자동 계산한다.\n",
        "y = x.reshape(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
        "#y = x.reshape(-1, 64 * 7 * 7)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-j0S-LrXRIn",
        "outputId": "7b1f019a-4d8f-4457-f1cf-d7f25c085f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3136])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.reshape(-1, x.shape[2] * x.shape[3])\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybYmQkHhYdy7",
        "outputId": "ab2d02ac-e92c-40da-c329-4467a1d3b1c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([640, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Permute"
      ],
      "metadata": {
        "id": "76-1AOEl2au-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 차원 변경\n",
        "x = torch.randn(10, 28, 28, 3)\n",
        "y = x.permute(0, 3, 1, 2)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGSeCP392fK1",
        "outputId": "d86c38cb-9c75-4b61-8376-c512aeff2d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전치 행렬\n",
        "x = torch.tensor([\n",
        "    [1, 2],\n",
        "    [3, 4]\n",
        "    ])\n",
        "print(x.permute(0, 1))\n",
        "print(x.t()) # Only for 2D Tensor\n",
        "print(x.transpose(1, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgZzWvVt2-Vb",
        "outputId": "55dbb6c2-01e9-42d8-9bd1-ab378be520c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor([[1, 3],\n",
            "        [2, 4]])\n",
            "tensor([[1, 3],\n",
            "        [2, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transpose를 이용한 차원 변환\n",
        "x = torch.randn(2, 3, 4)\n",
        "y = x.transpose(1, 2)  # 1번째와 2번째 차원을 교환\n",
        "print(y.shape)  # torch.Size([2, 4, 3])"
      ],
      "metadata": {
        "id": "u3N-sNqw3s1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise Lab1\n"
      ],
      "metadata": {
        "id": "aU9LLO6lBZD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 리스트로 텐서를 정의할 수 있다.\n",
        "x = torch.tensor([1, 2, 3, 4])\n",
        "print(f'{x}')\n",
        "\n",
        "# Dictioary는 사용하지 못한다.\n",
        "#x = torch.tensor({1, 2, 3, 4})\n",
        "#print(f'{x}')\n",
        "\n",
        "# list-range를 이용하여 정의할 수 있다.\n",
        "x = torch.tensor(list(range(1, 5)))\n",
        "print(f'{x}')\n",
        "\n",
        "# torch.arange를 이용해서도 정의할 수 있다.\n",
        "x = torch.arange(1, 5)\n",
        "print(f'{x}')\n",
        "\n",
        "x = torch.tensor(list(range(10)))\n",
        "print(f'{x}')\n",
        "\n",
        "x = torch.arange(10)\n",
        "print(f'{x}')\n",
        "\n",
        "print(f'{x.dtype}, {x.shape}, {x.device}')\n",
        "\n",
        "# reshape을 이용하여 1 x 10 텐서를 2 x 5 텐서로 reshape할 수 있다.\n",
        "x_r = x.reshape(2, 5)\n",
        "print(f'Reshaped: {x_r}, {x_r.shape}')\n",
        "\n",
        "# 텐서에서 max 값을 얻어 올 수 있다.\n",
        "print(f'Max: {torch.max(x_r)}')\n",
        "\n",
        "# dim1에서 최대 값을 얻어 올 수 있다.\n",
        "print(f'Max in dim=1: {torch.max(x_r, dim=1)}')\n",
        "\n",
        "# 두 텐서의 각 멤버들을 비교해서 큰 값을 얻어 올 수 있다.\n",
        "print(f'Max in each list: {torch.max(x_r[0], x_r[1])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjoMuaccBlS-",
        "outputId": "e18cae2d-563c-44bd-f37d-497fc2a3780e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4])\n",
            "tensor([1, 2, 3, 4])\n",
            "tensor([1, 2, 3, 4])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "torch.int64, torch.Size([10]), cpu\n",
            "Reshaped: tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 7, 8, 9]]), torch.Size([2, 5])\n",
            "Max: 9\n",
            "Max in dim=1: torch.return_types.max(\n",
            "values=tensor([4, 9]),\n",
            "indices=tensor([4, 4]))\n",
            "Max in each list: tensor([5, 6, 7, 8, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인공지능"
      ],
      "metadata": {
        "id": "My6cef27IMXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 목표: 인간처럼 생각하고 학습하고 문제를 해결\n",
        "- 분석\n",
        "- 학습\n",
        "- 추론 (예측, CoT)\n",
        "\n",
        "### 분류\n",
        "- ANI: 좁은 인공지능\n",
        "- AGI: General AI\n",
        "- ASI: 초지능 (사람을 초월하는 인간의 능력)\n",
        "\n",
        "### 모델의 선택\n",
        "- 통계적 기법 (정형 데이터)\n",
        "- 알고리즘 기반 (정형 데이터)\n",
        "- 신경만 기반 (비 정형 데이터)"
      ],
      "metadata": {
        "id": "rf-8nTWIIhIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  머신러닝\n",
        "- 명시적인 프로그래밍 없음, 입력을 토대로 출력을 만들기 위한 규칙을 찾아 내는 것"
      ],
      "metadata": {
        "id": "s_dqNzU3Ioi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델\n",
        "  - 입력 데이터를 바탕으로 예측이나 결정을 내리는 학습의 결과물 (선형회귀, 의사결정 트리, 신경망 등 다양한 알고리즘을 통해 규칙을 학습)\n",
        "  - 판별 모델(Discriminative Models): 주어진 입력 데이터를 기반으로 목표 변수를 직접 예측\n",
        "    - Classification\n",
        "  - 생성 모델(Generative Models): 주어진 입력 데이터를 통해 분포를 학습하여, 새 데이터를 생성"
      ],
      "metadata": {
        "id": "VXLrdG0rJxKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 프로그래밍과 머신러닝의 차이\n",
        "- 생성과정\n",
        "- 프로그래밍: 프로그래머가 입력과 출력을 분석하여 로직을 설계 (주어진 입력과 출력이 적어도 Rule을 만들 수 있다.)\n",
        "  - 경우의 수가 많아 지면, 만들기 어렵다.\n",
        "- 머신러닝:\n",
        "  - ...\n",
        "- Task\n",
        "- 프로그래밍: 상대적으로 정형화된 문제\n",
        "- 머신러닝: 상대적으로 정형화되지 않은 문제"
      ],
      "metadata": {
        "id": "pcywo9UpJ1ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 학습\n",
        "  - W, b를 찾는다 (y`이 실제 값 y에 가까워 지도록), 과적합이 발생할 수 있다.\n",
        "    - 표현력도 좋고\n",
        "    - 일반화도 **잘되는**"
      ],
      "metadata": {
        "id": "8Kwojdh1I0GK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 머신 러닝의 학습 절차\n",
        "  1. 문제 정의\n",
        "    - 해결하려는 문제를 규정하고 목표를 설정한다.\n",
        "    - 문제가 분류, 예측, 생성 등 어떤 유형인지 결정하고, 필요한 결과를 정의한다.\n",
        "  1. 데이터 수집\n",
        "  1. 데이터 전처리\n",
        "    - 분석 과정의 전처리: 특징을 추출하기 위한 목적, 독립 변수의 통폐합\n",
        "\t- 학습을 하기위한 전처리: 인코딩 등(ex, 문자열-> 숫자열, 값의 Scaling 적용)\n",
        "  1. 모델 선택\n",
        "  1. 모델 학습\n",
        "    - 데이터에서 패턴을 학습하고 파라미터를 최적화한다.\n",
        "  1. 모델 평가\n",
        "    - 표현력: 학습 데이터로 평가한다.\n",
        "      - 학습 데이터를 잘 반영했는지 여부\n",
        "\t- 일반화: 테스트 데이터와의 차이로 평가한다.\n",
        "\t  - 모집단의 내용이 잘 반영되는지 여부. 학습되지 않는 데이터에 대한 결과도 잘 내야 함\n",
        "  1. 모델 개선\n",
        "    - 하이퍼파라미터 튜닝\n",
        "\t- 데이터 확장\n",
        "\t- 알고리즘 변경\n",
        "  1. 예측 및 적용"
      ],
      "metadata": {
        "id": "0e2YexWoJJh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 좋은 머신 러닝 모델\n",
        "- 성능 (일반화)\n",
        "- 예측도\n",
        "- 복잡도 (데이터의 복잡도에 맞는 적절한 복잡성)\n",
        "  - 정확한 계산은 불가하지만, 그러나 LLM에서는 사용함, 단어 개수 등"
      ],
      "metadata": {
        "id": "njffb6yUKBsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 머신 러닝 학습 유형\n",
        "\n",
        "### 지도학습 (Supervised Learning)\n",
        "- 정답(y, 레이블)이 있는 데이터를 이용하여 학습. 모델은 주어진 입력(Features)에 대한 출력(y`)을 예측한다.\n",
        "- 지도하는 과정-정답과 출력의 차이를 줄이는 방향으로 지도하는 과정이 있어야 함\n",
        "- Regression (회귀), Classification (분류)\n",
        "\n",
        "### 비지도 학습 (Unsupervised Learning)\n",
        "- 레이블이 없는 데이터 이거나, 레이블이 필요 없는 경우 모델이 데이터의 구조를 분석하고 패턴을 찾는다.\n",
        "- 지도하는 과정이 없다.\n",
        "- 군집화, 차원 축소 등의 기법을 사용한다.\n",
        "- 사례: 고객 세분화, 데이터 시각화, 이상 탐지\n",
        "\t  \n",
        "### 자가 지도 학습 (Self Supervised Learning)\n",
        "- 데이터 자체에서 레이블을 생성하여 학습하는 방법이다.\n",
        "- 지도학습 임 (자기 자신을 복원하거나..)\n",
        "- Auto Encoder, Transformer\n",
        "- 사례: 언어 모델링, 이미지 복원, 대규모 사전 학습 모델 등"
      ],
      "metadata": {
        "id": "qYuS608JKJpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 머신 러닝 주요 용어\n",
        "### 데이터 셋\n",
        "- 훈련(Training) 데이터 셋과 평가(Test) 데이터 셋은 교집합이 없어야 한다.(Hold Out)\n",
        "  - 평가 데이터 넷은 훈련 데이터 넷에 누설 되면 안된다.\n",
        "### Features\n",
        "### Label\n",
        "- True Value (Label)\n",
        "### 예측 값 (Predicted Value)\n",
        "### 실제 값과 예측 값의 관계\n",
        "- 회귀: 손실, 오차/잔차\n",
        "- 분류: 손실 (Cross Entropy에서는 오차/잔차라는 말을 쓰지 않는다)"
      ],
      "metadata": {
        "id": "D3XFPQrjLGww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 딥러닝\n",
        "- 비정형 데이터 문제를 해결하는데 특화됨\n",
        "  - 전통적인 머신 러닝과 비교해서 특징 추출 과정에서 발생 할 수 있는 Feature 로스가 발생하지 않아 성능이 좋다."
      ],
      "metadata": {
        "id": "FtlA8USgLn93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNN(Deep Neural Network)\n",
        "- 심층 신경망\n",
        "- Fully Connected Network(FCN), Feed Forward Network(FFN) 이라고도 불리운다"
      ],
      "metadata": {
        "id": "iB8Vgx8YMDVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 구성\n",
        "- 다음의 층으로 구성된다.\n",
        "- 각 층은 Perceptron 또는 Neuron 이라 불리는 계산 단위로 구성된다.\n",
        "  - 가중치(Weight)와 편향 (Bias) 이라는 파라미터를 사용하여 데이터를 처리함   \n",
        "\n",
        "1. 입력층 (Input Layer)\n",
        "  - 데이터를 받아들이는 층이다.\n",
        "  - 실존하지는 않고, 개념적으로 만 있다. (Keras에서는 있도록 권장한다)\n",
        "1. 은닉층 (Hiddle Layer)\n",
        "  - 10년 전만 해도 10 ~ 20개 층이었다.  \n",
        "  - 특징을 추출한다.\n",
        "  - Perceptron으로 구성되어 있다.\n",
        "  - hidden layer들 간에 Fully Connected 되어 있어서 FC Layer라고도 한다.\n",
        "1. 출력층 (Output Layer)\n",
        "  - Perceptron으로 구성되어 있다."
      ],
      "metadata": {
        "id": "dZkZR2XaLwJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.sstatic.net/Urm63.png)\n",
        "- 가중치 (Weight)\n",
        "  - 각 입력에 할당된 가중치, 입력의 중요도 (?). 학습 과정에서 가중치가 조정된다.\n",
        "- 편향 (Bias)\n",
        "  - 가중합된 입력 데이터의 위치를 조정. 학습 과정에서 편향이 조정되낟.\n",
        "- 활성화 함수 (Step Function/Activation Function)\n",
        "  - 가중합 결과를 비선형적으로 변환. 신경망이 복잡한(비선형 적) 데이터 패턴을 학습"
      ],
      "metadata": {
        "id": "dzigAPCWMLV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vsYFpwHVn-Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 심층 신경망 학습 과정"
      ],
      "metadata": {
        "id": "B1N6CFMRMT3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 단계 개요\n",
        "- 순전파\n",
        "- 손실계산\n",
        "- 역전파(미분)\n",
        "  - 제프리 힌튼이 개발했다.\n",
        "- 최적화\n",
        "  - 경사 하강법"
      ],
      "metadata": {
        "id": "xNuYGivBMY4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 순전파(Feed Forward Propagation)\n",
        "- 예측값을 생성하는 것 (입력 데이터를 받아 출력 결과를 계산하는 과정)\n",
        "- 과정\n",
        "1. 입력층을 통해 뉴런에 전달된 데이터는 각 입력별로 가중치가 곱해짐\n",
        "1. 가중치가 곱해진 특성과 편향값을 모두 합산(선형 결합)한 후 비선형 활성화 함수에 전달\n",
        "1. 호라성화 함수는 가중합 결과를 비선형적으로 결합하여, 신경망이 복잡한 데이터 패턴을 학습하게 한다.\n",
        "- 사용처\n",
        "  - 학습\n",
        "  - 평가\n",
        "  - 추론    \n",
        "  - 가중치\n",
        "  - 편향\n",
        "  - 활성화\n",
        "  - 다각화\n",
        "  - 하이퍼 파라미터\n",
        "  - 뉴런 개수\n",
        "  - 계층 수"
      ],
      "metadata": {
        "id": "m6EsrJdeMl_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 손실 계산\n",
        "- 손실 값이 최소화 되도록 하기 위해, 예측 값과 실제 값(레이블) 사이의 차이를 Loss Function(손실함수/목적 함수)을 사용해서 계산 (ex. 차이의 제곲의 합이 최소화 되도록)\n",
        "  - 손실\n",
        "    - 모델이 얼마나 정확하게 예측하고 있는지를 수치적으로 표현한다.\n",
        "    - 손실 값이 클 수록 모델의 예측이 부정확함을 의미한다.\n",
        "  - 모델은 학습 과정에서 손실 값을 최소화하는 방향으로 가중치와 편향을 업데이트 한다.\n",
        "\n",
        "- 회귀에서 사용 가능한 손실 함수\n",
        "\n",
        "  - 실제와 예측값을 차분으로 계산 가능할 때 사용하는 손실 함수 이다.\n",
        "\n",
        "  1. MSE (Mean Square Error) : 오차의 절대값이 클 수록 큰 페널티 (큰 오차를 더 확대 해석)\n",
        "    - 차분이 지수적으로 증가함. 크게 튀는 오차를 줄이게 됨. 작은 오차는 커짐. 중간에서 만남.\n",
        "    - 오차의 편차는 줄어듬.\n",
        "\n",
        "  1. MAE (Mean Absolute Error)\n",
        "    - 큰 오차도 줄고, 작은 오차도 줄어듬 (오차 크기와 무관하게 축소/확대 해석 없이 그대로 반영함. 즉, 이상치에 덜 민감함)\n",
        "    - 오차의 편차가 크지 않지만, 가끔씩 튀는 것들이 나옴\n",
        "\n",
        " > 오차/잔차\n",
        " >   - 오차: 예측값을 만드는 데이터가 모집단인 경우\n",
        " >   - 잔차: 예측값을 만드는 데이터가 표본인 경우\n",
        "\n",
        "- 분류 (Classification)에서 사용하는 손실 함수\n",
        "  - 분류의 출력을 확률로 표현할 수 있을 때 사용한다.\n",
        "  - 확률 분포 간의 차이를 계산하는 방법을 사용한다.\n",
        "\n",
        "  1. Cross Entropy\n",
        "    - 다중 클래스 분류 문제 (선택할 수 있는 레이블의 개수가 여러 개임)\n",
        "\n",
        "  1. Binary Cross Entropy\n",
        "    - 이진 분류 문제 분석에 사용"
      ],
      "metadata": {
        "id": "M1mFQ8OBMvHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 역전파\n",
        "- 경사하강법 (Gradient Descent)\n",
        "  - 이 작업을 수행하면 미분 값이 작아진다.\n",
        "  - W, b를 조정한다. (Otimizer)\n",
        "- Loss에 대한 미분 값을 반영하는게 경사 하강법 임\n",
        "  - 출력층으로 부터 입력층 방향으로 미분 + Chain Rule을 적용하여 미분을 계산하는 방법\n",
        "  - 미분 값을 재사용함\n",
        "\n",
        "- 경사 하강법을 쓰기 위한 것임"
      ],
      "metadata": {
        "id": "TPMP1jg-PvH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 활성화 함수\n",
        "- 선형 결합 결과 (가중치 곱의 합)을 비선형 결과로 변환\n",
        "  - 출력층의 활성화 함수\n",
        "    - 출력층의 형태로 변환하기 위한 것\n",
        "    - 쓸수도 있고 쓰지 않을 수도 있음\n",
        "  - 은닉층의 활성화 함수\n",
        "    - 모델에 비 선형성을 준다.(가중합을 비선형 변환)"
      ],
      "metadata": {
        "id": "EybAXCmGQCa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sigmoid\n",
        "- 입력 값을 0~1사이로 압축\n",
        "- 기울기 소실 (Gradient Vanishing)\n",
        "  - 은닉층에 sigmoid를 쓰면 미분 값이 매우 작아 지는 효과가 있어서 잘 쓰지 않는다.\n",
        "  - 미분 값이 너무 작아져서 학습이 안된다.\n",
        "  - Sigmoid의 미분은 0 ~ 0.25가 된다."
      ],
      "metadata": {
        "id": "J9cPRW5-QmkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tanh\n",
        "- 입력 값이 범위가 -1 ~ 1\n",
        "- 미분 값의 범위가 0 ~ 1\n",
        "  - Gradient Vanishing이 완화된다."
      ],
      "metadata": {
        "id": "qZUz5PXDQpu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ReLu (Reflected LU)\n",
        "- 음수면 0, 양수면 그 값을 그대로 쓴다.\n",
        "- 미분 값은 0, 1\n",
        "  - 활성화 함수에 의한 기울기 소실 문제가 없다.\n",
        "- 1이 나오면 좋음\n",
        "- 0은 가끔씩 나오는 경우에 사용 가능. 계속 0이 나오면 학습이 안됨\n",
        "  - Dying ReLU (입력 결과와 무관하게 항상 0이 나오면 학습이 진행되지 않음)\n",
        "    \n",
        "- 미분 불가지점(0)이 있다. 좌미분을 해서 0으로 처리    \n",
        "- 연산량이 적다."
      ],
      "metadata": {
        "id": "i9CYZsv5Qwd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Leaky ReLu\n",
        " - 음수 인 경우에 0보다 살짝 큰 값을 줌.\n",
        "\n",
        "- ReLu의 대체제\n",
        "  - 출력 값이 양수 이면 쓰기 좋고, 음수가 많이 나오면 미분 값이 아주 작아서 학습이 잘안된다.\n",
        "  - 양수 값이 많다면 ReLU의 훌륭한 대체제이다.\n",
        "  - 연산량이 적다."
      ],
      "metadata": {
        "id": "zpAkvSgQQzpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ELU\n",
        "- 음수가 완만하게 줄어든다.\n",
        "- 연산량이 많다. (Exponential 연산)"
      ],
      "metadata": {
        "id": "2MWzVRsDRC76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 활성화 함수 고르기\n",
        "- Gradient Vanishing이 발생하는지.\n",
        "- 출력 데이터가 양수가 많은지, 음수가 많은지에 따라서 고름\n",
        "- 재학습이 용이한지 여부\n",
        "- 처음 하는 사람은 ReLU나 Leakt ReLU를 쓰면 됨.\n",
        "\n",
        "![Activation Function](https://raw.githubusercontent.com/jaegon-kim/python_study/main/src/ai_essential_250317/Note_image/activation_function.PNG)\n"
      ],
      "metadata": {
        "id": "5ycuJXhcRIWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 자동화 미분\n",
        "  - 각 Training Step 마다, 미분을 하는 데 미분 값을 누적하지 않고 초기화 한다.\n",
        "  - Fine Tuning 등을 하는 경우, 초기화 하지 않고 iteration을 한 후 평균 값을 반영한다.\n"
      ],
      "metadata": {
        "id": "Y-v2fqCLThZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 경사 하강법 유형\n",
        "- 1 Epoch\n",
        "  - 전체 N개 데이터 모두에 대해서 w, b update 하는것\n",
        "\n",
        "- 학습하는 데이터의 크기에 따른 분류\n",
        "\n",
        "  - 배치 최적화 도구/배치 경사 하강법(Batch Gradient Descent)\n",
        "    - 전체 N개 데이터를 한번에 넣어서 손실을 평균 내고 w, b를 '한번' 업데이트 하는 것.  손실이 1개이다.\n",
        "    - 안정적이다 (왔다 갔다 널뛰지 않고 조금씩 내려 온다)\n",
        "      - 불안정적: 기울기 0을 기준으로 왔다 갔다 널뛰는 것\n",
        "    - 정확하다. (학습 데이터에 대해 과적합된다는 의미, 좋지 않다)\n",
        "    - 한번에 메모리 요구사항도 커서 비효율 적이다.\n",
        "    - 여러 epoch을 돌려줘야 한다.\n",
        "\n",
        "  - 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)\n",
        "    - 학습 데이터에서 랜덤 샘플링(확률) 하고, 매 반복마다 한개의 데이터 포인트를 사용하여 기울기로 계산\n",
        "    - N개의 데이터로 N개의 w, b 업데이트 한다.\n",
        "      - N개의 업데이트가 되므로, N개 데이터로 학습하는데 부하가 더 큼\n",
        "    - 계산이 빠르고(1개의 계산만 하므로..), 적은 메모리를 사용한다.\n",
        "    - 안정적이지 않다. (진동이 발생한다). 수렴이 잘 안된다.\n",
        "    - epoch을 많이 안돌려도 된다.\n",
        "\n",
        "  - 미니 배치 경사 하강법 (Mini-batch Gradient Descent)\n",
        "    - M개 샘플링의 미니 batch로 학습을 하겠다. 각 배치에 대해 w, b를 계산하고 업데이트 한다.\n",
        "    - 계산 효율성과 안정성을 모두 제공하고, 크기에 따라 성능이 달라질 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "swYuhcPrTwlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer (최적화 도구)\n",
        "- Loss를 최소화 하기 위해, 경사하강법 등의 알고리즘을 사용하여 W, b 업데이트하는 것이다.\n",
        "\n",
        "- 주요 기능\n",
        "  1. 파라미터 업데이트: 역전파를 통해 계산된 그레디언트를 이용해 파라미터 업데이트\n",
        "  1. 학습률 조정: Learning Rate를 기반으로 파라미터를 얼마나 빠르게 갱신할 지 결졍\n",
        "  1. 추가 옵션 : Momentum, Weight Decay(가중치 감쇠), 학습률 스케줄링 등의 거법을 지원한다.\n",
        "\n",
        "> Pytorch에서는 torch.optim 모듈에서 다양한 옵티마이저를 제공한다.\n",
        "\n",
        "\n",
        "> 모멘텀 (Momentum)\n",
        ">  - 기울기의 누적치를 사용하여 진동을 줄이고, 더 빠르게 최적점에 도달하는 방법\n",
        ">  - 로컬 미니멈을 벗어나서, Global 미니멈 굴러 떨어지게 만들도록 함\n",
        ">  - Nesterov Momentum (Global 미니멈도 굴러 떨어지는 문제를 보완)\n",
        ">\n",
        "> 적응형 학습률\n",
        ">  - eta (learning rate) 앞에 scale factor를 곱해줌\n",
        ">    - 기울기가 클때는 작게 조정, 기울기가 완만 할 때는 크게 조정(빨리 벗어남)\n",
        ">  - AdaGrad (수식에 오류가 있음. 오래 학습하면 learning rate이 0으로 수렴),\n",
        ">  - RMSprop (AdaGrad 수식 오류를 보완한 것이다)\n",
        ">  - Adam (RMSprop + Momentum)\n",
        ">    - Adam의 Variation 들도 있지만 크 성능상의 차이가 있지는 않다."
      ],
      "metadata": {
        "id": "QfNoAVWUUIzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 반복학습\n",
        "- 데이터를 점진적으로 학습하는 과정\n",
        "- epoch: 학습 데이터를 한번에 모두 통과시키는 과정\n",
        "  - 너무 많은 epoch를 하면 학습 데이터에 과적함 됨"
      ],
      "metadata": {
        "id": "yetAgizgbSOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch 데이터 로더\n",
        "- torch.tensor()로 파이썬 리스트, NuymPy 배열을 텐서로 변환\n",
        "- torchvision.transforms 모듈의 ToTensor()로 이미지 데이터를 텐서로 변환\n",
        "- torch.utils.data.DataSet 클래스를 상속하여 사용자 정의 데이터 세트 클래스 정의 가능\n",
        "- DataLoader로 미니 배치 단위로 데이터를 쉽게 불러올 수 있으며, 배치 크기, 셔플 여부 등의 옵션을 설정할 수 있다.\n"
      ],
      "metadata": {
        "id": "XrHVBFMVsDfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset"
      ],
      "metadata": {
        "id": "cM-GN6bBtj05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataSet(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        y = self.labels[idx]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "uFQSs1uXtkqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "data = torch.randn(100, 3)\n",
        "print(data.shape)\n",
        "labels = torch.randint(0, 2, (100,))\n",
        "print(labels.shape)\n",
        "\n",
        "dataset = CustomDataSet(data, labels)\n",
        "\n",
        "print(len(dataset))\n",
        "print(data[0], labels[0])\n",
        "print(*dataset[0]) # *: 언팩 연산자\n",
        "print(dataset[0])\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOhGm6F_uK0z",
        "outputId": "2e67367d-8a2d-4e06-dcd1-3ef90d24ff42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 3])\n",
            "torch.Size([100])\n",
            "100\n",
            "tensor([-1.1258, -1.1524, -0.2506]) tensor(1)\n",
            "tensor([-1.1258, -1.1524, -0.2506]) tensor(1)\n",
            "(tensor([-1.1258, -1.1524, -0.2506]), tensor(1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 32로 구성된 미니 배치를 반환한다.\n",
        "for x, y in dataloader:\n",
        "    print(f'{x.shape}, {y.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diFRCumUvoVY",
        "outputId": "7aeeefda-0bff-4e67-e5a2-a61b6fe634fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3]), torch.Size([32])\n",
            "torch.Size([32, 3]), torch.Size([32])\n",
            "torch.Size([32, 3]), torch.Size([32])\n",
            "torch.Size([4, 3]), torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU60d6QHwTll",
        "outputId": "dc3b89df-f9e6-4b91-978d-2eba6b7ac56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 3]), torch.Size([4]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=False, drop_last=False)\n",
        "for x, y in dataloader:\n",
        "    print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS1HV9H8wnrH",
        "outputId": "79f06f7b-28fd-4f5e-ba45-cc39fdd14af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3]) torch.Size([64])\n",
            "torch.Size([36, 3]) torch.Size([36])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[-4:], y[-4:] # 마지막 미니 배치의 마지막 4개 샘플 조회"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W02he5Zcwso4",
        "outputId": "0b00e768-facd-4f0a-fe1a-33f38a4fa3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.6232, -0.2162, -0.4887],\n",
              "         [ 0.7870, -0.7328,  0.5143],\n",
              "         [ 0.3976,  0.6435, -1.1980],\n",
              "         [ 0.4784, -1.2295, -1.3700]]),\n",
              " tensor([1, 0, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  shuffle=False이므로 위와 같은 값이 출력될 것이다.\n",
        "for x, y in dataloader:\n",
        "    print(x.shape, y.shape)\n",
        "x[-4:], y[-4:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbFRKCptxPA4",
        "outputId": "3c394b9b-485c-4016-cd29-af2767a88909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3]) torch.Size([64])\n",
            "torch.Size([36, 3]) torch.Size([36])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.6232, -0.2162, -0.4887],\n",
              "         [ 0.7870, -0.7328,  0.5143],\n",
              "         [ 0.3976,  0.6435, -1.1980],\n",
              "         [ 0.4784, -1.2295, -1.3700]]),\n",
              " tensor([1, 0, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=False)\n",
        "for x, y in dataloader:\n",
        "    print(x.shape, y.shape)\n",
        "x[-4:], y[-4:] # 마지막 미니 배치의 마지막 4개 샘플 조회 > 값 변경 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etR1J1nhxITd",
        "outputId": "35cbe7af-4f93-45c5-839c-6f44ddbc014a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3]) torch.Size([64])\n",
            "torch.Size([36, 3]) torch.Size([36])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0769,  0.3380,  0.4544],\n",
              "         [ 0.7440,  1.5210,  3.4105],\n",
              "         [-1.0631, -0.0773,  0.1164],\n",
              "         [ 0.3976,  0.6435, -1.1980]]),\n",
              " tensor([0, 1, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  shuffle=True이므로 위와 다른 값이 출력될 것이다.\n",
        "for x, y in dataloader:\n",
        "    print(x.shape, y.shape)\n",
        "x[-4:], y[-4:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cui8E9NRxoKx",
        "outputId": "aa4d0c67-6aa4-42d3-cbe8-9f760d318764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3]) torch.Size([64])\n",
            "torch.Size([36, 3]) torch.Size([36])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.4777, -1.7557,  0.0762],\n",
              "         [-0.5596,  0.5335,  0.4069],\n",
              "         [ 0.7814, -1.0787, -0.7209],\n",
              "         [-0.4339,  0.8487,  0.6920]]),\n",
              " tensor([1, 1, 1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop_last=True 이므로 마지막 batch는 버려질 것이다.\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "for x, y in dataloader:\n",
        "    print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_25bRrDwx22H",
        "outputId": "8df360db-08b5-434f-8f13-6f5ec273d233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, drop_last=False)\n",
        "for x, y in dataloader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "# 첫번째 배치 얻기\n",
        "for x, y in dataloader:\n",
        "    print(x[0], y[0])\n",
        "    break\n",
        "\n",
        "# 첫번째 배치를 얻는 다른 방법 (iterator가 초기화 되나 ?)\n",
        "x, y = next(iter(dataloader))\n",
        "print(x[0], y[0])\n",
        "\n",
        "# 마지막 배치 얻어 오기\n",
        "x, y = list(dataloader)[-1]\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLxD1dL0yP9S",
        "outputId": "bca13ba4-ee8b-43e4-8a43-385a89144e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3]) torch.Size([32])\n",
            "torch.Size([32, 3]) torch.Size([32])\n",
            "torch.Size([32, 3]) torch.Size([32])\n",
            "torch.Size([4, 3]) torch.Size([4])\n",
            "tensor([-1.1258, -1.1524, -0.2506]) tensor(1)\n",
            "tensor([-1.1258, -1.1524, -0.2506]) tensor(1)\n",
            "torch.Size([4, 3]) torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 심층신경망 구현 - 모델\n",
        "- 출력층의 솔실 함수, 활성화 함수, 출력 뉴런 개수를 어떻게 구성하는지\n",
        "- 회귀\n",
        "  - Loss Function: MSE/MAE (차로 계산할 수 있는 Loss Function)\n",
        "  - 출력층 활성화 함수: 안씀\n",
        "- 이진 분류\n",
        "  - Loss Function: BCE (nn.BCELoss)\n",
        "  - 출력층 활성화 함수: Sigmoid 등\n",
        "- 다중 분류\n",
        "  - Loss Function: Cross Entropy\n",
        "  - 출력층 활성화 함수: 없음(내부적으로 Softmax)\n",
        "    - Softmax를 사용하나, PyTorch에서는 nn.CrossEntropyLoss()가 내부적으로 Log Softmax를 사용하고 있음 (계산 안정성의 장점이 있음\n"
      ],
      "metadata": {
        "id": "txvHA9fhbWwK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8dnoiwiO2kNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 과대 적합 (Overfitting)\n",
        "- 모델이 학습 데이터의 노이즈나 세부 패턴을 지나치게 학습하여 새로운 데이터에 대한 일반화 성능이 떨어지는 현상\n",
        "  - 학습 데이터에는 높은 정확도를 보이지만, 평가 데이터나 학습하지 않은 데이터에 대해서는 성능이 저하\n",
        "- 원인\n",
        "  - 계층 수가 많고, 뉴런 수가 많은 복잡한 모델은 학습 데이터의 세부 패턴까지 학습할 수 있음\n",
        "  - 데이터가 충분하지 않으면, 학습 데이터에 특화된 학습을 수행"
      ],
      "metadata": {
        "id": "RzN2WWaHAWdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 과대 적합 완화 방안\n",
        "- 데이터를 더 많이  수집한다.\n",
        "- Drop Out\n",
        "  - 학습 중에 신경만의 일부 뉴런을 무작위로 꺼서, 신경망이 특정 뉴런이나 경로에 과도하게 의존하는 것을 방지\n",
        "  - 학습: Drop Out 비율 만큼 0으로 설정\n",
        "  - 추론: 0으로 설정하지 않고, Sclale 한다....\n",
        "  - model.train(), model.eval() 에 의해 구분 된다.\n",
        "\n",
        "- Regularization (정칙화)\n",
        "  - W의 절대 값의 크기를 작게 유지(제어) 한다.\n",
        "    - 절대 값이 커지면 모델의 표현력이 좋아진다.\n",
        "    - 절대 값이 작아지면 모델의 표현력이 낮아진다.\n",
        "    - W가 커치면 손실이 커지는 상황이 발생할 수 있다.\n",
        "    - 손실함수에 Penaty를 추가 하는 방식으로 적용한다.\n",
        "    - PyTorch에서는 정칙화를 구현하지 않고, Weight Decay W = W0 - eta(미분) 하는 방법을 사용함\n",
        "    - SGD에서 Wegith Decaly를 적용하는 방법은 다음과 같음\n",
        "      - optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\n",
        "  - L1/L2 정칙화를 적용하면 모델의 목적을 수정하여 모델이 학습 데이터 손실에 집중하는 것을 방해 (?)\n",
        "\n",
        "- Early Stopping: Epoch 제어 (과대적합이 시작되기 전에 학습을 중지)\n",
        "  - Epoch를 크게 줘도 성능이 나빠질것 같으면, 자동으로 동작 시킴. 모델 Check Pointing을 같이 씀\n",
        "  - Check Pointing : 모델 성능이 잘 나올 떄, 모델 파라미터를 저장시킴.\n",
        "\n",
        "- Batch Normalization (배치 정규화)\n",
        "  - 각 계층의 입력 분포는 학습 과정동안 변화할 수 있으며, 내부 공변량 변화(Internal Covariate Shift)라고 함.\n",
        "    - 내부 공변량 변화가 심할 수 록 학습이 불안정해지고 느려질 수 있음.\n",
        "  - Feature Scaling (Data 전처리 작업에서 사용한다.)\n",
        "    - Scaling이 다른 Feature들의 Scale을 맞춘다.\n",
        "    - 수렵의 속도 상승을 위해 개발되었다.\n",
        "  - Batch Normalization은 전처리가 아니라 각 FC에 적용하는 것이다.\n",
        "    - 표준화를 하면 절대적인 크기는 줄고, 상대적인 크기는 유지된다.\n",
        "    - 정규분포(평균이 0이고 표준 편차가 1이된다.) Out Lier를 짤라져서 Overfitting 완화가 된다.\n",
        "    - 각 미니 배치에서 각 신경망의 출력값을 정규화하여, 신경 망의 각 층이 적절한 분포의 입력을 받을 수 있도록 함.      \n",
        "      - 값을 누적해 놓았다가, 추론할 때 적용한다. (train과 Eval의 동작이 다르다)\n",
        "    - Linear와 Activation 함수 사이에 넣는 것이 성능이 가장 좋다.\n",
        "\n",
        "\n",
        "- Ensemble - 모델을 여러개 만들고, 모델의 편향을 줄임\n",
        "  - 자원을 많이 먹음 (대부분의 모델에서 성능 향상됨)\n",
        "  - Mixture of Expert in LLM 도 Ensemble에 적용되는 방법임"
      ],
      "metadata": {
        "id": "Ym2ZMNqddf3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN\n",
        "- 2D Data: DNN에서 했던 데이터는 순서/위치가 없는(순서가 바뀌어도 상관 없음) 2d 독립 변수\n",
        "- 4D Data: 그림 이미지: 3차원의 의미가 있음. 순서가 중요함.\n",
        "  - 4D Data를 2D Data로 바꿔서(평탄화) 할 수 있다. (Location 정보가 소실 된다)\n",
        "  - 2D로 바뀌어도 이미지의 특질을 어떻게 유지할 수 있을까 ? (이미지를 가공/특징을 추출 한다) --> CNN\n",
        "- 특징 추출 부 + DNN 으로 구성된다.\n",
        "- CNN의 은닉층: 비선형 활성화 함수가 붙는 부분을 CNN 은닉층이라 한다.\n",
        "- DNN이 풀지 못하는 4차원 데이터(이미지 데이터) 문제를 푸는데 특화된 딥러닝 모델\n",
        "- 합성곱 층, 풀링 층,FC 층으로 구성된다.\n",
        "  - Feature 추출 : 합성곱 층 + 풀링 층\n",
        "  - 의사 결정 : FC 층\n"
      ],
      "metadata": {
        "id": "lranxc9AUTmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 합성곱 층\n",
        "- 이미지와 같은 데이터에서 유용한 특징을 추출하는 계층\n",
        "- 필터를 슬라이딩 입력 처리하여 특징 맵(Feature Map)을 만든다.\n",
        "  - 데이터의 공간적 구조를 유지하며, 유의미한 패턴(가장자리, 질감)을 학습\n",
        "  - 필터(또는 커널)는 작은 크기 행렬로 구성\n",
        "  - Receptive Field: 특징을 추출하는 필드. 필터를 적용하여 곱하고, 더한다 (합성 곱)\n",
        "\n",
        "  - 필터 자체가 학습을 통해서 완성된다. (필터가 학습된다)\n",
        "    - 기존에 알려져 있는 미분 필터나, 블러링 필터가 아닐까 추정하지만 완전한 해석은 되지 않는다.\n",
        "    - 3x3 커널은 9개의 W를 갖는다.    \n",
        "![](https://raw.githubusercontent.com/jaegon-kim/python_study/main/src/ai_essential_250317/Note_image/convolution.PNG)\n",
        "  - 특징 맵의 크기는 원본 이미지 보다 작아진다.\n",
        "- 패딩 (짜투리 공간에 0을 채운다)\n",
        "- 스트라이드 (스트라이드에 따라 연산이 안되는 영역이 생길 수 도있다)\n",
        "- channel\n",
        "  - 이미지가 R/G/B로 표현된다고 한다면, R/G/B 각각이 하나의 channel이된다.\n",
        "![](https://raw.githubusercontent.com/jaegon-kim/python_study/main/src/ai_essential_250317/Note_image/cnn_multi_channel.PNG)\n",
        "- 특징 맵 크기 계산\n",
        "  - 28, 28이미지 3X3 필터 적용\n",
        "![](https://raw.githubusercontent.com/jaegon-kim/python_study/main/src/ai_essential_250317/Note_image/cnn_map_size.PNG)  \n",
        "  - K: 3, P: 0, S: 1을 많이 쓴다."
      ],
      "metadata": {
        "id": "wXROuaw-UsOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 풀링 층\n",
        "![](https://raw.githubusercontent.com/jaegon-kim/python_study/main/src/ai_essential_250317/Note_image/cnn_pooling.PNG)\n",
        "- 특징 맵의 공간적 크기를 줄여 준다. (계산 복잡도 감소, 과적합 방지). 특정한 위치 변화에 대해 모델이 더 강인해 지도록 함.\n",
        "  - Overfitting을 완화하는 역할도 중요하다.\n",
        "- 필터를 만들어서 최대 값(Max Pooling)을 뽑거나, 아니면 평균(Average Pooling)을 적용한다. 현재는 Max Pooling 만 사용한다."
      ],
      "metadata": {
        "id": "XvLHZBC2V9iV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FC (완전 연결 층)\n",
        "\n",
        "- 모델\n",
        "Conv > ReLU > Conv > ReLU > Max Pooling > Cov > ReLu > Pool\n",
        "--(4D)--> FLAT --(2D)--> Linear > ReLu > Linear > Softmax  "
      ],
      "metadata": {
        "id": "Fx1zC7qFWou5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Pytorch\n",
        "### nn.Conv2d\n",
        "- 2d: h, w + ch\n",
        "- parameter\n",
        "  - in_channels: 입력 이미지 채널 수 (흑백:1, RGB: 3)\n",
        "  - out_channels: feature map의 수\n",
        "  - kernel_size: 커널 크기 (3x3을 중첩해서 많이 씀) 2014년 논문\n",
        "  - stride: 1 (2쓰는 경우는 거의 없음)\n",
        "  - padding: 입력의 가장 자리에 추가되는 패딩 양\n",
        "\n",
        "### MaxPool2d\n",
        "- parameter\n",
        "  - kernel_size: 풀링 적용 영역 크기 2\n",
        "  - stride: 풀링 창이 이동하는 간격\n",
        "  - padding: 0 (안쓴다)\n",
        "\n",
        "### 학습 데이터 셋\n",
        "- Fashion-MNIST"
      ],
      "metadata": {
        "id": "ShsPBY_1Wzk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전이학습(Transfer Learning)과 미세조정\n",
        "- 전이학습: 이미 학습된 모델을 활용하여 새로운 작업에 적응 시키는 기법\n",
        "  - Top/Head가 포함되어 있는 계층(FC 계층)을 교체함\n",
        "  - 대규모 데이터 세트(Image Net)에서 학습된  CNN 모델(VGG, RestNet, Inception)을 활용함\n",
        "    - 기존 모델은 이미지의 저수준 특징(ex, 엣지/색상/질감) 부터 고수준 특징까지 잘 학습되어 있어 이를 새로운 문제에 적용할 수 있음\n",
        "    - 기존 모델의 가중치를 고정(freeze)하고, 새로 학습할 데이터 세트에서 분류 계층만 재학습\n",
        "    - Conv 계층은 동결하고, FC만 다른 모델로 교체하여 학습\n",
        "    - 'requires_grad = False'를 활용하여 동결한다.\n",
        "  - 미세 조정(Fine Uning)은, 기존 학습 모델의 일부/전체 계층을 고정하지 않고, 새로운 데이터에 맞게 재학습\n",
        "    - Conv 계층의 앞 부분은 고정하고, 일부와 FC만 교체하여 학습\n",
        "    - 새로운 데이터 셋트가 원레 데이터 셋트와 크게 다르지 않거나, 더 나은 성능이 필요할 때 주로 사용"
      ],
      "metadata": {
        "id": "y5pO4SruW_qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN 동향\n"
      ],
      "metadata": {
        "id": "BNiAgFizXIbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ILSVRC 대회\n",
        "- AlexNet\n",
        "  - Drop Out, ReLu 사용이 제안되었음\n",
        "- VGG Net (2014)\n",
        "  - 3x3 커널 중첩\n",
        "  - 계층이 깊이\n",
        "- RestNet (2015)\n",
        "  - 계층의 깊이의 한계를 없앰\n",
        "  - 20~25층의 성능이 한계다 (50층 보다 20층 성능이 더 좋더라..., Vanishing Graident 라고 생각했음)\n",
        "  - 20층의 출력을 50층으로 바로 전달 해줬더니 성능이 더 좋아짐\n",
        "  - 잔차(ByPass) : 층간 통신\n",
        "- Vision Transformers (2020)\n",
        "  - Nvidia DLSS (샘플링)\n"
      ],
      "metadata": {
        "id": "IAODMxd_XOpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN 응용\n",
        "- 분류\n",
        "- 분류 + Localization\n",
        "  - Bounding Box\n",
        "- 객체 탐지\n",
        "- 세그멘테이션\n",
        "- 초해상도\n",
        "  - Low Resolution --> High Resolution"
      ],
      "metadata": {
        "id": "g4fXJontXUNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOLO 예제 (Ultraytics)"
      ],
      "metadata": {
        "id": "0Dw24k4FXZcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 초해상도 예제"
      ],
      "metadata": {
        "id": "E5QtXbTJXaBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stable Diffusion 예제"
      ],
      "metadata": {
        "id": "muhcANwiXa1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 생성형 AI"
      ],
      "metadata": {
        "id": "ypGp4hWVXnpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auto Encoder\n",
        "- Encoder: 이미지를 압축해서 Latent Vector로 만듬\n",
        "- Decoder: Latent Vector로 부터 다시 이미지를 만듬"
      ],
      "metadata": {
        "id": "Hopruq6jXrDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAE(Variational Autoencoder)\n",
        "- 입력 이미지의 Latent Vector의 분포(평균/분산)를 학습해서, Random Sampling 해서 이미지를 생성\n",
        "- Diffusion Model의 보조 도구 등으로 사용 중임"
      ],
      "metadata": {
        "id": "zadG6b-5Xq3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN(Generative Adversarial Network)\n",
        "- Generator(가짜 이미지를 만드는)와 Discriminator(진짜 인지를 판별하는) 간에 경쟁\n",
        "- Discriminator가 Collapse되면 학습이 안되는 문제가 있음\n",
        "- 작은 이미지 밖에 못만듬. Upscaler 등으로 사용 가능"
      ],
      "metadata": {
        "id": "AjXzc8OUXrfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diffusion\n",
        "- Forward Diffusion\n",
        "  - Noise를 추가 (평균과 표준 편차)\n",
        "- Reverse Diffusion\n",
        "  - Denoise 하면서 어떤 Noise를 뺄지 학습 시킴\n",
        "- 레이블\n",
        "  - Fowarding 단계에서 추가되는  Noise\n",
        "  - 해당 Noise들은 저장되었다가 Reverse 단계에서 Label로 활용한다.\n",
        "- Condition\n",
        "- Diffusion + VAE\n",
        "  - Stable Diffusion\n",
        "\n",
        "- 이미지 생성형 모델 WebUI 설치 (스테이블 디퓨전)\n",
        "https://github.com/AUTOMATIC1111/stable-diffusion-webui\n",
        "모델 정보: https://civitai.com/"
      ],
      "metadata": {
        "id": "PkQQBHIPXqXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN\n",
        "순차적 데이터 - 현재를 이해하기 위해 과거/미래 영향"
      ],
      "metadata": {
        "id": "VidJlDM9YbZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 데이터\n",
        "- 단어와 문장 결합. 비정형 데이터. 문맥이 중요함\n",
        "- Data Cleansing (데이터 형태) - 텍스트 데이터에서 불필요한 부분(태그, 특수 문자, 공백) 제거\n",
        "- Data Filtering (의미) - 개인정보/비밀 정보/오염된 정보 등을 제거\n"
      ],
      "metadata": {
        "id": "98HVmPXrYfTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 데이터 처리 - 토큰화 (시험에 나온다)\n",
        "- 텍스트 데이터를 단어/구 등을 최소 단위로 나누는 과정 (토큰 화)\n",
        "- 토큰\n",
        "  - Word\n",
        "  - Subword\n",
        "- 토큰화 과정 (전처리)\n",
        "  - 1. Tokenize (Batch Size: 3 - 3개 문장을 의미한다 )\n",
        "    - A  / Whole / new / World\n",
        "    - Heal  / the / World\n",
        "    - Under / the / sea\n",
        "  - 2. 정수화 (각 토큰에 숫자를 할당하고, 숫자로 문장을 표현한다)\n",
        "    - A (1) / Whole (2) / new (3) / World (4)\n",
        "    - Heal (5) / the (6) / World (4)\n",
        "    - Under (7) / the (6) / sea (8)\n",
        "  - 3. Padding (Sequence Len: 4) (동일한 길이로 맞춘다)\n",
        "    - 1 / 2 / 3 / 4\n",
        "    - 5 / 6 / 4 / 0\n",
        "    - 7 / 6 / 8 / 0\n",
        "- 토큰 실습\n",
        "  - https://colab.research.google.com/drive/1PcpZ8LNNtrGiN1eMBYcCYhjk5e2wDXwA?usp=sharing\n",
        "  - vocab_size: 토큰의 개수\n",
        "  - unk_token: Unknown_token\n",
        "  - pad_token: Padding token"
      ],
      "metadata": {
        "id": "3DETjTT9YmJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nn.Embedding\n",
        "- 임의의 정수가 부여된 예제 'A (1) / Whole (2) / new (3) / World (4)'\n",
        "  - 정수 ID를 부여했을 뿐인데, 모델은 new가 Whole 보다 크다고 생각한다.\n",
        "  - Wole이 World 보다 A에 더 가깝다.\n",
        "- 벡터화\n",
        "  - 하나의 정수가 아니라, 여러 실수의 조합으로 단어를 표현함\n",
        "- 임베딩\n",
        "  - 학습을 통해 벡터화 하는 것을 임베딩이라고 한다. 각 벡터는 다 파라미터(W)이다.\n",
        "  - 단어/아이템/카테고리 등의 이산적인 입력 데이터를 dense venctor로 변환해 주낟. 단어를 고차원 벡터 공간에 매핑한다.\n",
        "  - 원 핫 인코딩 처럼 고차원의 희소 벡터 대신, 저차원 밀집 벡터로 변환해 메모리와 계산 효율성을 높인다.\n",
        "  - 임베딩 테이블을 유지한다. Padding + Vocab의 개수 크기 사이즈로 인덱싱됨"
      ],
      "metadata": {
        "id": "0kITC5BLY8tF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 순환 신경망 개요\n",
        "- 순차 데이터 처리하는 NN. 이전 입력에서 학습한 내용을 현재 학습에 반영할 수 있는 구조\n",
        "  - Feed Forward Network: 은닉층의 출력이 출력층 방향으로만 전달\n",
        "  - Feed Back Network: 은닉층의 출력이 자기 자신으로 전달\n",
        "\n",
        "- 기본 단위 : Cell\n",
        "  - Neuron 과 비슷하지만 Recurrent Path를 갖는다.\n",
        "    - h_t = tanh(W_xh* x_t + W_hh * h_t-1 + b_h)\n",
        "\n",
        "  - ReLU를 쓰지 않고, tanh를 쓴다.\n",
        "    - ReLU를 쓰면 Exploring Gradient 문제가 생긴다.\n",
        "    - Sigmoid를 쓰면 Vanishing Gradient 문제가 생긴다.\n",
        "    - tanh도 Seqeunce가 길어지면 Vanishing Gradient가 발생한다.\n",
        "  - 다음 Step으로 간다."
      ],
      "metadata": {
        "id": "Z-O5BljcZJyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla RNN Cell\n",
        "- 출력 결과가 직전 계산 결과에 크게 의존함.\n",
        "- 장기 의존성 문제\n",
        "  - 시퀀스 길이가 길어지면 오래된 정보가 뒤로 충분히 전달되지 못한다.\n",
        "  - Back Propagation을 하면 오래된 W은 잘 업데이트가 되지 않는다.\n"
      ],
      "metadata": {
        "id": "GCXRJwnnZVIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM(Long Short Term Memory)\n",
        "- 긴 시퀀스 데이터에서 장기 의존성(Long Term Dependencies) 문제를 해결하기 위해 Gate 라는 개념을 도입한 구조\n",
        "  \n",
        "- Long Term: 오래 기억될 정보는 Long Term Memory에 저장하겠다.\n",
        "- Short Term: 당장 사용될 정보\n",
        "- LSTM Cell\n",
        "  - σ : Sigmoid 0 ~ 1 사이의 비율, Gate 역할을 한다. Long Term과 Short Term의 비율을 정한다.\n",
        "  - f_t : forget gate - h_t-1d에서 들어온 것을 이용해서 Forget 정도를 결정한다.\n",
        "  - i_t : input gate - h_t-1d에서 들어온 것을 이용해서 Memory 정보를 결정하고, 그 자체를 tanh를 거쳐 전달한다.\n",
        "  - o_t : output gate - 현재 시점의 기억이 얼마나 출력으로 반영될 지 결정\n",
        "![](https://raw.githubusercontent.com/jaegon-kim/python_study/main/src/ai_essential_250317/Note_image/lstm_cell.PNG)"
      ],
      "metadata": {
        "id": "Etih9y_OZZHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN의 역전파 과정\n",
        "- RNN의 역전파는\n",
        "  - 계산의 반대 방향 뿐만 아니라\n",
        "  - BTT(Back Propagation Through Time) - 시간을 거슬러 올라가며 계산을 해줌\n",
        "![](https://raw.githubusercontent.com/jaegon-kim/python_study/main/src/ai_essential_250317/Note_image/rnn_bptt.PNG)"
      ],
      "metadata": {
        "id": "7ntOS9nrZv6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 심화구조\n",
        "- 문제 유형에 따라 Unidirectional 또는 Bidirectional 모델을 선택할 수 있음\n",
        "  - Unidirectional RNN: 과거 정보만 중요한 경우\n",
        "  - Bidirectional RNN: 과거 미래 정보가 모두 중요한 경우\n",
        "- Single Layer/Multi Layer\n",
        "  - Single Layer:\n",
        "    - 많이 쓰이지는 않았지만, Feature 가 1개일 경우 Single Layer를 쓰는 경우도 있다.  \n",
        "  - Multi Layer:\n",
        "- Multi Layer 구조\n",
        "  - One to One\n",
        "  - One to Many\n",
        "  - Many to One\n",
        "    - 실제로는 Many to Many 이지만, 의미 있는 것이 한개라서..\n",
        "    - 분류/회귀, 감성분류\n",
        "  - Many to Many (Async)\n",
        "    - 자기 회귀적 (Auto Regressive) - 출력을 만들어 낼 때, 이전 출력을 자기 입력으로 받음\n",
        "      - 가장 많이 쓰려고 시도한 구조임\n",
        "  - Many to Many (Sync)\n"
      ],
      "metadata": {
        "id": "OgJzHMCoZ86e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN 예제\n",
        "#### 예제 실행 결과의 분석\n",
        "06-021 를 실행하면 LSTM의 W은 업데이트 되지만, Embeding은 업데이트가 잘 되지 않는다.\n",
        "왜냐하면 출력층에 가까운 LSTM 만을 업데이트 하는 것 만으로 Loss가 낮아지기 때문이다.\n",
        "토큰 임베딩 까지 업데이트 하려면, 모델이 충분히 복잡해야 한다."
      ],
      "metadata": {
        "id": "KSa6q5sqaHnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq\n",
        "- Async Many to Many와 같은 것이다."
      ],
      "metadata": {
        "id": "YYpVhoHEaPdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention\n",
        "- Seq2Seq 모델에서 인코더와 디코더 간의 정보 전달을 개선하고, 디코더가 인코더의 모든 출력 정보를 활용 할 수 있게 하는 기법\n",
        "- Q: Decoder의 Outut\n",
        "- K: Encoder의 각 Step의 Output\n",
        "- V: Q * K한 가중 합\n",
        "\n",
        "- 유사도 score(Q, K) = Q * K_t\n",
        "- Dot Product Attention\n",
        "  - Attention (Q, K, V) = softmax(Q * K_t) * V\n",
        "    -> 값이 커질 수록 Scale이 커지는 문제가 있음\n",
        "- Scaled Dot Product Attention\n",
        "  -  Attention (Q, K, V) = softmax(Q * K_t / root(d))\n",
        "- Multi heade attention\n",
        "- Self attention\n",
        "  - 무엇을 학습 하겠다는 것인가 ?\n",
        "  - W_q, W_k, W_v를 학습한다. Q, K, V로써의 stance가 달라지는 것을 반영한다."
      ],
      "metadata": {
        "id": "RgYlMHWbaSib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "- Encoder\n",
        "  - 전체 문맥을 보는 강점\n",
        "- Decoder\n",
        "  - 생성하는 역할\n",
        "  - Masked Multi Head Self Attention\n",
        "    - Cosal Mask를 사용함. (Softmax에 무한 값을 넣으면 0에 가까운 값이 나옴)\n",
        "\n",
        "- Feed Forward Network\n",
        "  - Transformer의 출력 층에 붙어 있음.\n",
        "    - Transformer\n",
        "  - Wide 구조 (다각화/Diversification)\n",
        "    - DNN에서 층이 깊어 지는 것과 비슷한 효과\n",
        "      - 층이 깊어질 수록 Gradient Vanishing 문제가 발생하는데, Wide에서는 이 문제가 상대적으로 덜함\n",
        "      - Overfitting을 줄이기에도 좋음\n",
        "    - Google이 발표한 Wide & Deep Learning(2016)에서 사용된 개념.\n",
        "    - 넓혔다가 줄여주는 행위\n",
        "    - 들어간 데이터를 다각화 했다가, 다시 줄이는 방법\n",
        "      - 내부 구조에서 의미 있는 것을 살리고, 의미 없는 것을 필터링하는 효과를 가짐\n",
        "\n",
        "- Positoinal Encoding\n",
        "  - Sinusoidal\n",
        "    - 문제가 있긴 하다. 지금은 다른 방식을 쓰고 있다.\n",
        "\n",
        "- Cross Entropy Loss를 사용한다.\n",
        "\n",
        "- Attention을 Main 기능으로 쓰는 것은 다 Transformer라고 하고 있음"
      ],
      "metadata": {
        "id": "MV7Wt2-eaatb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT(Bidirectional Encoder Representations from Transformers) (2018)\n",
        "- Encoder로 구성되어 있음\n",
        "  - 단어의 양방향 문맥 파악\n",
        "    - 빈칸 채우기\n",
        "    - 문장 분류\n",
        "    - 문양 유사도\n",
        "- Embedding 모델로 지금도 쓰고 있음(동적 임베딩 모델)"
      ],
      "metadata": {
        "id": "3osN6HRlamLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT(Generative Pre-trained Transformer) (2018, 2019)\n",
        "- Decoder로 구성되어 있음\n",
        "  - 문장 생성\n",
        "- GPT3\n",
        "  - 사람이 알아 듣는 느낌을 준다. (Attention == In Context Learning / Meta Learning)"
      ],
      "metadata": {
        "id": "nT7-MmD0aph7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer의 단점\n",
        "- Input Sequence의 제곱 만큼의 연산량이 필요함\n"
      ],
      "metadata": {
        "id": "19Ba_wWja6zT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "- LLM을 활용하여 다양한 APP을 구축할 수 있도록 돕는 Python 기반 프레임워크\n",
        "  - LLM 통합\n",
        "  - 체인(Chain): 여러 모델과 API의 결과를 순차적으로 연결\n",
        "  - 에이전트\n",
        "  - 메모리\n",
        "  - 지식 통합 ()\n",
        "\n",
        "  - 구조\n",
        "  - LangChain과 LangGraph를 사용해 앱 개발, LangSmith로 모니터링 및 최적화, LangGraphCloud로 프로덕션 환경에 배포\n",
        "  - 기본\n",
        "    - LangChain\n",
        "    - LangChain-core\n",
        "    - LangChain-community\n",
        "  - 추가 구조\n",
        "    - LangGraph\n",
        "      - Multi Actor Agent를 만드는데 특화된 FW\n",
        "      - Agent 들 간의 복잡현 연결 / Workflow를 구성하기 편리하게 되어 있음\n",
        "    - LangServe\n",
        "    - LangSmith\n",
        "- LCEL(LangChain Expression Language)\n",
        "  - Lang Chain의 도메인 특화 언어 (DSL)\n",
        "  - 구성\n",
        "    - Chain 구성 문법 (Chain의 형태를 선언의 형태로 쉽게 연결할 수 있음)\n",
        "      - ex) Chain = Pompt | LLM | OutputParser\n",
        "    - Runnable 프로토콜 (Runnalbe 프로토콜을 지키는 컴포넌트를 Runnable 컴포넌트라고 한다.)\n",
        "      - Runnalbe 인터페이스(stream, invoke, batch, astream, ainvoke, abatch, astream_log, astream_events)를 Class에서 구현한다.\n",
        "\n",
        "\n",
        "- Prompt Component\n",
        "  - Prompt Template\n",
        "    - Dictionary 형태로 입력해 주어야 함\n",
        "    - 출력은 Prompt Values로 반환됨\n",
        "\n",
        "  - Chat Prompt\n",
        "    - Zero\n",
        "      - 질문 바로 주고 답을 받는 것 (요청만 함)\n",
        "    - One Shot\n",
        "      - Zero Shot (질의 응답) 예시를 하나 주고 질문을 한 뒤 답을 받도록 하는 것.\n",
        "\n",
        "  - Chat Prompt Template\n",
        "  - Role 위치에 들어 갈 수 있는 Key Word는 정해져 있음. system, placeholder, ai, human\n",
        "\n",
        "  - Few Shot Prompting\n",
        "    - 질의 응답을 여러 개 주는 것\n",
        "\n",
        "\n",
        "- Model Component\n",
        "  - temperature (default: 1) Softmax 나누기 e^x에서 x 값으로 쓰인다. temparature 가 1이면 model이 준 값을 그대로 줌\n",
        "  - temperature 가 1보다 작아 지면, 확률이 큰 것이 더 커진다.\n",
        "  - temperature가 1보다 커지면, 큰 놈의 확률이 더 작아진다.\n",
        "  - Open AI는 확률이 가장 높은 것을 쓰는 것이 아니라, 정규 분포선에서 랜덤 선택하기 때문에 확률이 가장 높은 것만 선택되지는 않음.\n",
        "  - temperature가 커지면 헛소리를 하기 시작한다.\n",
        "\n",
        "- Output Parser\n",
        "\n"
      ],
      "metadata": {
        "id": "R0HOrD2JbAN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit\n"
      ],
      "metadata": {
        "id": "naJ7jkhJbvvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG (Retrieval - Augmented Generation)"
      ],
      "metadata": {
        "id": "sv-Rt1kybxZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 개요\n",
        "- 프롬프트를 증강 시키기 위한 기술 (프롬프트를 잘 만들기 위한 것임)\n",
        "- 정확도와 신뢰성을 높임 (환각을 줄임)\n",
        "- RAG로 작은 모델의 성능을 보완하려는 생각은 하지 말자. 오래된 모델의 정보를 업데이트 하는 용도로 활용하자.\n",
        "\n",
        "- RAG 단계\n",
        "  - Indexing (저장소 생성) (중요)\n",
        "    - 오프라인 단계(Test 이전 단계를 의미함). DB/Web Site Doc에서 문서 수집. 수집된 데이터를 정리하여 필요없는 정보를 제거한다. 토큰화 하여 검색에 적합한 형태로 만듬.\n",
        "\n",
        "    - 순서 예시\n",
        "      - 데이터 로드 (Load)\n",
        "        - TXT 엔진, PDF 엔진, HTML 엔징 등 로더들이 다 다름\n",
        "\n",
        "      - 텍스트 분할 (Split)\n",
        "        - 문서가 작아지면 유사도 검색이 쉬워짐. 그러나 반환 되는 문서가 작으므로 포함된 정보가 부족\n",
        "          - Multi Vector Retrieval : 작은 문서로 검색이 되면, 해당 문서가 있던 원래 문서를 반환 하는 것\n",
        "        - 문서가 커지면 편차가 커져서 유사도 검색이 어려워짐.\n",
        "\n",
        "      - 임베딩/저장 및 인덱싱 (Store)\n",
        "        - 임베딩 Vector는 워드 임베딩이 없으므로 가져다가 써야 함\n",
        "\n",
        "  - Retreival and generation (중요)\n",
        "    - 검색\n",
        "      - 키워드 기반 검색 (BM25 - Elastic Search 등에서 사용)\n",
        "      - TF-IDF (문서 희귀도) 기반을 통해 검색을 하기도 해줌\n",
        "      - 벡터 기반 검색 (임베딩 유사도 검색)\n",
        "    - Top K 개 문서를 반환해 줌\n",
        "  \n",
        "  - Chain\n",
        "    - 질문 --> 검색 --> 프롬프트 --> LLM --> 정답\n",
        "      - 질문이 프롬프트로 직접 전달되는 By Pass 경로도 있음\n",
        "\n",
        "- 토큰 임베딩/Setence 임베딩\n",
        "   - 토큰 임베딩: 토큰들 과의 관계\n",
        "   - Setence 임베딩: 문장들 간의 유사도 (아마 토큰 임베딩을 활용해서 문장들 자체의 임베딩을 만들어 놓았을 듯함)"
      ],
      "metadata": {
        "id": "ZuO8oPbXb21a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lang Chain RAG 지원\n",
        "- Lang Chain에서 Threshold와 MMR을 이용한 유사도 검색 기준 방법을 지원한다.\n",
        "\n",
        "- MMR(Maximum Marginal Relevance)\n",
        "   - 유사도가 높은 것은 추가적인 정보 획득의 효과가 적음\n",
        "   - 유사도와 '다양성'을 밸런싱\n"
      ],
      "metadata": {
        "id": "VG5UsDiRchZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent\n",
        "- 특정 작업을 수행하거나 문제를 해결하기 위해 다양한 도구들을 사용하는 역할\n",
        "  - 단순한 질의 응답 기능을 넘어 다양한 외부 도구와 상호 작용할 수 있는 지능적인 시스템\n",
        "- RAG는 AI Agent 가 활용할 수 있는 지식 검색의 방안이라고 할 수 있음\n",
        "- Lang Chain은 구현이 매우 간단함.\n",
        "  - Lang Graph 같은 FW등을 쓰는 것이 더 나음."
      ],
      "metadata": {
        "id": "1hXSq76-cohk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReAct (Reasoning and Acting)\n",
        "- AI 시스템이 문제를 해결할 때, 추론(reasoning)과 행동(acting)을 동시 수행하는 방법론"
      ],
      "metadata": {
        "id": "yYhl8whTcuac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reasoning 구현 방법\n",
        "- Human Interaction Reasoning\n",
        "- Reasoning Model (모델 자체가 추론)\n",
        "  - Fine Tuning (?)\n",
        "    - 방안 1) Label이 Chain Of Thought 임. 즉, CoT 과정을 Training 함\n",
        "    - 방안 2) 강화학습. Chain of Thought의 각 단계를 episode로 취급하여 강화학습 수행\n",
        "\n",
        "- Reasoning & Acting\n",
        "  - Human - Agent - 도구 (LLM, Tools)\n",
        "    - 답을 하기 어려우면 검색을 수행함\n",
        "  - ChatGPT 4o\n",
        "  \n",
        "- Open AI의 Functions Agent\n"
      ],
      "metadata": {
        "id": "bciTzk2Ac2js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG\n",
        "- LLM은 Metric으로 평가 측정하는 시대는 지났음."
      ],
      "metadata": {
        "id": "_m0RSZNdc4FB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAGAS\n",
        "- 평가 방법\n",
        "  - Generation: 생성된 답변의 품질이 어떤가\n",
        "  - 검색이 얼마나 잘 되었는가\n",
        "- 평가 척도\n",
        "  - Generation\n",
        "    - Faithfulness\n",
        "      - Context가 사실로 봤을 때, 만들어진 문장이 사실(Context에 포함된 정보)에 부합하는가 ?\n",
        "      - Claim 중에서 맞춘 Claim이 몇개 인지\n",
        "    - Answer Relevance\n",
        "      - 만들어진 결과가 질문에 부합하게 만들어 졌는가 ?\n",
        "        - 답변에 기반해서, n개 질문을 다시 만들고, 만들어진 질문과 원래 질문 과의 Cosine 유사도를 계산함.\n",
        "  - Retrieval\n",
        "    - Context Recall\n",
        "      - Recall :재현율/민감도, 2진 분류라면 얼마자 1을 맞췄는지 평가 하는 평가, 잘 맞췄는가를 평가하는 것이 아니다\n",
        "      - 틀린 정보를 얼마나 가지고 있는가는 보지 않고, 사실의 정보를 Claim 별로 얼마나 갖춰져 있는지\n",
        "      - Positive를 얼마나 많이 맞추는가\n",
        "      - 예)\n",
        "        - 정상 100, 코로나 100 (전체 100)\n",
        "        - 코로나 환자가 200 이라고 하면 코로나 100을 다 커버하니까 Recall이 높다.\n",
        "    - Context Precison\n",
        "      - Positive를 얼마나 적게 틀리는가\n",
        "      - 예)\n",
        "        - 정상 100, 코로나 100 (전체 100)\n",
        "        - 정상 190, 코로나 10이라고 하면 아무튼 100명중 10명알 맞췄으니까 Precison이 높다\n",
        "          - F1 Score\n",
        "          - Answer Precision\n",
        "      - 틀렸다 : 순서가 잘못됨"
      ],
      "metadata": {
        "id": "inWd7mK8c4V_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "oihHQb5Qc4-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowlege Distillation"
      ],
      "metadata": {
        "id": "MTNbBsZGc5bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 방법 1\n",
        "- 큰 모델의 지식을 작은 모델로 압축하는 기법\n",
        "  - 400B --> 70B/10B/8B\n",
        "  - Teacher Model(교사 모델): 학습된 대형 모델로, 성능이 뛰어나지만 계산량이 많고 메모리를 많이 차지하는 모델\n",
        "  - Student Model(학생 모델): 교사 모델로 부터 지식을 학습하는 작은 모델. 교사 모델과 유사한 성능을 갖지만 연산 자원이 적음\n",
        "    - Teacher Model의 분포를 모방한다.\n",
        "      - Teacher Model은 Soft Label을 만들고, Student Model는 Soft Prediction을 만들어서 Loss Func을 만든다. --> distill lation loss\n",
        "      - 동시에 Student prediction은 Hard Prediction과 Hard Label을 이용하여 Label을 계산한다.\n",
        "\n",
        "\n",
        "  - Teacher Model을 동렬하고, Soft Label을 만든다."
      ],
      "metadata": {
        "id": "l9SBwHAPc55F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 방법 2\n",
        "- Input 데이터를 Teacher Model에 넣어서 Label을 만들어서 Training Set을 만든다.\n",
        "  - 출력의 분포가 잘 샘플링 된 데이터만 학습될 것이다. (노이즈를 학습되지 않는...)\n",
        "- 해당 Trainsing Set으로 Studnet Model을 Supervised Learning 시킨다.\n"
      ],
      "metadata": {
        "id": "NJCFPYmkc6j0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PEFT (Parameter Efficient Fine Tuning)\n",
        "- 사전 학습된 언어 모델의 파라미터들을 동결하고 Adapter 계층을 추가하여 학습 시킴"
      ],
      "metadata": {
        "id": "GUpbDcsLdT8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoRA(Low Rand Adaptation)\n",
        "- 대규모 모델의 가중치를 그대로 유지하면서 Low Rand Matrix Factorization을 통해 적은 수의 추가 파라미터를 도입\n",
        "- Matrix Fatorization (원래는 Matrix를 분해하는 것인데)\n",
        "  - LoRA는 분해되어 있는 Matrix를 합치는 동작을 수행함 .\n",
        "    - (N x 1 ) * (1 x M) = (N * M) 행렬이 됨. 학습은 '(N x 1 ) * (1 x M)'를 학습 시킴\n",
        "\n",
        "![](https://raw.githubusercontent.com/jaegon-kim/python_study/main/src/ai_essential_250317/Note_image/lora.PNG)\n"
      ],
      "metadata": {
        "id": "2YTk5XMLdT5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QLoRA\n",
        "- Pretrained Model을 양자화 한 다음에 Freeze 하고 학습을 진행"
      ],
      "metadata": {
        "id": "QKcB9BskdT2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LoftQ\n",
        "- Pretrained Model은 그대로 두고, Adapter를 양자화"
      ],
      "metadata": {
        "id": "z-ObH8-YdTzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RSLoRA"
      ],
      "metadata": {
        "id": "CIpnya-ndTwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "파이토치 텐서 18, 20, 23, 26, Lab1 연습문제\n",
        "\n",
        "머신러닝 학습 절차 34-35\n",
        "\n",
        "머신러닝 학습 유형 37\n",
        "\n",
        "심층신경망 45\n",
        "\n",
        "뉴런(퍼셉트론) 47\n",
        "\n",
        "손실함수 50\n",
        "\n",
        "최적화도구(optimizer) 68-69\n",
        "\n",
        "데이터로더 71-76, Lab1 연습문제\n",
        "\n",
        "DNN 모델 구성 81\n",
        "\n",
        "DNN 모델링 83-88, Lab1 연습문제, WS1\n",
        "\n",
        "과적합 90-91\n",
        "\n",
        "배치정규화 99-100\n",
        "\n",
        "합성곱층 105-106\n",
        "\n",
        "풀링층 113-114\n",
        "\n",
        "CNN 모델 구성 121-123, Lab2 연습문제, WS2\n",
        "\n",
        "전이학습 125\n",
        "\n",
        "임베딩 160\n",
        "\n",
        "순환신경망168-169\n",
        "\n",
        "\n",
        "\n",
        "LSTM 172-173\n",
        "\n",
        "LSTM 모델 구성 181-182, Lab3 연습문제, WS3\n",
        "\n",
        "랭체인 215-216\n",
        "\n",
        "RAG 246-251\n",
        "\n",
        "벡터스토어 260\n"
      ],
      "metadata": {
        "id": "lLc8wJH4kZbx"
      }
    }
  ]
}